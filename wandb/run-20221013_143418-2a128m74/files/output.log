gym.envs.registry.env_specs[env_id].entry_point gym.envs.classic_control:AcrobotEnv
gym.envs.registry.env_specs[env_id].entry_point gym.envs.classic_control:AcrobotEnv
OrderedDict([('actor_delay', 3),
             ('advantage_flipped_rate', 0.2),
             ('batch_size', 16),
             ('device', 1),
             ('ent_coef', 0.0),
             ('gae_lambda', 0.94),
             ('gamma', 0.99),
             ('independent_value_net', True),
             ('learning_rate', 0.01),
             ('n_envs', 1),
             ('n_epochs', 200),
             ('n_steps', 256),
             ('n_timesteps', 1500000),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs',
              {'optimizer_class': <class 'torch.optim.sgd.SGD'>}),
             ('policy_update_scheme', 1),
             ('rgamma', 0.9)])
Using 1 environments
Creating test environment
Normalization activated: {'gamma': 0.99, 'norm_reward': False}
Normalization activated: {'gamma': 0.99}
Using cuda:1 device
setup model true
Log path: logs/hpo/Acrobot-v1_154
n_eval_episodes 100
Logging to runs/Acrobot-v1__hpo__456__1665642855/Acrobot-v1/HPO_1
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------
| Time/                   |          |
|    collect_computeV/... | 0.00917  |
|    collect_computeV/Sum | 2.35     |
|    collect_rollout/Mean | 3.69     |
|    collect_rollout/Sum  | 3.69     |
| time/                   |          |
|    fps                  | 69       |
|    iterations           | 1        |
|    time_elapsed         | 3        |
|    total_timesteps      | 256      |
--------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0734      |
|    negative_advantag... | 0.48087865  |
|    positive_advantag... | 0.51912135  |
|    prob_ratio           | 1.1533455   |
|    rollout_return       | -0.90100706 |
| Time/                   |             |
|    collect_computeV/... | 0.00904     |
|    collect_computeV/Sum | 2.32        |
|    collect_rollout/Mean | 3.05        |
|    collect_rollout/Sum  | 3.05        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 125         |
|    train_epoch/Sum      | 125         |
|    train_loss/Mean      | 0.00689     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 2           |
|    time_elapsed         | 132         |
|    total_timesteps      | 512         |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.004038524 |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.0885     |
|    learning_rate        | 0.01        |
|    loss                 | 0.214       |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.0267      |
|    value_loss           | 0.328       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.077      |
|    negative_advantag... | 0.48789504 |
|    positive_advantag... | 0.5121049  |
|    prob_ratio           | 1.2637076  |
|    rollout_return       | -1.1672444 |
| Time/                   |            |
|    collect_computeV/... | 0.0093     |
|    collect_computeV/Sum | 2.38       |
|    collect_rollout/Mean | 3.13       |
|    collect_rollout/Sum  | 3.13       |
|    train_action_adv/... | 0.00666    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.6       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00692    |
|    train_loss/Sum       | 22.2       |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 3          |
|    time_elapsed         | 260        |
|    total_timesteps      | 768        |
| train/                  |            |
|    active_example       | 256        |
|    approx_kl            | 0.05793669 |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | -1.92      |
|    learning_rate        | 0.01       |
|    loss                 | 0.154      |
|    n_updates            | 400        |
|    policy_gradient_loss | 0.0163     |
|    value_loss           | 0.0279     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0725      |
|    negative_advantag... | 0.47126725  |
|    positive_advantag... | 0.5287328   |
|    prob_ratio           | 1.3998123   |
|    rollout_return       | -1.5668123  |
| Time/                   |             |
|    collect_computeV/... | 0.00913     |
|    collect_computeV/Sum | 2.34        |
|    collect_rollout/Mean | 3.08        |
|    collect_rollout/Sum  | 3.08        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00692     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 4           |
|    time_elapsed         | 389         |
|    total_timesteps      | 1024        |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.037899476 |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | -0.544      |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.0157      |
|    value_loss           | 0.0085      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0733     |
|    negative_advantag... | 0.4690203  |
|    positive_advantag... | 0.53097963 |
|    prob_ratio           | 1.2971414  |
|    rollout_return       | -1.7780085 |
| Time/                   |            |
|    collect_computeV/... | 0.0091     |
|    collect_computeV/Sum | 2.33       |
|    collect_rollout/Mean | 3.07       |
|    collect_rollout/Sum  | 3.07       |
|    train_action_adv/... | 0.00664    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0151     |
|    train_computeV/Sum   | 48.5       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00694    |
|    train_loss/Sum       | 22.2       |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 5          |
|    time_elapsed         | 518        |
|    total_timesteps      | 1280       |
| train/                  |            |
|    active_example       | 256        |
|    approx_kl            | 0.09895931 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.00911    |
|    value_loss           | 0.0357     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0801      |
|    negative_advantag... | 0.52875036  |
|    positive_advantag... | 0.47124958  |
|    prob_ratio           | 1.8747271   |
|    rollout_return       | -2.1004782  |
| Time/                   |             |
|    collect_computeV/... | 0.00914     |
|    collect_computeV/Sum | 2.34        |
|    collect_rollout/Mean | 3.08        |
|    collect_rollout/Sum  | 3.08        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00693     |
|    train_loss/Sum       | 22.2        |
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | -497        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 6           |
|    time_elapsed         | 647         |
|    total_timesteps      | 1536        |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.023841832 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.97       |
|    explained_variance   | -0.114      |
|    learning_rate        | 0.01        |
|    loss                 | 0.28        |
|    n_updates            | 1000        |
|    policy_gradient_loss | 0.0201      |
|    value_loss           | 0.00689     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0735      |
|    negative_advantag... | 0.47900972  |
|    positive_advantag... | 0.5209903   |
|    prob_ratio           | 1.5615762   |
|    rollout_return       | -2.101765   |
| Time/                   |             |
|    collect_computeV/... | 0.00913     |
|    collect_computeV/Sum | 2.34        |
|    collect_rollout/Mean | 3.07        |
|    collect_rollout/Sum  | 3.07        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.9        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | -497        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 7           |
|    time_elapsed         | 776         |
|    total_timesteps      | 1792        |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.014010511 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0181      |
|    n_updates            | 1200        |
|    policy_gradient_loss | 0.00156     |
|    value_loss           | 0.0231      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0705     |
|    negative_advantag... | 0.46949893 |
|    positive_advantag... | 0.53050107 |
|    prob_ratio           | 1.618546   |
|    rollout_return       | -2.4411829 |
| Time/                   |            |
|    collect_computeV/... | 0.00921    |
|    collect_computeV/Sum | 2.36       |
|    collect_rollout/Mean | 3.1        |
|    collect_rollout/Sum  | 3.1        |
|    train_action_adv/... | 0.00662    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.6       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00691    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | -476       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 8          |
|    time_elapsed         | 905        |
|    total_timesteps      | 2048       |
| train/                  |            |
|    active_example       | 256        |
|    approx_kl            | 0.12196779 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.849     |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 1400       |
|    policy_gradient_loss | 0.00359    |
|    value_loss           | 0.00359    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0654      |
|    negative_advantag... | 0.4514226   |
|    positive_advantag... | 0.54857737  |
|    prob_ratio           | 1.9700845   |
|    rollout_return       | -2.3797271  |
| Time/                   |             |
|    collect_computeV/... | 0.00907     |
|    collect_computeV/Sum | 2.32        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.9        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 428         |
|    ep_rew_mean          | -427        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 9           |
|    time_elapsed         | 1033        |
|    total_timesteps      | 2304        |
| train/                  |             |
|    active_example       | 254         |
|    approx_kl            | -0.12656432 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.00385     |
|    value_loss           | 0.0232      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.065       |
|    negative_advantag... | 0.47172096  |
|    positive_advantag... | 0.5282656   |
|    prob_ratio           | 2.713429    |
|    rollout_return       | -2.43379    |
| Time/                   |             |
|    collect_computeV/... | 0.00921     |
|    collect_computeV/Sum | 2.36        |
|    collect_rollout/Mean | 3.09        |
|    collect_rollout/Sum  | 3.09        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | -398        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 10          |
|    time_elapsed         | 1162        |
|    total_timesteps      | 2560        |
| train/                  |             |
|    active_example       | 255         |
|    approx_kl            | -0.07488635 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.01        |
|    loss                 | 0.544       |
|    n_updates            | 1800        |
|    policy_gradient_loss | 0.0145      |
|    value_loss           | 0.025       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0515     |
|    negative_advantag... | 0.37132958 |
|    positive_advantag... | 0.62867033 |
|    prob_ratio           | 1.6681683  |
|    rollout_return       | -2.501831  |
| Time/                   |            |
|    collect_computeV/... | 0.00903    |
|    collect_computeV/Sum | 2.31       |
|    collect_rollout/Mean | 3.04       |
|    collect_rollout/Sum  | 3.04       |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00689    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 396        |
|    ep_rew_mean          | -395       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 11         |
|    time_elapsed         | 1292       |
|    total_timesteps      | 2816       |
| train/                  |            |
|    active_example       | 250        |
|    approx_kl            | 0.19848177 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.01       |
|    loss                 | 0.389      |
|    n_updates            | 2000       |
|    policy_gradient_loss | 0.0108     |
|    value_loss           | 0.0196     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0553        |
|    negative_advantag... | 0.39791745    |
|    positive_advantag... | 0.6020826     |
|    prob_ratio           | 2.2693872     |
|    rollout_return       | -2.4165533    |
| Time/                   |               |
|    collect_computeV/... | 0.00906       |
|    collect_computeV/Sum | 2.32          |
|    collect_rollout/Mean | 3.06          |
|    collect_rollout/Sum  | 3.06          |
|    train_action_adv/... | 0.00665       |
|    train_action_adv/Sum | 21.3          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.5          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.00689       |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 370           |
|    ep_rew_mean          | -369          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 12            |
|    time_elapsed         | 1420          |
|    total_timesteps      | 3072          |
| train/                  |               |
|    active_example       | 212           |
|    approx_kl            | -0.0018989444 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.642        |
|    explained_variance   | 0.0428        |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 2200          |
|    policy_gradient_loss | 0.00481       |
|    value_loss           | 0.0174        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0559     |
|    negative_advantag... | 0.41008198 |
|    positive_advantag... | 0.589918   |
|    prob_ratio           | 2.7024632  |
|    rollout_return       | -2.4784613 |
| Time/                   |            |
|    collect_computeV/... | 0.00919    |
|    collect_computeV/Sum | 2.35       |
|    collect_rollout/Mean | 3.1        |
|    collect_rollout/Sum  | 3.1        |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00687    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 354        |
|    ep_rew_mean          | -353       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 13         |
|    time_elapsed         | 1549       |
|    total_timesteps      | 3328       |
| train/                  |            |
|    active_example       | 245        |
|    approx_kl            | 0.12556124 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.568     |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.01       |
|    loss                 | 0.0945     |
|    n_updates            | 2400       |
|    policy_gradient_loss | 0.00396    |
|    value_loss           | 0.027      |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0513     |
|    negative_advantag... | 0.37180537 |
|    positive_advantag... | 0.62819463 |
|    prob_ratio           | 2.824501   |
|    rollout_return       | -2.434794  |
| Time/                   |            |
|    collect_computeV/... | 0.00927    |
|    collect_computeV/Sum | 2.37       |
|    collect_rollout/Mean | 3.12       |
|    collect_rollout/Sum  | 3.12       |
|    train_action_adv/... | 0.00667    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00689    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 354        |
|    ep_rew_mean          | -353       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 14         |
|    time_elapsed         | 1678       |
|    total_timesteps      | 3584       |
| train/                  |            |
|    active_example       | 245        |
|    approx_kl            | 0.07550628 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 2600       |
|    policy_gradient_loss | 0.00219    |
|    value_loss           | 0.0171     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0468      |
|    negative_advantag... | 0.40066177  |
|    positive_advantag... | 0.59933823  |
|    prob_ratio           | 10.25313    |
|    rollout_return       | -1.4619956  |
| Time/                   |             |
|    collect_computeV/... | 0.00919     |
|    collect_computeV/Sum | 2.35        |
|    collect_rollout/Mean | 3.09        |
|    collect_rollout/Sum  | 3.09        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00701     |
|    train_loss/Sum       | 22.4        |
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | -368        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 15          |
|    time_elapsed         | 1807        |
|    total_timesteps      | 3840        |
| train/                  |             |
|    active_example       | 245         |
|    approx_kl            | -0.05414799 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | -0.362      |
|    learning_rate        | 0.01        |
|    loss                 | 0.683       |
|    n_updates            | 2800        |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 0.00886     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0653     |
|    negative_advantag... | 0.50624025 |
|    positive_advantag... | 0.49375975 |
|    prob_ratio           | 6.1175904  |
|    rollout_return       | -2.3263106 |
| Time/                   |            |
|    collect_computeV/... | 0.00914    |
|    collect_computeV/Sum | 2.34       |
|    collect_rollout/Mean | 3.07       |
|    collect_rollout/Sum  | 3.07       |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.0069     |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 368        |
|    ep_rew_mean          | -368       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 16         |
|    time_elapsed         | 1937       |
|    total_timesteps      | 4096       |
| train/                  |            |
|    active_example       | 247        |
|    approx_kl            | 0.04352885 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.01       |
|    loss                 | 0.275      |
|    n_updates            | 3000       |
|    policy_gradient_loss | 0.00565    |
|    value_loss           | 0.0351     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0479       |
|    negative_advantag... | 0.3943692    |
|    positive_advantag... | 0.6056307    |
|    prob_ratio           | 12.530647    |
|    rollout_return       | -2.7738068   |
| Time/                   |              |
|    collect_computeV/... | 0.00925      |
|    collect_computeV/Sum | 2.37         |
|    collect_rollout/Mean | 3.11         |
|    collect_rollout/Sum  | 3.11         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 380          |
|    ep_rew_mean          | -380         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 17           |
|    time_elapsed         | 2066         |
|    total_timesteps      | 4352         |
| train/                  |              |
|    active_example       | 249          |
|    approx_kl            | 0.0020009875 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | -9.43        |
|    learning_rate        | 0.01         |
|    loss                 | 0.323        |
|    n_updates            | 3200         |
|    policy_gradient_loss | 0.0033       |
|    value_loss           | 0.00286      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.054        |
|    negative_advantag... | 0.43898907   |
|    positive_advantag... | 0.56099147   |
|    prob_ratio           | 25.16714     |
|    rollout_return       | -2.800848    |
| Time/                   |              |
|    collect_computeV/... | 0.00926      |
|    collect_computeV/Sum | 2.37         |
|    collect_rollout/Mean | 3.12         |
|    collect_rollout/Sum  | 3.12         |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 374          |
|    ep_rew_mean          | -374         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 18           |
|    time_elapsed         | 2194         |
|    total_timesteps      | 4608         |
| train/                  |              |
|    active_example       | 170          |
|    approx_kl            | 0.0058889687 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 3400         |
|    policy_gradient_loss | 0.00745      |
|    value_loss           | 0.0595       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0494     |
|    negative_advantag... | 0.37305573 |
|    positive_advantag... | 0.62694424 |
|    prob_ratio           | 34.09271   |
|    rollout_return       | -2.715345  |
| Time/                   |            |
|    collect_computeV/... | 0.00901    |
|    collect_computeV/Sum | 2.31       |
|    collect_rollout/Mean | 3.04       |
|    collect_rollout/Sum  | 3.04       |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.9       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.0069     |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 374        |
|    ep_rew_mean          | -374       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 19         |
|    time_elapsed         | 2323       |
|    total_timesteps      | 4864       |
| train/                  |            |
|    active_example       | 242        |
|    approx_kl            | 0.11218798 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.428     |
|    explained_variance   | 0.108      |
|    learning_rate        | 0.01       |
|    loss                 | 0.328      |
|    n_updates            | 3600       |
|    policy_gradient_loss | 0.00375    |
|    value_loss           | 0.0316     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.034       |
|    negative_advantag... | 0.268236    |
|    positive_advantag... | 0.7317446   |
|    prob_ratio           | 7.2866735   |
|    rollout_return       | -3.0918064  |
| Time/                   |             |
|    collect_computeV/... | 0.00911     |
|    collect_computeV/Sum | 2.33        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00667     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 384         |
|    ep_rew_mean          | -383        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 20          |
|    time_elapsed         | 2452        |
|    total_timesteps      | 5120        |
| train/                  |             |
|    active_example       | 240         |
|    approx_kl            | 0.086391024 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | -34.6       |
|    learning_rate        | 0.01        |
|    loss                 | 0.233       |
|    n_updates            | 3800        |
|    policy_gradient_loss | 0.00542     |
|    value_loss           | 0.00388     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0608     |
|    negative_advantag... | 0.50750524 |
|    positive_advantag... | 0.4924947  |
|    prob_ratio           | 115.568085 |
|    rollout_return       | -3.1154826 |
| Time/                   |            |
|    collect_computeV/... | 0.00911    |
|    collect_computeV/Sum | 2.33       |
|    collect_rollout/Mean | 3.07       |
|    collect_rollout/Sum  | 3.07       |
|    train_action_adv/... | 0.00664    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00689    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 384        |
|    ep_rew_mean          | -383       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 21         |
|    time_elapsed         | 2581       |
|    total_timesteps      | 5376       |
| train/                  |            |
|    active_example       | 178        |
|    approx_kl            | 0.07758062 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.0637     |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 4000       |
|    policy_gradient_loss | 0.00636    |
|    value_loss           | 0.0972     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0614      |
|    negative_advantag... | 0.54465026  |
|    positive_advantag... | 0.45513535  |
|    prob_ratio           | 209.28894   |
|    rollout_return       | -3.2328374  |
| Time/                   |             |
|    collect_computeV/... | 0.00931     |
|    collect_computeV/Sum | 2.38        |
|    collect_rollout/Mean | 3.13        |
|    collect_rollout/Sum  | 3.13        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00692     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | -392        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 22          |
|    time_elapsed         | 2709        |
|    total_timesteps      | 5632        |
| train/                  |             |
|    active_example       | 249         |
|    approx_kl            | 0.084237024 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | -21.3       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 4200        |
|    policy_gradient_loss | 0.00421     |
|    value_loss           | 0.00596     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0501     |
|    negative_advantag... | 0.41729787 |
|    positive_advantag... | 0.5826632  |
|    prob_ratio           | 32.426567  |
|    rollout_return       | -3.2399936 |
| Time/                   |            |
|    collect_computeV/... | 0.00917    |
|    collect_computeV/Sum | 2.35       |
|    collect_rollout/Mean | 3.08       |
|    collect_rollout/Sum  | 3.08       |
|    train_action_adv/... | 0.00665    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.5       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00692    |
|    train_loss/Sum       | 22.2       |
| rollout/                |            |
|    ep_len_mean          | 392        |
|    ep_rew_mean          | -392       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 23         |
|    time_elapsed         | 2838       |
|    total_timesteps      | 5888       |
| train/                  |            |
|    active_example       | 207        |
|    approx_kl            | 0.04016545 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.0288     |
|    learning_rate        | 0.01       |
|    loss                 | 0.302      |
|    n_updates            | 4400       |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 0.127      |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0441       |
|    negative_advantag... | 0.36399567   |
|    positive_advantag... | 0.6360043    |
|    prob_ratio           | 12.925663    |
|    rollout_return       | -2.9931793   |
| Time/                   |              |
|    collect_computeV/... | 0.00899      |
|    collect_computeV/Sum | 2.3          |
|    collect_rollout/Mean | 3.03         |
|    collect_rollout/Sum  | 3.03         |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0153       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00687      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 399          |
|    ep_rew_mean          | -399         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 24           |
|    time_elapsed         | 2966         |
|    total_timesteps      | 6144         |
| train/                  |              |
|    active_example       | 114          |
|    approx_kl            | -0.034251608 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.285       |
|    explained_variance   | -9.36        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 4600         |
|    policy_gradient_loss | 0.00655      |
|    value_loss           | 0.0125       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0511        |
|    negative_advantag... | 0.43161       |
|    positive_advantag... | 0.56829286    |
|    prob_ratio           | 34.35231      |
|    rollout_return       | -2.9700646    |
| Time/                   |               |
|    collect_computeV/... | 0.00919       |
|    collect_computeV/Sum | 2.35          |
|    collect_rollout/Mean | 3.1           |
|    collect_rollout/Sum  | 3.1           |
|    train_action_adv/... | 0.00665       |
|    train_action_adv/Sum | 21.3          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.8          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.00691       |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 399           |
|    ep_rew_mean          | -399          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 25            |
|    time_elapsed         | 3095          |
|    total_timesteps      | 6400          |
| train/                  |               |
|    active_example       | 240           |
|    approx_kl            | -0.0057356693 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.352        |
|    explained_variance   | 0.0644        |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 4800          |
|    policy_gradient_loss | 0.00621       |
|    value_loss           | 0.028         |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0564      |
|    negative_advantag... | 0.48119473  |
|    positive_advantag... | 0.51847357  |
|    prob_ratio           | 184.46709   |
|    rollout_return       | -3.0566678  |
| Time/                   |             |
|    collect_computeV/... | 0.00917     |
|    collect_computeV/Sum | 2.35        |
|    collect_rollout/Mean | 3.09        |
|    collect_rollout/Sum  | 3.09        |
|    train_action_adv/... | 0.00667     |
|    train_action_adv/Sum | 21.4        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.6        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 406         |
|    ep_rew_mean          | -405        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 26          |
|    time_elapsed         | 3224        |
|    total_timesteps      | 6656        |
| train/                  |             |
|    active_example       | 246         |
|    approx_kl            | 0.040086582 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | -1.72       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 5000        |
|    policy_gradient_loss | 0.000413    |
|    value_loss           | 0.00519     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0432      |
|    negative_advantag... | 0.34693474  |
|    positive_advantag... | 0.65304583  |
|    prob_ratio           | 27.243546   |
|    rollout_return       | -3.1541193  |
| Time/                   |             |
|    collect_computeV/... | 0.00934     |
|    collect_computeV/Sum | 2.39        |
|    collect_rollout/Mean | 3.15        |
|    collect_rollout/Sum  | 3.15        |
|    train_action_adv/... | 0.0066      |
|    train_action_adv/Sum | 21.1        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 406         |
|    ep_rew_mean          | -405        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 27          |
|    time_elapsed         | 3353        |
|    total_timesteps      | 6912        |
| train/                  |             |
|    active_example       | 166         |
|    approx_kl            | 0.079487786 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.0685      |
|    learning_rate        | 0.01        |
|    loss                 | 0.679       |
|    n_updates            | 5200        |
|    policy_gradient_loss | 0.00902     |
|    value_loss           | 0.111       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0713       |
|    negative_advantag... | 0.6225788    |
|    positive_advantag... | 0.37706992   |
|    prob_ratio           | 404.42664    |
|    rollout_return       | -2.992105    |
| Time/                   |              |
|    collect_computeV/... | 0.0092       |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.1          |
|    collect_rollout/Sum  | 3.1          |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.0069       |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 411          |
|    ep_rew_mean          | -411         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 28           |
|    time_elapsed         | 3482         |
|    total_timesteps      | 7168         |
| train/                  |              |
|    active_example       | 239          |
|    approx_kl            | -0.009789757 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | -18.8        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 5400         |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 0.0109       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0481      |
|    negative_advantag... | 0.39882687  |
|    positive_advantag... | 0.6011342   |
|    prob_ratio           | 36.693142   |
|    rollout_return       | -2.9658246  |
| Time/                   |             |
|    collect_computeV/... | 0.00915     |
|    collect_computeV/Sum | 2.34        |
|    collect_rollout/Mean | 3.09        |
|    collect_rollout/Sum  | 3.09        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | -411        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 29          |
|    time_elapsed         | 3611        |
|    total_timesteps      | 7424        |
| train/                  |             |
|    active_example       | 201         |
|    approx_kl            | 0.119225666 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.188       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 5600        |
|    policy_gradient_loss | 0.00231     |
|    value_loss           | 0.0752      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0701     |
|    negative_advantag... | 0.6050174  |
|    positive_advantag... | 0.39482638 |
|    prob_ratio           | 367.8203   |
|    rollout_return       | -2.8131506 |
| Time/                   |            |
|    collect_computeV/... | 0.0092     |
|    collect_computeV/Sum | 2.36       |
|    collect_rollout/Mean | 3.11       |
|    collect_rollout/Sum  | 3.11       |
|    train_action_adv/... | 0.00665    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00691    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | -416       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 30         |
|    time_elapsed         | 3740       |
|    total_timesteps      | 7680       |
| train/                  |            |
|    active_example       | 163        |
|    approx_kl            | 0.03370433 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | -0.92      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 5800       |
|    policy_gradient_loss | 0.00488    |
|    value_loss           | 0.0209     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0619       |
|    negative_advantag... | 0.52808833   |
|    positive_advantag... | 0.47187263   |
|    prob_ratio           | 233.44684    |
|    rollout_return       | -2.920176    |
| Time/                   |              |
|    collect_computeV/... | 0.00927      |
|    collect_computeV/Sum | 2.37         |
|    collect_rollout/Mean | 3.12         |
|    collect_rollout/Sum  | 3.12         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.6         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00692      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 416          |
|    ep_rew_mean          | -416         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 31           |
|    time_elapsed         | 3869         |
|    total_timesteps      | 7936         |
| train/                  |              |
|    active_example       | 247          |
|    approx_kl            | -0.029279213 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.333       |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.01         |
|    loss                 | 0.223        |
|    n_updates            | 6000         |
|    policy_gradient_loss | 0.00464      |
|    value_loss           | 0.0326       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.061        |
|    negative_advantag... | 0.5284928    |
|    positive_advantag... | 0.47123528   |
|    prob_ratio           | 230.358      |
|    rollout_return       | -3.1290185   |
| Time/                   |              |
|    collect_computeV/... | 0.0091       |
|    collect_computeV/Sum | 2.33         |
|    collect_rollout/Mean | 3.07         |
|    collect_rollout/Sum  | 3.07         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.6         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 421          |
|    ep_rew_mean          | -420         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 32           |
|    time_elapsed         | 3998         |
|    total_timesteps      | 8192         |
| train/                  |              |
|    active_example       | 245          |
|    approx_kl            | -0.033285335 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.299       |
|    explained_variance   | -1.39        |
|    learning_rate        | 0.01         |
|    loss                 | 0.226        |
|    n_updates            | 6200         |
|    policy_gradient_loss | 0.00565      |
|    value_loss           | 0.00453      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0628       |
|    negative_advantag... | 0.5287424    |
|    positive_advantag... | 0.4712576    |
|    prob_ratio           | 52.774185    |
|    rollout_return       | -3.2241244   |
| Time/                   |              |
|    collect_computeV/... | 0.0091       |
|    collect_computeV/Sum | 2.33         |
|    collect_rollout/Mean | 3.07         |
|    collect_rollout/Sum  | 3.07         |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0151       |
|    train_computeV/Sum   | 48.4         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.0069       |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 421          |
|    ep_rew_mean          | -420         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 33           |
|    time_elapsed         | 4126         |
|    total_timesteps      | 8448         |
| train/                  |              |
|    active_example       | 158          |
|    approx_kl            | -0.056079727 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.385       |
|    explained_variance   | 0.419        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 6400         |
|    policy_gradient_loss | 0.00601      |
|    value_loss           | 0.0415       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0562     |
|    negative_advantag... | 0.4710558  |
|    positive_advantag... | 0.5289054  |
|    prob_ratio           | 126.630196 |
|    rollout_return       | -3.169097  |
| Time/                   |            |
|    collect_computeV/... | 0.00921    |
|    collect_computeV/Sum | 2.36       |
|    collect_rollout/Mean | 3.1        |
|    collect_rollout/Sum  | 3.1        |
|    train_action_adv/... | 0.00666    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00694    |
|    train_loss/Sum       | 22.2       |
| rollout/                |            |
|    ep_len_mean          | 425        |
|    ep_rew_mean          | -424       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 34         |
|    time_elapsed         | 4255       |
|    total_timesteps      | 8704       |
| train/                  |            |
|    active_example       | 256        |
|    approx_kl            | 0.0471468  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | -6.05      |
|    learning_rate        | 0.01       |
|    loss                 | 0.0105     |
|    n_updates            | 6600       |
|    policy_gradient_loss | 0.00512    |
|    value_loss           | 0.0159     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.037       |
|    negative_advantag... | 0.28828084  |
|    positive_advantag... | 0.71169966  |
|    prob_ratio           | 48.369877   |
|    rollout_return       | -3.0434077  |
| Time/                   |             |
|    collect_computeV/... | 0.00924     |
|    collect_computeV/Sum | 2.37        |
|    collect_rollout/Mean | 3.12        |
|    collect_rollout/Sum  | 3.12        |
|    train_action_adv/... | 0.00662     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00689     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 425         |
|    ep_rew_mean          | -424        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 35          |
|    time_elapsed         | 4384        |
|    total_timesteps      | 8960        |
| train/                  |             |
|    active_example       | 227         |
|    approx_kl            | -0.05714626 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | -0.11       |
|    learning_rate        | 0.01        |
|    loss                 | 0.205       |
|    n_updates            | 6800        |
|    policy_gradient_loss | 0.00339     |
|    value_loss           | 0.091       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0462       |
|    negative_advantag... | 0.378947     |
|    positive_advantag... | 0.62076026   |
|    prob_ratio           | 415.82425    |
|    rollout_return       | -2.7947235   |
| Time/                   |              |
|    collect_computeV/... | 0.00906      |
|    collect_computeV/Sum | 2.32         |
|    collect_rollout/Mean | 3.05         |
|    collect_rollout/Sum  | 3.05         |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00694      |
|    train_loss/Sum       | 22.2         |
| rollout/                |              |
|    ep_len_mean          | 428          |
|    ep_rew_mean          | -428         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 36           |
|    time_elapsed         | 4513         |
|    total_timesteps      | 9216         |
| train/                  |              |
|    active_example       | 108          |
|    approx_kl            | 0.0058465675 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | -1.63        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 7000         |
|    policy_gradient_loss | 0.0048       |
|    value_loss           | 0.0317       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0596       |
|    negative_advantag... | 0.4941743    |
|    positive_advantag... | 0.50553286   |
|    prob_ratio           | 211.01463    |
|    rollout_return       | -2.936717    |
| Time/                   |              |
|    collect_computeV/... | 0.00906      |
|    collect_computeV/Sum | 2.32         |
|    collect_rollout/Mean | 3.05         |
|    collect_rollout/Sum  | 3.05         |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 428          |
|    ep_rew_mean          | -428         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 37           |
|    time_elapsed         | 4642         |
|    total_timesteps      | 9472         |
| train/                  |              |
|    active_example       | 254          |
|    approx_kl            | 0.0029140264 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.361       |
|    explained_variance   | 0.792        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 7200         |
|    policy_gradient_loss | 0.00199      |
|    value_loss           | 0.0096       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0316     |
|    negative_advantag... | 0.24376912 |
|    positive_advantag... | 0.75623095 |
|    prob_ratio           | 13.449424  |
|    rollout_return       | -2.749263  |
| Time/                   |            |
|    collect_computeV/... | 0.00913    |
|    collect_computeV/Sum | 2.34       |
|    collect_rollout/Mean | 3.07       |
|    collect_rollout/Sum  | 3.07       |
|    train_action_adv/... | 0.00661    |
|    train_action_adv/Sum | 21.1       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.6       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00689    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | -431       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 38         |
|    time_elapsed         | 4770       |
|    total_timesteps      | 9728       |
| train/                  |            |
|    active_example       | 246        |
|    approx_kl            | 0.01751794 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | -1.18      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 7400       |
|    policy_gradient_loss | 0.00175    |
|    value_loss           | 0.00784    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0342       |
|    negative_advantag... | 0.264965     |
|    positive_advantag... | 0.7350351    |
|    prob_ratio           | 126.93793    |
|    rollout_return       | -3.15521     |
| Time/                   |              |
|    collect_computeV/... | 0.00913      |
|    collect_computeV/Sum | 2.34         |
|    collect_rollout/Mean | 3.08         |
|    collect_rollout/Sum  | 3.08         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 431          |
|    ep_rew_mean          | -431         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 39           |
|    time_elapsed         | 4899         |
|    total_timesteps      | 9984         |
| train/                  |              |
|    active_example       | 128          |
|    approx_kl            | -0.005905047 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.293       |
|    explained_variance   | 0.47         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 7600         |
|    policy_gradient_loss | 0.0031       |
|    value_loss           | 0.0376       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=10000, episode_reward=-460.80 +/- 48.16
Episode length: 461.20 +/- 47.67
New best mean reward!
------------------------------------------
| HPO/                    |              |
|    margin               | 0.037        |
|    negative_advantag... | 0.2950815    |
|    positive_advantag... | 0.704156     |
|    prob_ratio           | 571.00726    |
|    rollout_return       | -2.888786    |
| Time/                   |              |
|    collect_computeV/... | 0.00929      |
|    collect_computeV/Sum | 2.38         |
|    collect_rollout/Mean | 7.18         |
|    collect_rollout/Sum  | 7.18         |
|    train_action_adv/... | 0.00666      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0153       |
|    train_computeV/Sum   | 48.9         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.0069       |
|    train_loss/Sum       | 22.1         |
| eval/                   |              |
|    mean_ep_length       | 461          |
|    mean_reward          | -461         |
| rollout/                |              |
|    ep_len_mean          | 434          |
|    ep_rew_mean          | -434         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 40           |
|    time_elapsed         | 5032         |
|    total_timesteps      | 10240        |
| train/                  |              |
|    active_example       | 246          |
|    approx_kl            | -0.030767128 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | -3           |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 7800         |
|    policy_gradient_loss | 0.00413      |
|    value_loss           | 0.0115       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0383        |
|    negative_advantag... | 0.31850395    |
|    positive_advantag... | 0.6813058     |
|    prob_ratio           | 235.03139     |
|    rollout_return       | -3.051271     |
| Time/                   |               |
|    collect_computeV/... | 0.00909       |
|    collect_computeV/Sum | 2.33          |
|    collect_rollout/Mean | 3.06          |
|    collect_rollout/Sum  | 3.06          |
|    train_action_adv/... | 0.00666       |
|    train_action_adv/Sum | 21.3          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.6          |
|    train_epoch/Mean     | 125           |
|    train_epoch/Sum      | 125           |
|    train_loss/Mean      | 0.00691       |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 437           |
|    ep_rew_mean          | -437          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 41            |
|    time_elapsed         | 5160          |
|    total_timesteps      | 10496         |
| train/                  |               |
|    active_example       | 224           |
|    approx_kl            | -0.0018955693 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.272        |
|    explained_variance   | 0.478         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 8000          |
|    policy_gradient_loss | 0.002         |
|    value_loss           | 0.0434        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0624     |
|    negative_advantag... | 0.554155   |
|    positive_advantag... | 0.44522017 |
|    prob_ratio           | 26121.21   |
|    rollout_return       | -2.7249215 |
| Time/                   |            |
|    collect_computeV/... | 0.00925    |
|    collect_computeV/Sum | 2.37       |
|    collect_rollout/Mean | 3.11       |
|    collect_rollout/Sum  | 3.11       |
|    train_action_adv/... | 0.00662    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00687    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 428        |
|    ep_rew_mean          | -428       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 42         |
|    time_elapsed         | 5289       |
|    total_timesteps      | 10752      |
| train/                  |            |
|    active_example       | 109        |
|    approx_kl            | 0.05661816 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.233     |
|    explained_variance   | -0.132     |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 8200       |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.0341     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0578      |
|    negative_advantag... | 0.4879149   |
|    positive_advantag... | 0.51190984  |
|    prob_ratio           | 90.66302    |
|    rollout_return       | -2.9265618  |
| Time/                   |             |
|    collect_computeV/... | 0.00918     |
|    collect_computeV/Sum | 2.35        |
|    collect_rollout/Mean | 3.09        |
|    collect_rollout/Sum  | 3.09        |
|    train_action_adv/... | 0.00666     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00691     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 428         |
|    ep_rew_mean          | -428        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 43          |
|    time_elapsed         | 5419        |
|    total_timesteps      | 11008       |
| train/                  |             |
|    active_example       | 249         |
|    approx_kl            | 0.056164503 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.01        |
|    loss                 | 0.272       |
|    n_updates            | 8400        |
|    policy_gradient_loss | 0.00232     |
|    value_loss           | 0.0162      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0515     |
|    negative_advantag... | 0.42363968 |
|    positive_advantag... | 0.5763408  |
|    prob_ratio           | 46.814945  |
|    rollout_return       | -3.2377234 |
| Time/                   |            |
|    collect_computeV/... | 0.0093     |
|    collect_computeV/Sum | 2.38       |
|    collect_rollout/Mean | 3.13       |
|    collect_rollout/Sum  | 3.13       |
|    train_action_adv/... | 0.00665    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00692    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 431        |
|    ep_rew_mean          | -431       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 44         |
|    time_elapsed         | 5548       |
|    total_timesteps      | 11264      |
| train/                  |            |
|    active_example       | 242        |
|    approx_kl            | 0.02146158 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.331     |
|    explained_variance   | -13.1      |
|    learning_rate        | 0.01       |
|    loss                 | 0.214      |
|    n_updates            | 8600       |
|    policy_gradient_loss | 0.00349    |
|    value_loss           | 0.00255    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0522        |
|    negative_advantag... | 0.42998084    |
|    positive_advantag... | 0.56998014    |
|    prob_ratio           | 54.41457      |
|    rollout_return       | -2.8360882    |
| Time/                   |               |
|    collect_computeV/... | 0.00926       |
|    collect_computeV/Sum | 2.37          |
|    collect_rollout/Mean | 3.12          |
|    collect_rollout/Sum  | 3.12          |
|    train_action_adv/... | 0.00661       |
|    train_action_adv/Sum | 21.2          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.7          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.00688       |
|    train_loss/Sum       | 22            |
| rollout/                |               |
|    ep_len_mean          | 431           |
|    ep_rew_mean          | -431          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 45            |
|    time_elapsed         | 5677          |
|    total_timesteps      | 11520         |
| train/                  |               |
|    active_example       | 170           |
|    approx_kl            | -0.0139890425 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.342        |
|    explained_variance   | 0.506         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 8800          |
|    policy_gradient_loss | 0.00331       |
|    value_loss           | 0.0238        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0523        |
|    negative_advantag... | 0.43258044    |
|    positive_advantag... | 0.567361      |
|    prob_ratio           | 139.43657     |
|    rollout_return       | -2.387441     |
| Time/                   |               |
|    collect_computeV/... | 0.00911       |
|    collect_computeV/Sum | 2.33          |
|    collect_rollout/Mean | 3.07          |
|    collect_rollout/Sum  | 3.07          |
|    train_action_adv/... | 0.00666       |
|    train_action_adv/Sum | 21.3          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.6          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.00691       |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 433           |
|    ep_rew_mean          | -433          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 46            |
|    time_elapsed         | 5806          |
|    total_timesteps      | 11776         |
| train/                  |               |
|    active_example       | 240           |
|    approx_kl            | -0.0066144876 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.333        |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 9000          |
|    policy_gradient_loss | 0.00381       |
|    value_loss           | 0.0156        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0694      |
|    negative_advantag... | 0.6143164   |
|    positive_advantag... | 0.3856662   |
|    prob_ratio           | 26.755833   |
|    rollout_return       | -1.4057736  |
| Time/                   |             |
|    collect_computeV/... | 0.00925     |
|    collect_computeV/Sum | 2.37        |
|    collect_rollout/Mean | 3.12        |
|    collect_rollout/Sum  | 3.12        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 125         |
|    train_epoch/Sum      | 125         |
|    train_loss/Mean      | 0.00689     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | -433        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 47          |
|    time_elapsed         | 5934        |
|    total_timesteps      | 12032       |
| train/                  |             |
|    active_example       | 107         |
|    approx_kl            | -0.03147433 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 9200        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 0.0115      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0446     |
|    negative_advantag... | 0.38185218 |
|    positive_advantag... | 0.6171137  |
|    prob_ratio           | 790.46875  |
|    rollout_return       | -2.767702  |
| Time/                   |            |
|    collect_computeV/... | 0.00903    |
|    collect_computeV/Sum | 2.31       |
|    collect_rollout/Mean | 3.04       |
|    collect_rollout/Sum  | 3.04       |
|    train_action_adv/... | 0.00664    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.6       |
|    train_epoch/Mean     | 125        |
|    train_epoch/Sum      | 125        |
|    train_loss/Mean      | 0.00687    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | -436       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 48         |
|    time_elapsed         | 6063       |
|    total_timesteps      | 12288      |
| train/                  |            |
|    active_example       | 96         |
|    approx_kl            | 0.04939083 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.257     |
|    explained_variance   | -0.552     |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 9400       |
|    policy_gradient_loss | 0.002      |
|    value_loss           | 0.0049     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0558      |
|    negative_advantag... | 0.46700817  |
|    positive_advantag... | 0.53057     |
|    prob_ratio           | 92688.77    |
|    rollout_return       | -2.421106   |
| Time/                   |             |
|    collect_computeV/... | 0.00908     |
|    collect_computeV/Sum | 2.32        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00662     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.9        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00686     |
|    train_loss/Sum       | 21.9        |
| rollout/                |             |
|    ep_len_mean          | 436         |
|    ep_rew_mean          | -436        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 49          |
|    time_elapsed         | 6191        |
|    total_timesteps      | 12544       |
| train/                  |             |
|    active_example       | 236         |
|    approx_kl            | 0.017188445 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.01        |
|    loss                 | 0.064       |
|    n_updates            | 9600        |
|    policy_gradient_loss | 0.000401    |
|    value_loss           | 0.0103      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.056       |
|    negative_advantag... | 0.49262956  |
|    positive_advantag... | 0.50697994  |
|    prob_ratio           | 1070.9689   |
|    rollout_return       | -3.0541675  |
| Time/                   |             |
|    collect_computeV/... | 0.00911     |
|    collect_computeV/Sum | 2.33        |
|    collect_rollout/Mean | 3.07        |
|    collect_rollout/Sum  | 3.07        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00691     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 436         |
|    ep_rew_mean          | -436        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 50          |
|    time_elapsed         | 6320        |
|    total_timesteps      | 12800       |
| train/                  |             |
|    active_example       | 246         |
|    approx_kl            | 0.029211804 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | -3.49       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 9800        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 0.00406     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.052      |
|    negative_advantag... | 0.42434132 |
|    positive_advantag... | 0.5710886  |
|    prob_ratio           | 18310.97   |
|    rollout_return       | -2.9117663 |
| Time/                   |            |
|    collect_computeV/... | 0.00937    |
|    collect_computeV/Sum | 2.4        |
|    collect_rollout/Mean | 3.15       |
|    collect_rollout/Sum  | 3.15       |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 49         |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00687    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 436        |
|    ep_rew_mean          | -436       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 51         |
|    time_elapsed         | 6449       |
|    total_timesteps      | 13056      |
| train/                  |            |
|    active_example       | 149        |
|    approx_kl            | 0.04254015 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.01       |
|    loss                 | 0.367      |
|    n_updates            | 10000      |
|    policy_gradient_loss | 0.00223    |
|    value_loss           | 0.0197     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0551       |
|    negative_advantag... | 0.4631909    |
|    positive_advantag... | 0.5361647    |
|    prob_ratio           | 382.8666     |
|    rollout_return       | -2.9475052   |
| Time/                   |              |
|    collect_computeV/... | 0.00949      |
|    collect_computeV/Sum | 2.43         |
|    collect_rollout/Mean | 3.2          |
|    collect_rollout/Sum  | 3.2          |
|    train_action_adv/... | 0.00662      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00692      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 439          |
|    ep_rew_mean          | -438         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 52           |
|    time_elapsed         | 6578         |
|    total_timesteps      | 13312        |
| train/                  |              |
|    active_example       | 250          |
|    approx_kl            | -0.005667515 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.383       |
|    explained_variance   | -2.23        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 10200        |
|    policy_gradient_loss | 0.00125      |
|    value_loss           | 0.00678      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0647     |
|    negative_advantag... | 0.52673537 |
|    positive_advantag... | 0.47303027 |
|    prob_ratio           | 3406.1626  |
|    rollout_return       | -2.8989468 |
| Time/                   |            |
|    collect_computeV/... | 0.00904    |
|    collect_computeV/Sum | 2.31       |
|    collect_rollout/Mean | 3.04       |
|    collect_rollout/Sum  | 3.04       |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00696    |
|    train_loss/Sum       | 22.3       |
| rollout/                |            |
|    ep_len_mean          | 439        |
|    ep_rew_mean          | -438       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 53         |
|    time_elapsed         | 6707       |
|    total_timesteps      | 13568      |
| train/                  |            |
|    active_example       | 230        |
|    approx_kl            | 0.09091572 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 10400      |
|    policy_gradient_loss | 0.00341    |
|    value_loss           | 0.014      |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0317        |
|    negative_advantag... | 0.24618317    |
|    positive_advantag... | 0.7537019     |
|    prob_ratio           | 8.079623      |
|    rollout_return       | -2.9885743    |
| Time/                   |               |
|    collect_computeV/... | 0.00925       |
|    collect_computeV/Sum | 2.37          |
|    collect_rollout/Mean | 3.12          |
|    collect_rollout/Sum  | 3.12          |
|    train_action_adv/... | 0.00664       |
|    train_action_adv/Sum | 21.2          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.7          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.0069        |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 441           |
|    ep_rew_mean          | -440          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 54            |
|    time_elapsed         | 6836          |
|    total_timesteps      | 13824         |
| train/                  |               |
|    active_example       | 121           |
|    approx_kl            | -0.0024427846 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.344        |
|    explained_variance   | -1.3          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 10600         |
|    policy_gradient_loss | 0.00161       |
|    value_loss           | 0.00271       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0581       |
|    negative_advantag... | 0.48925123   |
|    positive_advantag... | 0.51074827   |
|    prob_ratio           | 87.33653     |
|    rollout_return       | -3.1096287   |
| Time/                   |              |
|    collect_computeV/... | 0.00934      |
|    collect_computeV/Sum | 2.39         |
|    collect_rollout/Mean | 3.15         |
|    collect_rollout/Sum  | 3.15         |
|    train_action_adv/... | 0.00663      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0153       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 436          |
|    ep_rew_mean          | -436         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 55           |
|    time_elapsed         | 6965         |
|    total_timesteps      | 14080        |
| train/                  |              |
|    active_example       | 247          |
|    approx_kl            | -0.008763846 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 0.434        |
|    learning_rate        | 0.01         |
|    loss                 | 0.0658       |
|    n_updates            | 10800        |
|    policy_gradient_loss | 0.00809      |
|    value_loss           | 0.0582       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0507       |
|    negative_advantag... | 0.41252178   |
|    positive_advantag... | 0.58734155   |
|    prob_ratio           | 113.75864    |
|    rollout_return       | -3.0398245   |
| Time/                   |              |
|    collect_computeV/... | 0.00905      |
|    collect_computeV/Sum | 2.32         |
|    collect_rollout/Mean | 3.06         |
|    collect_rollout/Sum  | 3.06         |
|    train_action_adv/... | 0.00661      |
|    train_action_adv/Sum | 21.1         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00688      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 433          |
|    ep_rew_mean          | -433         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 56           |
|    time_elapsed         | 7094         |
|    total_timesteps      | 14336        |
| train/                  |              |
|    active_example       | 231          |
|    approx_kl            | -0.053440228 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.379       |
|    explained_variance   | 0.629        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 11000        |
|    policy_gradient_loss | 0.00253      |
|    value_loss           | 0.0291       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0321      |
|    negative_advantag... | 0.2456625   |
|    positive_advantag... | 0.7543376   |
|    prob_ratio           | 13.402466   |
|    rollout_return       | -2.7635298  |
| Time/                   |             |
|    collect_computeV/... | 0.00906     |
|    collect_computeV/Sum | 2.32        |
|    collect_rollout/Mean | 3.05        |
|    collect_rollout/Sum  | 3.05        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | -433        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 57          |
|    time_elapsed         | 7222        |
|    total_timesteps      | 14592       |
| train/                  |             |
|    active_example       | 141         |
|    approx_kl            | 0.019876853 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.01        |
|    loss                 | 0.00885     |
|    n_updates            | 11200       |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 0.0124      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0436       |
|    negative_advantag... | 0.35926682   |
|    positive_advantag... | 0.6381551    |
|    prob_ratio           | 57023.785    |
|    rollout_return       | -3.011521    |
| Time/                   |              |
|    collect_computeV/... | 0.00923      |
|    collect_computeV/Sum | 2.36         |
|    collect_rollout/Mean | 3.1          |
|    collect_rollout/Sum  | 3.1          |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 434          |
|    ep_rew_mean          | -433         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 58           |
|    time_elapsed         | 7351         |
|    total_timesteps      | 14848        |
| train/                  |              |
|    active_example       | 248          |
|    approx_kl            | 0.0041134804 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 11400        |
|    policy_gradient_loss | 0.000254     |
|    value_loss           | 0.0126       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0528       |
|    negative_advantag... | 0.451651     |
|    positive_advantag... | 0.548349     |
|    prob_ratio           | 94.63718     |
|    rollout_return       | -2.0277681   |
| Time/                   |              |
|    collect_computeV/... | 0.00917      |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.09         |
|    collect_rollout/Sum  | 3.09         |
|    train_action_adv/... | 0.00663      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 434          |
|    ep_rew_mean          | -433         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 59           |
|    time_elapsed         | 7480         |
|    total_timesteps      | 15104        |
| train/                  |              |
|    active_example       | 158          |
|    approx_kl            | -0.013990462 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 11600        |
|    policy_gradient_loss | 0.00765      |
|    value_loss           | 0.00665      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0452        |
|    negative_advantag... | 0.37056518    |
|    positive_advantag... | 0.6252185     |
|    prob_ratio           | 4094.0437     |
|    rollout_return       | -2.9058213    |
| Time/                   |               |
|    collect_computeV/... | 0.0093        |
|    collect_computeV/Sum | 2.38          |
|    collect_rollout/Mean | 3.14          |
|    collect_rollout/Sum  | 3.14          |
|    train_action_adv/... | 0.00662       |
|    train_action_adv/Sum | 21.2          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.8          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.0069        |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 426           |
|    ep_rew_mean          | -426          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 60            |
|    time_elapsed         | 7609          |
|    total_timesteps      | 15360         |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | -0.0102638975 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.314        |
|    explained_variance   | -0.588        |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 11800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0054        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0534      |
|    negative_advantag... | 0.4213852   |
|    positive_advantag... | 0.57818645  |
|    prob_ratio           | 85735.66    |
|    rollout_return       | -2.7562757  |
| Time/                   |             |
|    collect_computeV/... | 0.00929     |
|    collect_computeV/Sum | 2.38        |
|    collect_rollout/Mean | 3.13        |
|    collect_rollout/Sum  | 3.13        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00692     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | -426        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 61          |
|    time_elapsed         | 7738        |
|    total_timesteps      | 15616       |
| train/                  |             |
|    active_example       | 244         |
|    approx_kl            | -0.03661872 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.01        |
|    loss                 | 0.296       |
|    n_updates            | 12000       |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 0.0184      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0398       |
|    negative_advantag... | 0.32189932   |
|    positive_advantag... | 0.67560065   |
|    prob_ratio           | 72428.17     |
|    rollout_return       | -2.2702987   |
| Time/                   |              |
|    collect_computeV/... | 0.00927      |
|    collect_computeV/Sum | 2.37         |
|    collect_rollout/Mean | 3.12         |
|    collect_rollout/Sum  | 3.12         |
|    train_action_adv/... | 0.00663      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.6         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 424          |
|    ep_rew_mean          | -424         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 62           |
|    time_elapsed         | 7866         |
|    total_timesteps      | 15872        |
| train/                  |              |
|    active_example       | 233          |
|    approx_kl            | 0.0085987225 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 12200        |
|    policy_gradient_loss | 0.000668     |
|    value_loss           | 0.00884      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0524      |
|    negative_advantag... | 0.4248552   |
|    positive_advantag... | 0.56700045  |
|    prob_ratio           | 115284.11   |
|    rollout_return       | -2.4650092  |
| Time/                   |             |
|    collect_computeV/... | 0.00914     |
|    collect_computeV/Sum | 2.34        |
|    collect_rollout/Mean | 3.08        |
|    collect_rollout/Sum  | 3.08        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | -421        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 63          |
|    time_elapsed         | 7995        |
|    total_timesteps      | 16128       |
| train/                  |             |
|    active_example       | 151         |
|    approx_kl            | 0.004333865 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 12400       |
|    policy_gradient_loss | 0.00847     |
|    value_loss           | 0.011       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0519       |
|    negative_advantag... | 0.4337439    |
|    positive_advantag... | 0.56541634   |
|    prob_ratio           | 87465.11     |
|    rollout_return       | -2.4251008   |
| Time/                   |              |
|    collect_computeV/... | 0.00917      |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.09         |
|    collect_rollout/Sum  | 3.09         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.5         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 417          |
|    ep_rew_mean          | -416         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 64           |
|    time_elapsed         | 8124         |
|    total_timesteps      | 16384        |
| train/                  |              |
|    active_example       | 234          |
|    approx_kl            | -0.027826786 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.756        |
|    learning_rate        | 0.01         |
|    loss                 | 0.054        |
|    n_updates            | 12600        |
|    policy_gradient_loss | 0.0023       |
|    value_loss           | 0.0159       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0401      |
|    negative_advantag... | 0.3186112   |
|    positive_advantag... | 0.68129134  |
|    prob_ratio           | 11785.216   |
|    rollout_return       | -2.1982257  |
| Time/                   |             |
|    collect_computeV/... | 0.00922     |
|    collect_computeV/Sum | 2.36        |
|    collect_rollout/Mean | 3.1         |
|    collect_rollout/Sum  | 3.1         |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.5        |
|    train_epoch/Mean     | 125         |
|    train_epoch/Sum      | 125         |
|    train_loss/Mean      | 0.0069      |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 413         |
|    ep_rew_mean          | -412        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 65          |
|    time_elapsed         | 8252        |
|    total_timesteps      | 16640       |
| train/                  |             |
|    active_example       | 161         |
|    approx_kl            | 0.023201384 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 12800       |
|    policy_gradient_loss | 0.00419     |
|    value_loss           | 0.00795     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.053       |
|    negative_advantag... | 0.4234568   |
|    positive_advantag... | 0.5763896   |
|    prob_ratio           | 157.68164   |
|    rollout_return       | -2.8686717  |
| Time/                   |             |
|    collect_computeV/... | 0.00909     |
|    collect_computeV/Sum | 2.33        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 409         |
|    ep_rew_mean          | -408        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 66          |
|    time_elapsed         | 8381        |
|    total_timesteps      | 16896       |
| train/                  |             |
|    active_example       | 184         |
|    approx_kl            | 0.037219338 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 13000       |
|    policy_gradient_loss | 0.00558     |
|    value_loss           | 0.0154      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0552     |
|    negative_advantag... | 0.44709542 |
|    positive_advantag... | 0.5521038  |
|    prob_ratio           | 403440.7   |
|    rollout_return       | -2.905363  |
| Time/                   |            |
|    collect_computeV/... | 0.00927    |
|    collect_computeV/Sum | 2.37       |
|    collect_rollout/Mean | 3.12       |
|    collect_rollout/Sum  | 3.12       |
|    train_action_adv/... | 0.00661    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00687    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 405        |
|    ep_rew_mean          | -404       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 67         |
|    time_elapsed         | 8510       |
|    total_timesteps      | 17152      |
| train/                  |            |
|    active_example       | 239        |
|    approx_kl            | 0.02639858 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 13200      |
|    policy_gradient_loss | 0.00145    |
|    value_loss           | 0.0182     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.044       |
|    negative_advantag... | 0.3457518   |
|    positive_advantag... | 0.65411156  |
|    prob_ratio           | 25436.35    |
|    rollout_return       | -2.7966132  |
| Time/                   |             |
|    collect_computeV/... | 0.0091      |
|    collect_computeV/Sum | 2.33        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.9        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | -400        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 68          |
|    time_elapsed         | 8639        |
|    total_timesteps      | 17408       |
| train/                  |             |
|    active_example       | 220         |
|    approx_kl            | 0.009696834 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0591      |
|    n_updates            | 13400       |
|    policy_gradient_loss | 0.00763     |
|    value_loss           | 0.0123      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0437       |
|    negative_advantag... | 0.33013493   |
|    positive_advantag... | 0.6678143    |
|    prob_ratio           | 3183.3357    |
|    rollout_return       | -2.6459851   |
| Time/                   |              |
|    collect_computeV/... | 0.00917      |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.09         |
|    collect_rollout/Sum  | 3.09         |
|    train_action_adv/... | 0.00666      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 398          |
|    ep_rew_mean          | -398         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 69           |
|    time_elapsed         | 8767         |
|    total_timesteps      | 17664        |
| train/                  |              |
|    active_example       | 177          |
|    approx_kl            | -0.017565846 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.326       |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.01         |
|    loss                 | 0.47         |
|    n_updates            | 13600        |
|    policy_gradient_loss | 0.00679      |
|    value_loss           | 0.00956      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0412     |
|    negative_advantag... | 0.31724858 |
|    positive_advantag... | 0.6799194  |
|    prob_ratio           | 23.794119  |
|    rollout_return       | -2.6092598 |
| Time/                   |            |
|    collect_computeV/... | 0.0092     |
|    collect_computeV/Sum | 2.36       |
|    collect_rollout/Mean | 3.1        |
|    collect_rollout/Sum  | 3.1        |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00691    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 396        |
|    ep_rew_mean          | -395       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 70         |
|    time_elapsed         | 8896       |
|    total_timesteps      | 17920      |
| train/                  |            |
|    active_example       | 239        |
|    approx_kl            | 0.01940754 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.314     |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 13800      |
|    policy_gradient_loss | 0.000349   |
|    value_loss           | 0.00617    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0283      |
|    negative_advantag... | 0.2199411   |
|    positive_advantag... | 0.771973    |
|    prob_ratio           | 132.76874   |
|    rollout_return       | -2.0119028  |
| Time/                   |             |
|    collect_computeV/... | 0.00921     |
|    collect_computeV/Sum | 2.36        |
|    collect_rollout/Mean | 3.11        |
|    collect_rollout/Sum  | 3.11        |
|    train_action_adv/... | 0.00663     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 395         |
|    ep_rew_mean          | -394        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 71          |
|    time_elapsed         | 9025        |
|    total_timesteps      | 18176       |
| train/                  |             |
|    active_example       | 126         |
|    approx_kl            | 0.021116123 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 14000       |
|    policy_gradient_loss | 0.00133     |
|    value_loss           | 0.0041      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0393        |
|    negative_advantag... | 0.3275482     |
|    positive_advantag... | 0.66555846    |
|    prob_ratio           | 127170.28     |
|    rollout_return       | -2.3306732    |
| Time/                   |               |
|    collect_computeV/... | 0.0091        |
|    collect_computeV/Sum | 2.33          |
|    collect_rollout/Mean | 3.07          |
|    collect_rollout/Sum  | 3.07          |
|    train_action_adv/... | 0.00664       |
|    train_action_adv/Sum | 21.2          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.8          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.00697       |
|    train_loss/Sum       | 22.3          |
| rollout/                |               |
|    ep_len_mean          | 392           |
|    ep_rew_mean          | -391          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 72            |
|    time_elapsed         | 9154          |
|    total_timesteps      | 18432         |
| train/                  |               |
|    active_example       | 134           |
|    approx_kl            | 9.4786286e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.23         |
|    explained_variance   | 0.753         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 14200         |
|    policy_gradient_loss | 0.00322       |
|    value_loss           | 0.0137        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0414      |
|    negative_advantag... | 0.3481979   |
|    positive_advantag... | 0.6466458   |
|    prob_ratio           | 271844.94   |
|    rollout_return       | -2.6066537  |
| Time/                   |             |
|    collect_computeV/... | 0.00938     |
|    collect_computeV/Sum | 2.4         |
|    collect_rollout/Mean | 3.15        |
|    collect_rollout/Sum  | 3.15        |
|    train_action_adv/... | 0.00667     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.6        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00693     |
|    train_loss/Sum       | 22.2        |
| rollout/                |             |
|    ep_len_mean          | 389         |
|    ep_rew_mean          | -389        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 73          |
|    time_elapsed         | 9283        |
|    total_timesteps      | 18688       |
| train/                  |             |
|    active_example       | 233         |
|    approx_kl            | -0.06797841 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 14400       |
|    policy_gradient_loss | 0.00266     |
|    value_loss           | 0.00752     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0368      |
|    negative_advantag... | 0.29537192  |
|    positive_advantag... | 0.70427704  |
|    prob_ratio           | 2000.3817   |
|    rollout_return       | -2.932868   |
| Time/                   |             |
|    collect_computeV/... | 0.00928     |
|    collect_computeV/Sum | 2.37        |
|    collect_rollout/Mean | 3.13        |
|    collect_rollout/Sum  | 3.13        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.6        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00692     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 386         |
|    ep_rew_mean          | -385        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 74          |
|    time_elapsed         | 9412        |
|    total_timesteps      | 18944       |
| train/                  |             |
|    active_example       | 219         |
|    approx_kl            | 0.020941898 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.01        |
|    loss                 | 0.37        |
|    n_updates            | 14600       |
|    policy_gradient_loss | 0.00629     |
|    value_loss           | 0.0143      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0324       |
|    negative_advantag... | 0.25056884   |
|    positive_advantag... | 0.7411501    |
|    prob_ratio           | 80177.555    |
|    rollout_return       | -2.8290086   |
| Time/                   |              |
|    collect_computeV/... | 0.00903      |
|    collect_computeV/Sum | 2.31         |
|    collect_rollout/Mean | 3.04         |
|    collect_rollout/Sum  | 3.04         |
|    train_action_adv/... | 0.00663      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00686      |
|    train_loss/Sum       | 21.9         |
| rollout/                |              |
|    ep_len_mean          | 383          |
|    ep_rew_mean          | -382         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 75           |
|    time_elapsed         | 9541         |
|    total_timesteps      | 19200        |
| train/                  |              |
|    active_example       | 150          |
|    approx_kl            | 0.0016017556 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.01         |
|    loss                 | 0.226        |
|    n_updates            | 14800        |
|    policy_gradient_loss | 0.0037       |
|    value_loss           | 0.0099       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0412     |
|    negative_advantag... | 0.3430177  |
|    positive_advantag... | 0.63750994 |
|    prob_ratio           | 59823.324  |
|    rollout_return       | -2.6058054 |
| Time/                   |            |
|    collect_computeV/... | 0.00919    |
|    collect_computeV/Sum | 2.35       |
|    collect_rollout/Mean | 3.1        |
|    collect_rollout/Sum  | 3.1        |
|    train_action_adv/... | 0.00663    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.8       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00691    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 378        |
|    ep_rew_mean          | -378       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 76         |
|    time_elapsed         | 9669       |
|    total_timesteps      | 19456      |
| train/                  |            |
|    active_example       | 247        |
|    approx_kl            | 0.02580247 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.233     |
|    explained_variance   | 0.926      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 15000      |
|    policy_gradient_loss | 0.000481   |
|    value_loss           | 0.00716    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0259      |
|    negative_advantag... | 0.19891287  |
|    positive_advantag... | 0.7851702   |
|    prob_ratio           | 2065.0684   |
|    rollout_return       | -2.612173   |
| Time/                   |             |
|    collect_computeV/... | 0.00926     |
|    collect_computeV/Sum | 2.37        |
|    collect_rollout/Mean | 3.12        |
|    collect_rollout/Sum  | 3.12        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0153      |
|    train_computeV/Sum   | 48.8        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00691     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 375         |
|    ep_rew_mean          | -374        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 77          |
|    time_elapsed         | 9798        |
|    total_timesteps      | 19712       |
| train/                  |             |
|    active_example       | 197         |
|    approx_kl            | 0.024920098 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 15200       |
|    policy_gradient_loss | 0.000908    |
|    value_loss           | 0.0037      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0334      |
|    negative_advantag... | 0.26115468  |
|    positive_advantag... | 0.7173452   |
|    prob_ratio           | 468537.7    |
|    rollout_return       | -2.2073498  |
| Time/                   |             |
|    collect_computeV/... | 0.00921     |
|    collect_computeV/Sum | 2.36        |
|    collect_rollout/Mean | 3.11        |
|    collect_rollout/Sum  | 3.11        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 125         |
|    train_epoch/Sum      | 125         |
|    train_loss/Mean      | 0.00688     |
|    train_loss/Sum       | 22          |
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | -369        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 78          |
|    time_elapsed         | 9927        |
|    total_timesteps      | 19968       |
| train/                  |             |
|    active_example       | 149         |
|    approx_kl            | 0.030307315 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 15400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00342     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=20000, episode_reward=-209.00 +/- 55.10
Episode length: 210.00 +/- 55.10
New best mean reward!
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0315       |
|    negative_advantag... | 0.25405356   |
|    positive_advantag... | 0.7327433    |
|    prob_ratio           | 1623377.2    |
|    rollout_return       | -2.29595     |
| Time/                   |              |
|    collect_computeV/... | 0.00939      |
|    collect_computeV/Sum | 2.4          |
|    collect_rollout/Mean | 4.98         |
|    collect_rollout/Sum  | 4.98         |
|    train_action_adv/... | 0.00662      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -209         |
| rollout/                |              |
|    ep_len_mean          | 366          |
|    ep_rew_mean          | -365         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 79           |
|    time_elapsed         | 10058        |
|    total_timesteps      | 20224        |
| train/                  |              |
|    active_example       | 233          |
|    approx_kl            | -0.039574504 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 15600        |
|    policy_gradient_loss | 0.000228     |
|    value_loss           | 0.00474      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0378     |
|    negative_advantag... | 0.29904008 |
|    positive_advantag... | 0.66074526 |
|    prob_ratio           | 753981.5   |
|    rollout_return       | -2.7505066 |
| Time/                   |            |
|    collect_computeV/... | 0.00913    |
|    collect_computeV/Sum | 2.34       |
|    collect_rollout/Mean | 3.07       |
|    collect_rollout/Sum  | 3.07       |
|    train_action_adv/... | 0.00665    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.6       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00692    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 364        |
|    ep_rew_mean          | -363       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 80         |
|    time_elapsed         | 10186      |
|    total_timesteps      | 20480      |
| train/                  |            |
|    active_example       | 185        |
|    approx_kl            | 0.02274748 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.185     |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 15800      |
|    policy_gradient_loss | 0.00171    |
|    value_loss           | 0.00597    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0371        |
|    negative_advantag... | 0.31283936    |
|    positive_advantag... | 0.6566334     |
|    prob_ratio           | 1275321.5     |
|    rollout_return       | -2.282209     |
| Time/                   |               |
|    collect_computeV/... | 0.00921       |
|    collect_computeV/Sum | 2.36          |
|    collect_rollout/Mean | 3.1           |
|    collect_rollout/Sum  | 3.1           |
|    train_action_adv/... | 0.00664       |
|    train_action_adv/Sum | 21.3          |
|    train_computeV/Mean  | 0.0152        |
|    train_computeV/Sum   | 48.6          |
|    train_epoch/Mean     | 126           |
|    train_epoch/Sum      | 126           |
|    train_loss/Mean      | 0.0069        |
|    train_loss/Sum       | 22.1          |
| rollout/                |               |
|    ep_len_mean          | 362           |
|    ep_rew_mean          | -361          |
| time/                   |               |
|    fps                  | 2             |
|    iterations           | 81            |
|    time_elapsed         | 10315         |
|    total_timesteps      | 20736         |
| train/                  |               |
|    active_example       | 146           |
|    approx_kl            | -0.0009341389 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.168        |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 16000         |
|    policy_gradient_loss | 0.00236       |
|    value_loss           | 0.00324       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0385       |
|    negative_advantag... | 0.2993808    |
|    positive_advantag... | 0.6423187    |
|    prob_ratio           | 1427404.6    |
|    rollout_return       | -2.2127638   |
| Time/                   |              |
|    collect_computeV/... | 0.00921      |
|    collect_computeV/Sum | 2.36         |
|    collect_rollout/Mean | 3.1          |
|    collect_rollout/Sum  | 3.1          |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0153       |
|    train_computeV/Sum   | 49           |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.0069       |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 360          |
|    ep_rew_mean          | -359         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 82           |
|    time_elapsed         | 10445        |
|    total_timesteps      | 20992        |
| train/                  |              |
|    active_example       | 228          |
|    approx_kl            | 0.0057626218 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 16200        |
|    policy_gradient_loss | 2.62e-05     |
|    value_loss           | 0.00486      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0288       |
|    negative_advantag... | 0.23106812   |
|    positive_advantag... | 0.71471316   |
|    prob_ratio           | 515851.6     |
|    rollout_return       | -2.2060263   |
| Time/                   |              |
|    collect_computeV/... | 0.00919      |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.1          |
|    collect_rollout/Sum  | 3.1          |
|    train_action_adv/... | 0.00664      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.7         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00688      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 359          |
|    ep_rew_mean          | -359         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 83           |
|    time_elapsed         | 10574        |
|    total_timesteps      | 21248        |
| train/                  |              |
|    active_example       | 210          |
|    approx_kl            | -0.028335914 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 16400        |
|    policy_gradient_loss | 0.000969     |
|    value_loss           | 0.00995      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0344      |
|    negative_advantag... | 0.25416085  |
|    positive_advantag... | 0.62837833  |
|    prob_ratio           | 909752.3    |
|    rollout_return       | -1.8759856  |
| Time/                   |             |
|    collect_computeV/... | 0.00922     |
|    collect_computeV/Sum | 2.36        |
|    collect_rollout/Mean | 3.11        |
|    collect_rollout/Sum  | 3.11        |
|    train_action_adv/... | 0.00665     |
|    train_action_adv/Sum | 21.3        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.7        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00691     |
|    train_loss/Sum       | 22.1        |
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | -355        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 84          |
|    time_elapsed         | 10703       |
|    total_timesteps      | 21504       |
| train/                  |             |
|    active_example       | 119         |
|    approx_kl            | 0.014054716 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 16600       |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 0.00783     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0361       |
|    negative_advantag... | 0.29600263   |
|    positive_advantag... | 0.6711849    |
|    prob_ratio           | 2575529.0    |
|    rollout_return       | -2.4683645   |
| Time/                   |              |
|    collect_computeV/... | 0.00916      |
|    collect_computeV/Sum | 2.35         |
|    collect_rollout/Mean | 3.09         |
|    collect_rollout/Sum  | 3.09         |
|    train_action_adv/... | 0.00663      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 355          |
|    ep_rew_mean          | -354         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 85           |
|    time_elapsed         | 10832        |
|    total_timesteps      | 21760        |
| train/                  |              |
|    active_example       | 231          |
|    approx_kl            | -0.008345425 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.807        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 16800        |
|    policy_gradient_loss | 0.000466     |
|    value_loss           | 0.0137       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0385       |
|    negative_advantag... | 0.33269775   |
|    positive_advantag... | 0.5689821    |
|    prob_ratio           | 1325333.2    |
|    rollout_return       | -2.1442564   |
| Time/                   |              |
|    collect_computeV/... | 0.00941      |
|    collect_computeV/Sum | 2.41         |
|    collect_rollout/Mean | 3.17         |
|    collect_rollout/Sum  | 3.17         |
|    train_action_adv/... | 0.00662      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.6         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00691      |
|    train_loss/Sum       | 22.1         |
| rollout/                |              |
|    ep_len_mean          | 351          |
|    ep_rew_mean          | -351         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 86           |
|    time_elapsed         | 10961        |
|    total_timesteps      | 22016        |
| train/                  |              |
|    active_example       | 206          |
|    approx_kl            | -0.010630079 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 17000        |
|    policy_gradient_loss | 0.00088      |
|    value_loss           | 0.0162       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0426      |
|    negative_advantag... | 0.37745112  |
|    positive_advantag... | 0.5536233   |
|    prob_ratio           | 605050.0    |
|    rollout_return       | -2.3667772  |
| Time/                   |             |
|    collect_computeV/... | 0.00907     |
|    collect_computeV/Sum | 2.32        |
|    collect_rollout/Mean | 3.06        |
|    collect_rollout/Sum  | 3.06        |
|    train_action_adv/... | 0.00664     |
|    train_action_adv/Sum | 21.2        |
|    train_computeV/Mean  | 0.0152      |
|    train_computeV/Sum   | 48.6        |
|    train_epoch/Mean     | 126         |
|    train_epoch/Sum      | 126         |
|    train_loss/Mean      | 0.00693     |
|    train_loss/Sum       | 22.2        |
| rollout/                |             |
|    ep_len_mean          | 350         |
|    ep_rew_mean          | -349        |
| time/                   |             |
|    fps                  | 2           |
|    iterations           | 87          |
|    time_elapsed         | 11089       |
|    total_timesteps      | 22272       |
| train/                  |             |
|    active_example       | 138         |
|    approx_kl            | 0.032124072 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 17200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.0198      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0486     |
|    negative_advantag... | 0.41082272 |
|    positive_advantag... | 0.4979664  |
|    prob_ratio           | 190650.62  |
|    rollout_return       | -2.334073  |
| Time/                   |            |
|    collect_computeV/... | 0.00924    |
|    collect_computeV/Sum | 2.36       |
|    collect_rollout/Mean | 3.11       |
|    collect_rollout/Sum  | 3.11       |
|    train_action_adv/... | 0.00662    |
|    train_action_adv/Sum | 21.2       |
|    train_computeV/Mean  | 0.0153     |
|    train_computeV/Sum   | 48.9       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00688    |
|    train_loss/Sum       | 22         |
| rollout/                |            |
|    ep_len_mean          | 346        |
|    ep_rew_mean          | -345       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 88         |
|    time_elapsed         | 11219      |
|    total_timesteps      | 22528      |
| train/                  |            |
|    active_example       | 228        |
|    approx_kl            | 0.07550979 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 17400      |
|    policy_gradient_loss | 0.00205    |
|    value_loss           | 0.0102     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0444     |
|    negative_advantag... | 0.38092    |
|    positive_advantag... | 0.56872857 |
|    prob_ratio           | 1575244.5  |
|    rollout_return       | -2.3632913 |
| Time/                   |            |
|    collect_computeV/... | 0.00906    |
|    collect_computeV/Sum | 2.32       |
|    collect_rollout/Mean | 3.05       |
|    collect_rollout/Sum  | 3.05       |
|    train_action_adv/... | 0.00665    |
|    train_action_adv/Sum | 21.3       |
|    train_computeV/Mean  | 0.0152     |
|    train_computeV/Sum   | 48.7       |
|    train_epoch/Mean     | 126        |
|    train_epoch/Sum      | 126        |
|    train_loss/Mean      | 0.00691    |
|    train_loss/Sum       | 22.1       |
| rollout/                |            |
|    ep_len_mean          | 344        |
|    ep_rew_mean          | -343       |
| time/                   |            |
|    fps                  | 2          |
|    iterations           | 89         |
|    time_elapsed         | 11347      |
|    total_timesteps      | 22784      |
| train/                  |            |
|    active_example       | 169        |
|    approx_kl            | 0.08303771 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.01       |
|    loss                 | 1.67       |
|    n_updates            | 17600      |
|    policy_gradient_loss | 0.00665    |
|    value_loss           | 0.0089     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0367       |
|    negative_advantag... | 0.28768975   |
|    positive_advantag... | 0.6339511    |
|    prob_ratio           | 1618117.1    |
|    rollout_return       | -2.3992376   |
| Time/                   |              |
|    collect_computeV/... | 0.00897      |
|    collect_computeV/Sum | 2.3          |
|    collect_rollout/Mean | 3.02         |
|    collect_rollout/Sum  | 3.02         |
|    train_action_adv/... | 0.00665      |
|    train_action_adv/Sum | 21.3         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.6         |
|    train_epoch/Mean     | 126          |
|    train_epoch/Sum      | 126          |
|    train_loss/Mean      | 0.00697      |
|    train_loss/Sum       | 22.3         |
| rollout/                |              |
|    ep_len_mean          | 342          |
|    ep_rew_mean          | -341         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 90           |
|    time_elapsed         | 11476        |
|    total_timesteps      | 23040        |
| train/                  |              |
|    active_example       | 145          |
|    approx_kl            | 0.0068729743 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.142       |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 17800        |
|    policy_gradient_loss | 0.0013       |
|    value_loss           | 0.00625      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0465       |
|    negative_advantag... | 0.39614528   |
|    positive_advantag... | 0.51139385   |
|    prob_ratio           | 1550837.0    |
|    rollout_return       | -2.0326052   |
| Time/                   |              |
|    collect_computeV/... | 0.00924      |
|    collect_computeV/Sum | 2.36         |
|    collect_rollout/Mean | 3.11         |
|    collect_rollout/Sum  | 3.11         |
|    train_action_adv/... | 0.00662      |
|    train_action_adv/Sum | 21.2         |
|    train_computeV/Mean  | 0.0152       |
|    train_computeV/Sum   | 48.8         |
|    train_epoch/Mean     | 125          |
|    train_epoch/Sum      | 125          |
|    train_loss/Mean      | 0.00689      |
|    train_loss/Sum       | 22           |
| rollout/                |              |
|    ep_len_mean          | 341          |
|    ep_rew_mean          | -340         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 91           |
|    time_elapsed         | 11605        |
|    total_timesteps      | 23296        |
| train/                  |              |
|    active_example       | 235          |
|    approx_kl            | -0.009242661 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.131       |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 18000        |
|    policy_gradient_loss | 0.00049      |
|    value_loss           | 0.00482      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.046        |
|    negative_advantag... | 0.37377056   |
|    positive_advantag... | 0.5105851    |
|    prob_ratio           | 758409.06    |
|    rollout_return       | -2.0611346   |
| Time/                   |              |
|    collect_computeV/... | 0.00921      |
|    collect_computeV/Sum | 2.36         |
|    collect_rollout/Mean | 3.11         |
|    collect_rollout/Sum  | 3.11         |
|    train_action_adv/... | 0.00697      |
|    train_action_adv/Sum | 22.3         |
|    train_computeV/Mean  | 0.0154       |
|    train_computeV/Sum   | 49.4         |
|    train_epoch/Mean     | 129          |
|    train_epoch/Sum      | 129          |
|    train_loss/Mean      | 0.00725      |
|    train_loss/Sum       | 23.2         |
| rollout/                |              |
|    ep_len_mean          | 339          |
|    ep_rew_mean          | -338         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 92           |
|    time_elapsed         | 11737        |
|    total_timesteps      | 23552        |
| train/                  |              |
|    active_example       | 181          |
|    approx_kl            | 0.0016664863 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 18200        |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 0.00411      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0373       |
|    negative_advantag... | 0.26497483   |
|    positive_advantag... | 0.58104086   |
|    prob_ratio           | 1566330.1    |
|    rollout_return       | -2.2350216   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00897      |
|    train_action_adv/Sum | 28.7         |
|    train_computeV/Mean  | 0.0177       |
|    train_computeV/Sum   | 56.5         |
|    train_epoch/Mean     | 152          |
|    train_epoch/Sum      | 152          |
|    train_loss/Mean      | 0.00929      |
|    train_loss/Sum       | 29.7         |
| rollout/                |              |
|    ep_len_mean          | 335          |
|    ep_rew_mean          | -334         |
| time/                   |              |
|    fps                  | 2            |
|    iterations           | 93           |
|    time_elapsed         | 11893        |
|    total_timesteps      | 23808        |
| train/                  |              |
|    active_example       | 130          |
|    approx_kl            | -0.003643319 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 18400        |
|    policy_gradient_loss | 0.00503      |
|    value_loss           | 0.00647      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0438       |
|    negative_advantag... | 0.35319442   |
|    positive_advantag... | 0.56950104   |
|    prob_ratio           | 708452.6     |
|    rollout_return       | -2.2789812   |
| Time/                   |              |
|    collect_computeV/... | 0.0123       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.15         |
|    collect_rollout/Sum  | 4.15         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 333          |
|    ep_rew_mean          | -332         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 94           |
|    time_elapsed         | 12056        |
|    total_timesteps      | 24064        |
| train/                  |              |
|    active_example       | 234          |
|    approx_kl            | -0.019326605 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 18600        |
|    policy_gradient_loss | 0.00112      |
|    value_loss           | 0.0139       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0346      |
|    negative_advantag... | 0.27424496  |
|    positive_advantag... | 0.6652671   |
|    prob_ratio           | 2252204.8   |
|    rollout_return       | -2.4313655  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00956     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | -328        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 95          |
|    time_elapsed         | 12218       |
|    total_timesteps      | 24320       |
| train/                  |             |
|    active_example       | 179         |
|    approx_kl            | 0.027666919 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 18800       |
|    policy_gradient_loss | 0.00262     |
|    value_loss           | 0.00485     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0391      |
|    negative_advantag... | 0.32607567  |
|    positive_advantag... | 0.59902227  |
|    prob_ratio           | 1742917.6   |
|    rollout_return       | -2.216856   |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 327         |
|    ep_rew_mean          | -326        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 96          |
|    time_elapsed         | 12380       |
|    total_timesteps      | 24576       |
| train/                  |             |
|    active_example       | 141         |
|    approx_kl            | -0.04324889 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 19000       |
|    policy_gradient_loss | 0.000322    |
|    value_loss           | 0.00791     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0307       |
|    negative_advantag... | 0.24738292   |
|    positive_advantag... | 0.69039077   |
|    prob_ratio           | 2441883.0    |
|    rollout_return       | -1.8696617   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 159          |
|    train_epoch/Sum      | 159          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 327          |
|    ep_rew_mean          | -326         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 97           |
|    time_elapsed         | 12543        |
|    total_timesteps      | 24832        |
| train/                  |              |
|    active_example       | 243          |
|    approx_kl            | -0.012590602 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 19200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00391      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0275      |
|    negative_advantag... | 0.216288    |
|    positive_advantag... | 0.7081847   |
|    prob_ratio           | 1081221.2   |
|    rollout_return       | -1.3089119  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 58          |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 325         |
|    ep_rew_mean          | -324        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 98          |
|    time_elapsed         | 12704       |
|    total_timesteps      | 25088       |
| train/                  |             |
|    active_example       | 114         |
|    approx_kl            | 0.037571106 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0855     |
|    explained_variance   | 0.884       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 19400       |
|    policy_gradient_loss | 6.25e-05    |
|    value_loss           | 0.00632     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0344       |
|    negative_advantag... | 0.24490431   |
|    positive_advantag... | 0.66179526   |
|    prob_ratio           | 1614616.0    |
|    rollout_return       | -2.2831435   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.23         |
|    collect_rollout/Mean | 4.24         |
|    collect_rollout/Sum  | 4.24         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00981      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 324          |
|    ep_rew_mean          | -323         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 99           |
|    time_elapsed         | 12866        |
|    total_timesteps      | 25344        |
| train/                  |              |
|    active_example       | 149          |
|    approx_kl            | -0.014275514 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 19600        |
|    policy_gradient_loss | 0.00195      |
|    value_loss           | 0.00274      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.034       |
|    negative_advantag... | 0.22668661  |
|    positive_advantag... | 0.57182926  |
|    prob_ratio           | 1285196.1   |
|    rollout_return       | -2.1755755  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 322         |
|    ep_rew_mean          | -321        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 100         |
|    time_elapsed         | 13029       |
|    total_timesteps      | 25600       |
| train/                  |             |
|    active_example       | 229         |
|    approx_kl            | 0.010270551 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 19800       |
|    policy_gradient_loss | 5.47e-05    |
|    value_loss           | 0.00427     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0286        |
|    negative_advantag... | 0.18865162    |
|    positive_advantag... | 0.6716415     |
|    prob_ratio           | 1334753.5     |
|    rollout_return       | -1.9927711    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 319           |
|    ep_rew_mean          | -318          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 101           |
|    time_elapsed         | 13190         |
|    total_timesteps      | 25856         |
| train/                  |               |
|    active_example       | 191           |
|    approx_kl            | -0.0013329685 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.142        |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 20000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00232       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0311      |
|    negative_advantag... | 0.19828525  |
|    positive_advantag... | 0.5747032   |
|    prob_ratio           | 1579014.9   |
|    rollout_return       | -1.9443595  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 318         |
|    ep_rew_mean          | -318        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 102         |
|    time_elapsed         | 13352       |
|    total_timesteps      | 26112       |
| train/                  |             |
|    active_example       | 119         |
|    approx_kl            | -0.01606223 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 20200       |
|    policy_gradient_loss | 0.00231     |
|    value_loss           | 0.0118      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0342     |
|    negative_advantag... | 0.25246173 |
|    positive_advantag... | 0.60298765 |
|    prob_ratio           | 1649132.0  |
|    rollout_return       | -1.8941786 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.2        |
|    collect_rollout/Sum  | 4.2        |
|    train_action_adv/... | 0.00951    |
|    train_action_adv/Sum | 30.4       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.5       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00983    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 317        |
|    ep_rew_mean          | -316       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 103        |
|    time_elapsed         | 13514      |
|    total_timesteps      | 26368      |
| train/                  |            |
|    active_example       | 236        |
|    approx_kl            | 0.03073318 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 20400      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00414    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0291      |
|    negative_advantag... | 0.18038142  |
|    positive_advantag... | 0.6556151   |
|    prob_ratio           | 2426129.0   |
|    rollout_return       | -2.2093456  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00953     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 316         |
|    ep_rew_mean          | -315        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 104         |
|    time_elapsed         | 13676       |
|    total_timesteps      | 26624       |
| train/                  |             |
|    active_example       | 173         |
|    approx_kl            | 0.004907936 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.01        |
|    loss                 | 0.199       |
|    n_updates            | 20600       |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 0.00192     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0362      |
|    negative_advantag... | 0.23907629  |
|    positive_advantag... | 0.5590684   |
|    prob_ratio           | 1333395.1   |
|    rollout_return       | -2.2149358  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00956     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 315         |
|    ep_rew_mean          | -314        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 105         |
|    time_elapsed         | 13838       |
|    total_timesteps      | 26880       |
| train/                  |             |
|    active_example       | 126         |
|    approx_kl            | -0.03085649 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.921       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 20800       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00728     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.048       |
|    negative_advantag... | 0.39101163  |
|    positive_advantag... | 0.4814497   |
|    prob_ratio           | 1346499.4   |
|    rollout_return       | -2.1191301  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.0099      |
|    train_loss/Sum       | 31.7        |
| rollout/                |             |
|    ep_len_mean          | 314         |
|    ep_rew_mean          | -313        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 106         |
|    time_elapsed         | 14000       |
|    total_timesteps      | 27136       |
| train/                  |             |
|    active_example       | 230         |
|    approx_kl            | -0.04051283 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 21000       |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 0.00442     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0368       |
|    negative_advantag... | 0.25979325   |
|    positive_advantag... | 0.57872254   |
|    prob_ratio           | 1748393.1    |
|    rollout_return       | -2.113351    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 311          |
|    ep_rew_mean          | -310         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 107          |
|    time_elapsed         | 14162        |
|    total_timesteps      | 27392        |
| train/                  |              |
|    active_example       | 178          |
|    approx_kl            | 0.0067897737 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.101       |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 21200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00218      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0439       |
|    negative_advantag... | 0.3347077    |
|    positive_advantag... | 0.52451396   |
|    prob_ratio           | 3246168.2    |
|    rollout_return       | -2.4409096   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00979      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 309          |
|    ep_rew_mean          | -309         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 108          |
|    time_elapsed         | 14325        |
|    total_timesteps      | 27648        |
| train/                  |              |
|    active_example       | 136          |
|    approx_kl            | 0.0002142489 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 21400        |
|    policy_gradient_loss | 0.00417      |
|    value_loss           | 0.00574      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0445       |
|    negative_advantag... | 0.3570566    |
|    positive_advantag... | 0.50335395   |
|    prob_ratio           | 2444670.2    |
|    rollout_return       | -2.060547    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00992      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 308          |
|    ep_rew_mean          | -308         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 109          |
|    time_elapsed         | 14487        |
|    total_timesteps      | 27904        |
| train/                  |              |
|    active_example       | 238          |
|    approx_kl            | -0.037498176 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 21600        |
|    policy_gradient_loss | 0.00181      |
|    value_loss           | 0.00311      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0338      |
|    negative_advantag... | 0.24836615  |
|    positive_advantag... | 0.56788397  |
|    prob_ratio           | 2959338.5   |
|    rollout_return       | -2.0756266  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 306         |
|    ep_rew_mean          | -305        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 110         |
|    time_elapsed         | 14649       |
|    total_timesteps      | 28160       |
| train/                  |             |
|    active_example       | 185         |
|    approx_kl            | 0.021726348 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 21800       |
|    policy_gradient_loss | 0.000245    |
|    value_loss           | 0.00246     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0292      |
|    negative_advantag... | 0.19711399  |
|    positive_advantag... | 0.6647416   |
|    prob_ratio           | 1265372.2   |
|    rollout_return       | -2.08028    |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 305         |
|    ep_rew_mean          | -304        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 111         |
|    time_elapsed         | 14810       |
|    total_timesteps      | 28416       |
| train/                  |             |
|    active_example       | 141         |
|    approx_kl            | 0.014499739 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 22000       |
|    policy_gradient_loss | 2.88e-05    |
|    value_loss           | 0.00243     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0419      |
|    negative_advantag... | 0.3065115   |
|    positive_advantag... | 0.55091095  |
|    prob_ratio           | 2994279.2   |
|    rollout_return       | -2.3746538  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 304         |
|    ep_rew_mean          | -303        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 112         |
|    time_elapsed         | 14972       |
|    total_timesteps      | 28672       |
| train/                  |             |
|    active_example       | 245         |
|    approx_kl            | 0.016376548 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 22200       |
|    policy_gradient_loss | 0.000132    |
|    value_loss           | 0.0017      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0342      |
|    negative_advantag... | 0.24429101  |
|    positive_advantag... | 0.57022077  |
|    prob_ratio           | 1528192.0   |
|    rollout_return       | -2.1974707  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 301         |
|    ep_rew_mean          | -300        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 113         |
|    time_elapsed         | 15134       |
|    total_timesteps      | 28928       |
| train/                  |             |
|    active_example       | 186         |
|    approx_kl            | 0.003781423 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 22400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00174     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0304      |
|    negative_advantag... | 0.20281178  |
|    positive_advantag... | 0.61390704  |
|    prob_ratio           | 2373504.2   |
|    rollout_return       | -2.0568223  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.0098      |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 300         |
|    ep_rew_mean          | -299        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 114         |
|    time_elapsed         | 15296       |
|    total_timesteps      | 29184       |
| train/                  |             |
|    active_example       | 132         |
|    approx_kl            | 0.014440611 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 22600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00228     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0362         |
|    negative_advantag... | 0.25644532     |
|    positive_advantag... | 0.56736344     |
|    prob_ratio           | 3365417.0      |
|    rollout_return       | -2.1767638     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.23           |
|    collect_rollout/Mean | 4.24           |
|    collect_rollout/Sum  | 4.24           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 299            |
|    ep_rew_mean          | -298           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 115            |
|    time_elapsed         | 15458          |
|    total_timesteps      | 29440          |
| train/                  |                |
|    active_example       | 244            |
|    approx_kl            | -0.00021453947 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.107         |
|    explained_variance   | 0.989          |
|    learning_rate        | 0.01           |
|    loss                 | 0.0562         |
|    n_updates            | 22800          |
|    policy_gradient_loss | 0.000173       |
|    value_loss           | 0.000846       |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0354       |
|    negative_advantag... | 0.23068734   |
|    positive_advantag... | 0.58042604   |
|    prob_ratio           | 2142566.0    |
|    rollout_return       | -2.0277882   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 296          |
|    ep_rew_mean          | -296         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 116          |
|    time_elapsed         | 15620        |
|    total_timesteps      | 29696        |
| train/                  |              |
|    active_example       | 183          |
|    approx_kl            | 0.0035699904 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0956      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 23000        |
|    policy_gradient_loss | 0.000622     |
|    value_loss           | 0.00166      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.035       |
|    negative_advantag... | 0.23834603  |
|    positive_advantag... | 0.5516765   |
|    prob_ratio           | 4604836.0   |
|    rollout_return       | -2.1887941  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 293         |
|    ep_rew_mean          | -292        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 117         |
|    time_elapsed         | 15783       |
|    total_timesteps      | 29952       |
| train/                  |             |
|    active_example       | 132         |
|    approx_kl            | 0.005065471 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0978     |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 23200       |
|    policy_gradient_loss | 0.00204     |
|    value_loss           | 0.00272     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=30000, episode_reward=-212.80 +/- 19.94
Episode length: 213.80 +/- 19.94
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0346         |
|    negative_advantag... | 0.24306415     |
|    positive_advantag... | 0.5420143      |
|    prob_ratio           | 2698471.2      |
|    rollout_return       | -2.0035715     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 6.18           |
|    collect_rollout/Sum  | 6.18           |
|    train_action_adv/... | 0.00946        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00983        |
|    train_loss/Sum       | 31.5           |
| eval/                   |                |
|    mean_ep_length       | 214            |
|    mean_reward          | -213           |
| rollout/                |                |
|    ep_len_mean          | 287            |
|    ep_rew_mean          | -286           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 118            |
|    time_elapsed         | 15947          |
|    total_timesteps      | 30208          |
| train/                  |                |
|    active_example       | 235            |
|    approx_kl            | -0.00041428208 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.103         |
|    explained_variance   | 0.957          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 23400          |
|    policy_gradient_loss | 9.51e-05       |
|    value_loss           | 0.0036         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0308       |
|    negative_advantag... | 0.22291881   |
|    positive_advantag... | 0.54708135   |
|    prob_ratio           | 2115787.2    |
|    rollout_return       | -2.1589584   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00942      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 285          |
|    ep_rew_mean          | -284         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 119          |
|    time_elapsed         | 16109        |
|    total_timesteps      | 30464        |
| train/                  |              |
|    active_example       | 178          |
|    approx_kl            | -0.011712715 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0914      |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 23600        |
|    policy_gradient_loss | 0.000342     |
|    value_loss           | 0.00399      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0378       |
|    negative_advantag... | 0.27867478   |
|    positive_advantag... | 0.5047629    |
|    prob_ratio           | 2618994.0    |
|    rollout_return       | -1.9883544   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 283          |
|    ep_rew_mean          | -282         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 120          |
|    time_elapsed         | 16272        |
|    total_timesteps      | 30720        |
| train/                  |              |
|    active_example       | 122          |
|    approx_kl            | 0.0031962693 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0884      |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 23800        |
|    policy_gradient_loss | 0.00079      |
|    value_loss           | 0.00208      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0325        |
|    negative_advantag... | 0.23091593    |
|    positive_advantag... | 0.5486936     |
|    prob_ratio           | 4161843.8     |
|    rollout_return       | -2.1254008    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 281           |
|    ep_rew_mean          | -280          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 121           |
|    time_elapsed         | 16433         |
|    total_timesteps      | 30976         |
| train/                  |               |
|    active_example       | 242           |
|    approx_kl            | -0.0015264526 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0983       |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 24000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00409       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0308       |
|    negative_advantag... | 0.18678781   |
|    positive_advantag... | 0.54707986   |
|    prob_ratio           | 2198150.2    |
|    rollout_return       | -2.175642    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 281          |
|    ep_rew_mean          | -280         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 122          |
|    time_elapsed         | 16595        |
|    total_timesteps      | 31232        |
| train/                  |              |
|    active_example       | 172          |
|    approx_kl            | -0.017028272 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0875      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 24200        |
|    policy_gradient_loss | 0.00269      |
|    value_loss           | 0.00233      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0321        |
|    negative_advantag... | 0.22116211    |
|    positive_advantag... | 0.53336924    |
|    prob_ratio           | 2292483.2     |
|    rollout_return       | -1.4471138    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 281           |
|    ep_rew_mean          | -280          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 123           |
|    time_elapsed         | 16757         |
|    total_timesteps      | 31488         |
| train/                  |               |
|    active_example       | 109           |
|    approx_kl            | -0.0004915595 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0865       |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 24400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00407       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0268     |
|    negative_advantag... | 0.1855377  |
|    positive_advantag... | 0.62133753 |
|    prob_ratio           | 1368354.9  |
|    rollout_return       | -1.4454936 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.2        |
|    collect_rollout/Sum  | 4.2        |
|    train_action_adv/... | 0.00947    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.2       |
|    train_epoch/Mean     | 157        |
|    train_epoch/Sum      | 157        |
|    train_loss/Mean      | 0.00982    |
|    train_loss/Sum       | 31.4       |
| rollout/                |            |
|    ep_len_mean          | 278        |
|    ep_rew_mean          | -278       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 124        |
|    time_elapsed         | 16919      |
|    total_timesteps      | 31744      |
| train/                  |            |
|    active_example       | 241        |
|    approx_kl            | 0.01329596 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0895    |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 24600      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.0057     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.028       |
|    negative_advantag... | 0.18497461  |
|    positive_advantag... | 0.49700093  |
|    prob_ratio           | 1489562.9   |
|    rollout_return       | -1.8343827  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 275         |
|    ep_rew_mean          | -274        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 125         |
|    time_elapsed         | 17081       |
|    total_timesteps      | 32000       |
| train/                  |             |
|    active_example       | 170         |
|    approx_kl            | 0.010147586 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0592     |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 24800       |
|    policy_gradient_loss | 0.000788    |
|    value_loss           | 0.0037      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0323       |
|    negative_advantag... | 0.20869882   |
|    positive_advantag... | 0.45175052   |
|    prob_ratio           | 3127369.0    |
|    rollout_return       | -1.98498     |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.22         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 272          |
|    ep_rew_mean          | -271         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 126          |
|    time_elapsed         | 17242        |
|    total_timesteps      | 32256        |
| train/                  |              |
|    active_example       | 107          |
|    approx_kl            | -0.022005886 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0793      |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 25000        |
|    policy_gradient_loss | 0.000596     |
|    value_loss           | 0.00754      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0295       |
|    negative_advantag... | 0.19560249   |
|    positive_advantag... | 0.5471908    |
|    prob_ratio           | 3697199.2    |
|    rollout_return       | -2.205262    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00997      |
|    train_loss/Sum       | 31.9         |
| rollout/                |              |
|    ep_len_mean          | 265          |
|    ep_rew_mean          | -264         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 127          |
|    time_elapsed         | 17405        |
|    total_timesteps      | 32512        |
| train/                  |              |
|    active_example       | 241          |
|    approx_kl            | -0.008913815 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0738      |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 25200        |
|    policy_gradient_loss | 0.000701     |
|    value_loss           | 0.00216      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0318      |
|    negative_advantag... | 0.1815119   |
|    positive_advantag... | 0.52768755  |
|    prob_ratio           | 2921692.8   |
|    rollout_return       | -2.0257752  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00944     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 262         |
|    ep_rew_mean          | -261        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 128         |
|    time_elapsed         | 17567       |
|    total_timesteps      | 32768       |
| train/                  |             |
|    active_example       | 183         |
|    approx_kl            | 0.018942162 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0883     |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 25400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00682     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.026        |
|    negative_advantag... | 0.15511413   |
|    positive_advantag... | 0.53318673   |
|    prob_ratio           | 3003669.5    |
|    rollout_return       | -1.9645351   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.23         |
|    collect_rollout/Mean | 4.25         |
|    collect_rollout/Sum  | 4.25         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 159          |
|    train_epoch/Sum      | 159          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 259          |
|    ep_rew_mean          | -258         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 129          |
|    time_elapsed         | 17730        |
|    total_timesteps      | 33024        |
| train/                  |              |
|    active_example       | 128          |
|    approx_kl            | 0.0029523373 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0946      |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 25600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00179      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0253      |
|    negative_advantag... | 0.15222502  |
|    positive_advantag... | 0.5137321   |
|    prob_ratio           | 3156530.2   |
|    rollout_return       | -1.6917988  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 252         |
|    ep_rew_mean          | -251        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 130         |
|    time_elapsed         | 17892       |
|    total_timesteps      | 33280       |
| train/                  |             |
|    active_example       | 244         |
|    approx_kl            | 0.009325959 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.077      |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 25800       |
|    policy_gradient_loss | 0.000724    |
|    value_loss           | 0.00176     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0293     |
|    negative_advantag... | 0.19017237 |
|    positive_advantag... | 0.5553196  |
|    prob_ratio           | 2501150.5  |
|    rollout_return       | -2.1004171 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.2        |
|    collect_rollout/Sum  | 4.2        |
|    train_action_adv/... | 0.00944    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.3       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00981    |
|    train_loss/Sum       | 31.4       |
| rollout/                |            |
|    ep_len_mean          | 250        |
|    ep_rew_mean          | -249       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 131        |
|    time_elapsed         | 18054      |
|    total_timesteps      | 33536      |
| train/                  |            |
|    active_example       | 169        |
|    approx_kl            | 0.00592009 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0964    |
|    explained_variance   | 0.942      |
|    learning_rate        | 0.01       |
|    loss                 | 0.265      |
|    n_updates            | 26000      |
|    policy_gradient_loss | 0.00376    |
|    value_loss           | 0.00606    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0145        |
|    negative_advantag... | 0.07399761    |
|    positive_advantag... | 0.76270175    |
|    prob_ratio           | 605647.9      |
|    rollout_return       | -0.77293175   |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0099        |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 247           |
|    ep_rew_mean          | -246          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 132           |
|    time_elapsed         | 18216         |
|    total_timesteps      | 33792         |
| train/                  |               |
|    active_example       | 191           |
|    approx_kl            | 0.00038531423 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0409       |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 26200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00275       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0345      |
|    negative_advantag... | 0.23580095  |
|    positive_advantag... | 0.4659769   |
|    prob_ratio           | 3266821.0   |
|    rollout_return       | -2.0111723  |
| Time/                   |             |
|    collect_computeV/... | 0.0123      |
|    collect_computeV/Sum | 3.16        |
|    collect_rollout/Mean | 4.15        |
|    collect_rollout/Sum  | 4.15        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 240         |
|    ep_rew_mean          | -239        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 133         |
|    time_elapsed         | 18378       |
|    total_timesteps      | 34048       |
| train/                  |             |
|    active_example       | 243         |
|    approx_kl            | 0.021919549 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0818     |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 26400       |
|    policy_gradient_loss | 0.000571    |
|    value_loss           | 0.00191     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0314      |
|    negative_advantag... | 0.22726282  |
|    positive_advantag... | 0.52582324  |
|    prob_ratio           | 1659282.1   |
|    rollout_return       | -1.8539966  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 237         |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 134         |
|    time_elapsed         | 18541       |
|    total_timesteps      | 34304       |
| train/                  |             |
|    active_example       | 171         |
|    approx_kl            | 0.017258018 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0844     |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 26600       |
|    policy_gradient_loss | 0.000304    |
|    value_loss           | 0.0021      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0367      |
|    negative_advantag... | 0.2827133   |
|    positive_advantag... | 0.46580258  |
|    prob_ratio           | 2097504.2   |
|    rollout_return       | -1.9945973  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 237         |
|    ep_rew_mean          | -236        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 135         |
|    time_elapsed         | 18702       |
|    total_timesteps      | 34560       |
| train/                  |             |
|    active_example       | 148         |
|    approx_kl            | 0.009124219 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0912     |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 26800       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00102     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0375       |
|    negative_advantag... | 0.27552488   |
|    positive_advantag... | 0.4625221    |
|    prob_ratio           | 1541489.8    |
|    rollout_return       | -1.7051172   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0099       |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 231          |
|    ep_rew_mean          | -230         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 136          |
|    time_elapsed         | 18864        |
|    total_timesteps      | 34816        |
| train/                  |              |
|    active_example       | 237          |
|    approx_kl            | -0.020005018 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.073       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 27000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00203      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0333      |
|    negative_advantag... | 0.23254327  |
|    positive_advantag... | 0.46269187  |
|    prob_ratio           | 1443667.0   |
|    rollout_return       | -1.9921713  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 228         |
|    ep_rew_mean          | -227        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 137         |
|    time_elapsed         | 19026       |
|    total_timesteps      | 35072       |
| train/                  |             |
|    active_example       | 166         |
|    approx_kl            | 0.014638543 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0885     |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 27200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00168     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0301       |
|    negative_advantag... | 0.20286429   |
|    positive_advantag... | 0.51619834   |
|    prob_ratio           | 1825648.1    |
|    rollout_return       | -2.0481622   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.23         |
|    collect_rollout/Sum  | 4.23         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 225          |
|    ep_rew_mean          | -224         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 138          |
|    time_elapsed         | 19188        |
|    total_timesteps      | 35328        |
| train/                  |              |
|    active_example       | 128          |
|    approx_kl            | -0.055642247 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0962      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 27400        |
|    policy_gradient_loss | 0.000267     |
|    value_loss           | 0.00244      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0299      |
|    negative_advantag... | 0.2038602   |
|    positive_advantag... | 0.45789784  |
|    prob_ratio           | 2087939.6   |
|    rollout_return       | -1.896352   |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.24        |
|    collect_rollout/Mean | 4.25        |
|    collect_rollout/Sum  | 4.25        |
|    train_action_adv/... | 0.00954     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 219         |
|    ep_rew_mean          | -218        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 139         |
|    time_elapsed         | 19349       |
|    total_timesteps      | 35584       |
| train/                  |             |
|    active_example       | 236         |
|    approx_kl            | 0.021492973 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0707     |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 27600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00337     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0309      |
|    negative_advantag... | 0.20874617  |
|    positive_advantag... | 0.5234218   |
|    prob_ratio           | 4317549.0   |
|    rollout_return       | -2.0861547  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.24        |
|    collect_rollout/Mean | 4.25        |
|    collect_rollout/Sum  | 4.25        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 218         |
|    ep_rew_mean          | -217        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 140         |
|    time_elapsed         | 19510       |
|    total_timesteps      | 35840       |
| train/                  |             |
|    active_example       | 167         |
|    approx_kl            | 0.010387614 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0767     |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 27800       |
|    policy_gradient_loss | 0.000383    |
|    value_loss           | 0.00592     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0348      |
|    negative_advantag... | 0.25438544  |
|    positive_advantag... | 0.42942324  |
|    prob_ratio           | 2517576.2   |
|    rollout_return       | -1.9144204  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | -215        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 141         |
|    time_elapsed         | 19673       |
|    total_timesteps      | 36096       |
| train/                  |             |
|    active_example       | 129         |
|    approx_kl            | 0.013024375 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.091      |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 28000       |
|    policy_gradient_loss | 0.00231     |
|    value_loss           | 0.00244     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0285       |
|    negative_advantag... | 0.18499006   |
|    positive_advantag... | 0.40911162   |
|    prob_ratio           | 1149493.0    |
|    rollout_return       | -2.0403209   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00987      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 211          |
|    ep_rew_mean          | -210         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 142          |
|    time_elapsed         | 19835        |
|    total_timesteps      | 36352        |
| train/                  |              |
|    active_example       | 235          |
|    approx_kl            | 0.0020542443 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0778      |
|    explained_variance   | 0.834        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 28200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0156       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0318      |
|    negative_advantag... | 0.21793817  |
|    positive_advantag... | 0.5048357   |
|    prob_ratio           | 2434654.5   |
|    rollout_return       | -2.3412633  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | -210        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 143         |
|    time_elapsed         | 19996       |
|    total_timesteps      | 36608       |
| train/                  |             |
|    active_example       | 163         |
|    approx_kl            | -0.01729042 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0935     |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 28400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00859     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0266      |
|    negative_advantag... | 0.14789335  |
|    positive_advantag... | 0.5096653   |
|    prob_ratio           | 1929495.4   |
|    rollout_return       | -1.9437375  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00978     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | -206        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 144         |
|    time_elapsed         | 20158       |
|    total_timesteps      | 36864       |
| train/                  |             |
|    active_example       | 124         |
|    approx_kl            | 0.020316392 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0852     |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 28600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00397     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0281      |
|    negative_advantag... | 0.1807467   |
|    positive_advantag... | 0.5130233   |
|    prob_ratio           | 3922608.8   |
|    rollout_return       | -2.3894584  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00942     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | -206        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 145         |
|    time_elapsed         | 20320       |
|    total_timesteps      | 37120       |
| train/                  |             |
|    active_example       | 240         |
|    approx_kl            | 0.032804117 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.078      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.01        |
|    loss                 | 0.232       |
|    n_updates            | 28800       |
|    policy_gradient_loss | 0.000874    |
|    value_loss           | 0.00178     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0296        |
|    negative_advantag... | 0.20109785    |
|    positive_advantag... | 0.45009366    |
|    prob_ratio           | 1998347.6     |
|    rollout_return       | -1.6990548    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00993       |
|    train_loss/Sum       | 31.8          |
| rollout/                |               |
|    ep_len_mean          | 207           |
|    ep_rew_mean          | -206          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 146           |
|    time_elapsed         | 20482         |
|    total_timesteps      | 37376         |
| train/                  |               |
|    active_example       | 169           |
|    approx_kl            | -0.0024339557 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0584       |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 29000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00517       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0274      |
|    negative_advantag... | 0.19262211  |
|    positive_advantag... | 0.46604988  |
|    prob_ratio           | 3166728.8   |
|    rollout_return       | -1.7987146  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 147         |
|    time_elapsed         | 20644       |
|    total_timesteps      | 37632       |
| train/                  |             |
|    active_example       | 122         |
|    approx_kl            | -0.01295647 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0724     |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 29200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00424     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0249      |
|    negative_advantag... | 0.16620934  |
|    positive_advantag... | 0.55041194  |
|    prob_ratio           | 1902559.5   |
|    rollout_return       | -2.1535196  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | -205        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 148         |
|    time_elapsed         | 20807       |
|    total_timesteps      | 37888       |
| train/                  |             |
|    active_example       | 236         |
|    approx_kl            | 0.008311287 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.071      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 29400       |
|    policy_gradient_loss | 0.000268    |
|    value_loss           | 0.00185     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0263      |
|    negative_advantag... | 0.17855826  |
|    positive_advantag... | 0.37027016  |
|    prob_ratio           | 3138505.0   |
|    rollout_return       | -1.8413504  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 205         |
|    ep_rew_mean          | -204        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 149         |
|    time_elapsed         | 20968       |
|    total_timesteps      | 38144       |
| train/                  |             |
|    active_example       | 197         |
|    approx_kl            | 0.011055514 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0607     |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 29600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00742     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0256      |
|    negative_advantag... | 0.122030765 |
|    positive_advantag... | 0.5366997   |
|    prob_ratio           | 1006020.75  |
|    rollout_return       | -1.9757373  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | -201        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 150         |
|    time_elapsed         | 21130       |
|    total_timesteps      | 38400       |
| train/                  |             |
|    active_example       | 124         |
|    approx_kl            | 0.00785023  |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0723     |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 29800       |
|    policy_gradient_loss | 0.000727    |
|    value_loss           | 0.00253     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0263      |
|    negative_advantag... | 0.16988073  |
|    positive_advantag... | 0.46250218  |
|    prob_ratio           | 2572254.5   |
|    rollout_return       | -1.8935623  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 58          |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00988     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | -201        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 151         |
|    time_elapsed         | 21291       |
|    total_timesteps      | 38656       |
| train/                  |             |
|    active_example       | 232         |
|    approx_kl            | 0.012197182 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0572     |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 30000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00573     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0243       |
|    negative_advantag... | 0.15015413   |
|    positive_advantag... | 0.5159403    |
|    prob_ratio           | 2380803.0    |
|    rollout_return       | -2.1231635   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00987      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 200          |
|    ep_rew_mean          | -199         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 152          |
|    time_elapsed         | 21452        |
|    total_timesteps      | 38912        |
| train/                  |              |
|    active_example       | 168          |
|    approx_kl            | -0.031985886 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0649      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 30200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00169      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0287       |
|    negative_advantag... | 0.17816667   |
|    positive_advantag... | 0.45222408   |
|    prob_ratio           | 2740008.8    |
|    rollout_return       | -1.9827378   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 199          |
|    ep_rew_mean          | -198         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 153          |
|    time_elapsed         | 21614        |
|    total_timesteps      | 39168        |
| train/                  |              |
|    active_example       | 129          |
|    approx_kl            | -0.013518199 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0683      |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 30400        |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 0.00606      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0342        |
|    negative_advantag... | 0.25488997    |
|    positive_advantag... | 0.38737598    |
|    prob_ratio           | 2788959.2     |
|    rollout_return       | -2.1517112    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 199           |
|    ep_rew_mean          | -198          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 154           |
|    time_elapsed         | 21776         |
|    total_timesteps      | 39424         |
| train/                  |               |
|    active_example       | 240           |
|    approx_kl            | -0.0056494772 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.072        |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 30600         |
|    policy_gradient_loss | 0.00121       |
|    value_loss           | 0.00309       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.029        |
|    negative_advantag... | 0.20758243   |
|    positive_advantag... | 0.48149976   |
|    prob_ratio           | 1184884.4    |
|    rollout_return       | -2.0690544   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00981      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 197          |
|    ep_rew_mean          | -196         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 155          |
|    time_elapsed         | 21938        |
|    total_timesteps      | 39680        |
| train/                  |              |
|    active_example       | 159          |
|    approx_kl            | -0.002345994 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0789      |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.01         |
|    loss                 | 0.189        |
|    n_updates            | 30800        |
|    policy_gradient_loss | 0.00136      |
|    value_loss           | 0.00208      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0336       |
|    negative_advantag... | 0.23997372   |
|    positive_advantag... | 0.38555387   |
|    prob_ratio           | 1795419.0    |
|    rollout_return       | -2.0261242   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 197          |
|    ep_rew_mean          | -196         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 156          |
|    time_elapsed         | 22099        |
|    total_timesteps      | 39936        |
| train/                  |              |
|    active_example       | 125          |
|    approx_kl            | -0.027876914 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0615      |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 31000        |
|    policy_gradient_loss | 0.000606     |
|    value_loss           | 0.0038       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=40000, episode_reward=-176.20 +/- 13.66
Episode length: 177.20 +/- 13.66
New best mean reward!
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0368        |
|    negative_advantag... | 0.3034092     |
|    positive_advantag... | 0.442509      |
|    prob_ratio           | 2211225.0     |
|    rollout_return       | -2.2179174    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 5.89          |
|    collect_rollout/Sum  | 5.89          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| eval/                   |               |
|    mean_ep_length       | 177           |
|    mean_reward          | -176          |
| rollout/                |               |
|    ep_len_mean          | 197           |
|    ep_rew_mean          | -196          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 157           |
|    time_elapsed         | 22263         |
|    total_timesteps      | 40192         |
| train/                  |               |
|    active_example       | 244           |
|    approx_kl            | 0.00046491623 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0849       |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 31200         |
|    policy_gradient_loss | 0.00025       |
|    value_loss           | 0.00345       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0256        |
|    negative_advantag... | 0.16627605    |
|    positive_advantag... | 0.44809905    |
|    prob_ratio           | 566865.2      |
|    rollout_return       | -1.8662925    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 197           |
|    ep_rew_mean          | -196          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 158           |
|    time_elapsed         | 22424         |
|    total_timesteps      | 40448         |
| train/                  |               |
|    active_example       | 178           |
|    approx_kl            | 0.00016888976 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0579       |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 31400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00215       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0274       |
|    negative_advantag... | 0.19038405   |
|    positive_advantag... | 0.5133661    |
|    prob_ratio           | 1030106.25   |
|    rollout_return       | -1.5189369   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 196          |
|    ep_rew_mean          | -195         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 159          |
|    time_elapsed         | 22586        |
|    total_timesteps      | 40704        |
| train/                  |              |
|    active_example       | 160          |
|    approx_kl            | 0.0022702068 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.041       |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 31600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00384      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0229       |
|    negative_advantag... | 0.13457508   |
|    positive_advantag... | 0.41866726   |
|    prob_ratio           | 2139656.2    |
|    rollout_return       | -1.8358191   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00987      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 195          |
|    ep_rew_mean          | -194         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 160          |
|    time_elapsed         | 22748        |
|    total_timesteps      | 40960        |
| train/                  |              |
|    active_example       | 226          |
|    approx_kl            | 0.0075119287 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0516      |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 31800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00439      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0307      |
|    negative_advantag... | 0.20186852  |
|    positive_advantag... | 0.4850652   |
|    prob_ratio           | 3880295.8   |
|    rollout_return       | -2.0900214  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00944     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | -193        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 161         |
|    time_elapsed         | 22910       |
|    total_timesteps      | 41216       |
| train/                  |             |
|    active_example       | 141         |
|    approx_kl            | -0.08588806 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0506     |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 32000       |
|    policy_gradient_loss | 0.00177     |
|    value_loss           | 0.00175     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.031         |
|    negative_advantag... | 0.18480541    |
|    positive_advantag... | 0.422558      |
|    prob_ratio           | 1565359.9     |
|    rollout_return       | -1.7326373    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 193           |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 162           |
|    time_elapsed         | 23072         |
|    total_timesteps      | 41472         |
| train/                  |               |
|    active_example       | 133           |
|    approx_kl            | -0.0010955036 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0805       |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 32200         |
|    policy_gradient_loss | 0.000239      |
|    value_loss           | 0.00251       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0308        |
|    negative_advantag... | 0.20952767    |
|    positive_advantag... | 0.4591834     |
|    prob_ratio           | 3671477.8     |
|    rollout_return       | -2.1831825    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 193           |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 163           |
|    time_elapsed         | 23233         |
|    total_timesteps      | 41728         |
| train/                  |               |
|    active_example       | 235           |
|    approx_kl            | -0.0017000586 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0563       |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 32400         |
|    policy_gradient_loss | 0.000506      |
|    value_loss           | 0.00294       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0318        |
|    negative_advantag... | 0.23343419    |
|    positive_advantag... | 0.42633176    |
|    prob_ratio           | 1495767.5     |
|    rollout_return       | -2.0665166    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.00943       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 193           |
|    ep_rew_mean          | -192          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 164           |
|    time_elapsed         | 23395         |
|    total_timesteps      | 41984         |
| train/                  |               |
|    active_example       | 173           |
|    approx_kl            | -4.848838e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0795       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 32600         |
|    policy_gradient_loss | 0.00166       |
|    value_loss           | 0.00261       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0318        |
|    negative_advantag... | 0.21073928    |
|    positive_advantag... | 0.4017218     |
|    prob_ratio           | 1032435.6     |
|    rollout_return       | -2.0871863    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 190           |
|    ep_rew_mean          | -189          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 165           |
|    time_elapsed         | 23557         |
|    total_timesteps      | 42240         |
| train/                  |               |
|    active_example       | 132           |
|    approx_kl            | 4.6491623e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0611       |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 32800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00369       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0339        |
|    negative_advantag... | 0.22964288    |
|    positive_advantag... | 0.37492752    |
|    prob_ratio           | 1885877.1     |
|    rollout_return       | -2.1156347    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 190           |
|    ep_rew_mean          | -189          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 166           |
|    time_elapsed         | 23718         |
|    total_timesteps      | 42496         |
| train/                  |               |
|    active_example       | 232           |
|    approx_kl            | -8.392334e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0627       |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 33000         |
|    policy_gradient_loss | 0.000705      |
|    value_loss           | 0.00523       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0299       |
|    negative_advantag... | 0.19824009   |
|    positive_advantag... | 0.37994367   |
|    prob_ratio           | 1903147.9    |
|    rollout_return       | -2.0086534   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00958      |
|    train_action_adv/Sum | 30.7         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 190          |
|    ep_rew_mean          | -189         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 167          |
|    time_elapsed         | 23880        |
|    total_timesteps      | 42752        |
| train/                  |              |
|    active_example       | 168          |
|    approx_kl            | -0.023915216 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0573      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 33200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00137      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0257       |
|    negative_advantag... | 0.17216748   |
|    positive_advantag... | 0.44875163   |
|    prob_ratio           | 2053517.1    |
|    rollout_return       | -1.306443    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00944      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 190          |
|    ep_rew_mean          | -189         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 168          |
|    time_elapsed         | 24042        |
|    total_timesteps      | 43008        |
| train/                  |              |
|    active_example       | 148          |
|    approx_kl            | 3.963709e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0447      |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 33400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00398      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0279       |
|    negative_advantag... | 0.20226009   |
|    positive_advantag... | 0.5206511    |
|    prob_ratio           | 1789914.1    |
|    rollout_return       | -1.7126406   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 190          |
|    ep_rew_mean          | -189         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 169          |
|    time_elapsed         | 24204        |
|    total_timesteps      | 43264        |
| train/                  |              |
|    active_example       | 249          |
|    approx_kl            | 0.0060782135 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0566      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 33600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00243      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0319      |
|    negative_advantag... | 0.18710804  |
|    positive_advantag... | 0.36810708  |
|    prob_ratio           | 1766578.5   |
|    rollout_return       | -1.8441561  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00988     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | -189        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 170         |
|    time_elapsed         | 24366       |
|    total_timesteps      | 43520       |
| train/                  |             |
|    active_example       | 162         |
|    approx_kl            | 0.011171684 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0531     |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.01        |
|    loss                 | 0.157       |
|    n_updates            | 33800       |
|    policy_gradient_loss | 0.000929    |
|    value_loss           | 0.00133     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0266       |
|    negative_advantag... | 0.16726188   |
|    positive_advantag... | 0.42863673   |
|    prob_ratio           | 1934207.4    |
|    rollout_return       | -1.4150193   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 189          |
|    ep_rew_mean          | -188         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 171          |
|    time_elapsed         | 24528        |
|    total_timesteps      | 43776        |
| train/                  |              |
|    active_example       | 134          |
|    approx_kl            | -0.014522083 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0542      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 34000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00253      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0312      |
|    negative_advantag... | 0.18985891  |
|    positive_advantag... | 0.35787567  |
|    prob_ratio           | 1580258.2   |
|    rollout_return       | -1.9987044  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00977     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | -188        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 172         |
|    time_elapsed         | 24689       |
|    total_timesteps      | 44032       |
| train/                  |             |
|    active_example       | 240         |
|    approx_kl            | 0.007170677 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0625     |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 34200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00155     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0255       |
|    negative_advantag... | 0.17839243   |
|    positive_advantag... | 0.330592     |
|    prob_ratio           | 592148.8     |
|    rollout_return       | -1.8223121   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | -187         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 173          |
|    time_elapsed         | 24851        |
|    total_timesteps      | 44288        |
| train/                  |              |
|    active_example       | 189          |
|    approx_kl            | 0.0016158521 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0633      |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 34400        |
|    policy_gradient_loss | 0.000511     |
|    value_loss           | 0.00529      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0308      |
|    negative_advantag... | 0.20749898  |
|    positive_advantag... | 0.38836056  |
|    prob_ratio           | 2311699.8   |
|    rollout_return       | -2.070898   |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00957     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 174         |
|    time_elapsed         | 25013       |
|    total_timesteps      | 44544       |
| train/                  |             |
|    active_example       | 136         |
|    approx_kl            | 0.014959678 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0618     |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 34600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00301     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0278        |
|    negative_advantag... | 0.17950968    |
|    positive_advantag... | 0.3946701     |
|    prob_ratio           | 621643.06     |
|    rollout_return       | -1.6761514    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 186           |
|    ep_rew_mean          | -185          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 175           |
|    time_elapsed         | 25175         |
|    total_timesteps      | 44800         |
| train/                  |               |
|    active_example       | 228           |
|    approx_kl            | -0.0048389733 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0587       |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 34800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00154       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0336        |
|    negative_advantag... | 0.20798165    |
|    positive_advantag... | 0.3389326     |
|    prob_ratio           | 1237655.5     |
|    rollout_return       | -2.090318     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 186           |
|    ep_rew_mean          | -185          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 176           |
|    time_elapsed         | 25337         |
|    total_timesteps      | 45056         |
| train/                  |               |
|    active_example       | 167           |
|    approx_kl            | -0.0016068518 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0629       |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 35000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000873      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0204        |
|    negative_advantag... | 0.12635568    |
|    positive_advantag... | 0.39536315    |
|    prob_ratio           | 839292.75     |
|    rollout_return       | -1.5996498    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 185           |
|    ep_rew_mean          | -184          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 177           |
|    time_elapsed         | 25499         |
|    total_timesteps      | 45312         |
| train/                  |               |
|    active_example       | 118           |
|    approx_kl            | -0.0070594847 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.064        |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 35200         |
|    policy_gradient_loss | 0.00264       |
|    value_loss           | 0.00328       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0243       |
|    negative_advantag... | 0.17107846   |
|    positive_advantag... | 0.29185134   |
|    prob_ratio           | 1550416.8    |
|    rollout_return       | -1.8582095   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | -185         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 178          |
|    time_elapsed         | 25661        |
|    total_timesteps      | 45568        |
| train/                  |              |
|    active_example       | 236          |
|    approx_kl            | 0.0012832284 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.053       |
|    explained_variance   | 0.859        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 35400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00686      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0328      |
|    negative_advantag... | 0.22612631  |
|    positive_advantag... | 0.27434278  |
|    prob_ratio           | 1190538.4   |
|    rollout_return       | -1.8333701  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00941     |
|    train_action_adv/Sum | 30.1        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 179         |
|    time_elapsed         | 25823       |
|    total_timesteps      | 45824       |
| train/                  |             |
|    active_example       | 168         |
|    approx_kl            | 0.019377284 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0554     |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 35600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00245     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0276       |
|    negative_advantag... | 0.19854197   |
|    positive_advantag... | 0.27489558   |
|    prob_ratio           | 1408885.5    |
|    rollout_return       | -1.7291301   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | -185         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 180          |
|    time_elapsed         | 25985        |
|    total_timesteps      | 46080        |
| train/                  |              |
|    active_example       | 111          |
|    approx_kl            | -0.020468906 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0604      |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 35800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00873      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0355        |
|    negative_advantag... | 0.24632177    |
|    positive_advantag... | 0.31864       |
|    prob_ratio           | 4495800.5     |
|    rollout_return       | -2.167327     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 187           |
|    ep_rew_mean          | -186          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 181           |
|    time_elapsed         | 26147         |
|    total_timesteps      | 46336         |
| train/                  |               |
|    active_example       | 234           |
|    approx_kl            | -0.0020000935 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0661       |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 36000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00255       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0288       |
|    negative_advantag... | 0.20144667   |
|    positive_advantag... | 0.286913     |
|    prob_ratio           | 853976.3     |
|    rollout_return       | -1.8595445   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 187          |
|    ep_rew_mean          | -186         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 182          |
|    time_elapsed         | 26309        |
|    total_timesteps      | 46592        |
| train/                  |              |
|    active_example       | 185          |
|    approx_kl            | -0.014201984 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0486      |
|    explained_variance   | 0.911        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 36200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00654      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0261       |
|    negative_advantag... | 0.17469402   |
|    positive_advantag... | 0.3520252    |
|    prob_ratio           | 856396.75    |
|    rollout_return       | -1.9641489   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 187          |
|    ep_rew_mean          | -186         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 183          |
|    time_elapsed         | 26471        |
|    total_timesteps      | 46848        |
| train/                  |              |
|    active_example       | 136          |
|    approx_kl            | -0.001672864 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0554      |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 36400        |
|    policy_gradient_loss | 0.000528     |
|    value_loss           | 0.00418      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0279      |
|    negative_advantag... | 0.21631123  |
|    positive_advantag... | 0.33859172  |
|    prob_ratio           | 219088.27   |
|    rollout_return       | -1.9793342  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 58          |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00993     |
|    train_loss/Sum       | 31.8        |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 184         |
|    time_elapsed         | 26632       |
|    total_timesteps      | 47104       |
| train/                  |             |
|    active_example       | 226         |
|    approx_kl            | 0.018983163 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0585     |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 36600       |
|    policy_gradient_loss | 0.000498    |
|    value_loss           | 0.0137      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0229         |
|    negative_advantag... | 0.18132885     |
|    positive_advantag... | 0.20742138     |
|    prob_ratio           | 633168.44      |
|    rollout_return       | -1.8295254     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.22           |
|    collect_rollout/Mean | 4.24           |
|    collect_rollout/Sum  | 4.24           |
|    train_action_adv/... | 0.00943        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00981        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 188            |
|    ep_rew_mean          | -187           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 185            |
|    time_elapsed         | 26794          |
|    total_timesteps      | 47360          |
| train/                  |                |
|    active_example       | 180            |
|    approx_kl            | -0.00046521425 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0372        |
|    explained_variance   | 0.853          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 36800          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.0102         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0289        |
|    negative_advantag... | 0.22299957    |
|    positive_advantag... | 0.30828997    |
|    prob_ratio           | 1489830.8     |
|    rollout_return       | -2.0059037    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 189           |
|    ep_rew_mean          | -188          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 186           |
|    time_elapsed         | 26956         |
|    total_timesteps      | 47616         |
| train/                  |               |
|    active_example       | 137           |
|    approx_kl            | -0.0049286336 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0845       |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 37000         |
|    policy_gradient_loss | 0.00232       |
|    value_loss           | 0.0032        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0275       |
|    negative_advantag... | 0.20571887   |
|    positive_advantag... | 0.29279697   |
|    prob_ratio           | 2149691.8    |
|    rollout_return       | -2.2498305   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | -187         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 187          |
|    time_elapsed         | 27118        |
|    total_timesteps      | 47872        |
| train/                  |              |
|    active_example       | 237          |
|    approx_kl            | -0.001332283 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0521      |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 37200        |
|    policy_gradient_loss | 0.000194     |
|    value_loss           | 0.00221      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0275       |
|    negative_advantag... | 0.18591776   |
|    positive_advantag... | 0.3326571    |
|    prob_ratio           | 1705834.5    |
|    rollout_return       | -2.3741426   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | -187         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 188          |
|    time_elapsed         | 27280        |
|    total_timesteps      | 48128        |
| train/                  |              |
|    active_example       | 163          |
|    approx_kl            | -0.005521193 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0723      |
|    explained_variance   | 0.927        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 37400        |
|    policy_gradient_loss | 0.00489      |
|    value_loss           | 0.00519      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0268      |
|    negative_advantag... | 0.20652886  |
|    positive_advantag... | 0.32737747  |
|    prob_ratio           | 935641.7    |
|    rollout_return       | -2.3086686  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 189         |
|    time_elapsed         | 27442       |
|    total_timesteps      | 48384       |
| train/                  |             |
|    active_example       | 131         |
|    approx_kl            | 0.015167624 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0561     |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 37600       |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 0.00827     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0295       |
|    negative_advantag... | 0.20584755   |
|    positive_advantag... | 0.37667215   |
|    prob_ratio           | 1996794.5    |
|    rollout_return       | -2.1963465   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 187          |
|    ep_rew_mean          | -186         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 190          |
|    time_elapsed         | 27604        |
|    total_timesteps      | 48640        |
| train/                  |              |
|    active_example       | 232          |
|    approx_kl            | 0.0113533735 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0862      |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.01         |
|    loss                 | 0.0489       |
|    n_updates            | 37800        |
|    policy_gradient_loss | 0.000289     |
|    value_loss           | 0.00402      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0286        |
|    negative_advantag... | 0.19407009    |
|    positive_advantag... | 0.3630394     |
|    prob_ratio           | 1955490.8     |
|    rollout_return       | -2.389497     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 187           |
|    ep_rew_mean          | -186          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 191           |
|    time_elapsed         | 27765         |
|    total_timesteps      | 48896         |
| train/                  |               |
|    active_example       | 169           |
|    approx_kl            | -0.0004902482 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0675       |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 38000         |
|    policy_gradient_loss | 0.00171       |
|    value_loss           | 0.00175       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0268         |
|    negative_advantag... | 0.1656218      |
|    positive_advantag... | 0.33976924     |
|    prob_ratio           | 1128515.0      |
|    rollout_return       | -1.9499899     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.21           |
|    collect_rollout/Mean | 4.22           |
|    collect_rollout/Sum  | 4.22           |
|    train_action_adv/... | 0.00941        |
|    train_action_adv/Sum | 30.1           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00979        |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 187            |
|    ep_rew_mean          | -186           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 192            |
|    time_elapsed         | 27927          |
|    total_timesteps      | 49152          |
| train/                  |                |
|    active_example       | 123            |
|    approx_kl            | -0.00096334517 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0547        |
|    explained_variance   | 0.951          |
|    learning_rate        | 0.01           |
|    loss                 | 0.0828         |
|    n_updates            | 38200          |
|    policy_gradient_loss | 0.00166        |
|    value_loss           | 0.00345        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0286     |
|    negative_advantag... | 0.19818626 |
|    positive_advantag... | 0.3260912  |
|    prob_ratio           | 1686410.8  |
|    rollout_return       | -1.8244172 |
| Time/                   |            |
|    collect_computeV/... | 0.0126     |
|    collect_computeV/Sum | 3.22       |
|    collect_rollout/Mean | 4.23       |
|    collect_rollout/Sum  | 4.23       |
|    train_action_adv/... | 0.00948    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.5       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00984    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | -186       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 193        |
|    time_elapsed         | 28089      |
|    total_timesteps      | 49408      |
| train/                  |            |
|    active_example       | 235        |
|    approx_kl            | 0.05049792 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0653    |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 38400      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00342    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0214      |
|    negative_advantag... | 0.11748661  |
|    positive_advantag... | 0.39362675  |
|    prob_ratio           | 1812377.2   |
|    rollout_return       | -1.609307   |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.00944     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | -186        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 194         |
|    time_elapsed         | 28251       |
|    total_timesteps      | 49664       |
| train/                  |             |
|    active_example       | 178         |
|    approx_kl            | 0.014646396 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0639     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 38600       |
|    policy_gradient_loss | 7.62e-05    |
|    value_loss           | 0.00383     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0227       |
|    negative_advantag... | 0.115878485  |
|    positive_advantag... | 0.4334191    |
|    prob_ratio           | 1755464.5    |
|    rollout_return       | -2.1525276   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | -185         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 195          |
|    time_elapsed         | 28413        |
|    total_timesteps      | 49920        |
| train/                  |              |
|    active_example       | 137          |
|    approx_kl            | -0.010610163 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0707      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 38800        |
|    policy_gradient_loss | 0.00211      |
|    value_loss           | 0.00134      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=50000, episode_reward=-274.00 +/- 116.33
Episode length: 274.80 +/- 115.94
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0226       |
|    negative_advantag... | 0.14151692   |
|    positive_advantag... | 0.40012378   |
|    prob_ratio           | 1832747.5    |
|    rollout_return       | -1.7045317   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.23         |
|    collect_rollout/Mean | 6.88         |
|    collect_rollout/Sum  | 6.88         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | -274         |
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | -185         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 196          |
|    time_elapsed         | 28578        |
|    total_timesteps      | 50176        |
| train/                  |              |
|    active_example       | 233          |
|    approx_kl            | 0.0011220574 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0629      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 39000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00176      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0259      |
|    negative_advantag... | 0.16108124  |
|    positive_advantag... | 0.36911422  |
|    prob_ratio           | 1101785.9   |
|    rollout_return       | -1.8643397  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.23        |
|    collect_rollout/Mean | 4.25        |
|    collect_rollout/Sum  | 4.25        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00975     |
|    train_loss/Sum       | 31.2        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 197         |
|    time_elapsed         | 28740       |
|    total_timesteps      | 50432       |
| train/                  |             |
|    active_example       | 165         |
|    approx_kl            | 0.017815262 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0649     |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 39200       |
|    policy_gradient_loss | 0.00337     |
|    value_loss           | 0.00128     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0273       |
|    negative_advantag... | 0.18368942   |
|    positive_advantag... | 0.3530099    |
|    prob_ratio           | 1362760.8    |
|    rollout_return       | -1.798003    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 184          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 198          |
|    time_elapsed         | 28902        |
|    total_timesteps      | 50688        |
| train/                  |              |
|    active_example       | 129          |
|    approx_kl            | 0.0013612509 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0649      |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.01         |
|    loss                 | 0.0686       |
|    n_updates            | 39400        |
|    policy_gradient_loss | 0.000909     |
|    value_loss           | 0.00474      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0245      |
|    negative_advantag... | 0.16227014  |
|    positive_advantag... | 0.3811872   |
|    prob_ratio           | 1343819.4   |
|    rollout_return       | -2.1560647  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 199         |
|    time_elapsed         | 29064       |
|    total_timesteps      | 50944       |
| train/                  |             |
|    active_example       | 244         |
|    approx_kl            | 0.005564913 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0637     |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 39600       |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.0022      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0285       |
|    negative_advantag... | 0.19553915   |
|    positive_advantag... | 0.3783087    |
|    prob_ratio           | 1838640.0    |
|    rollout_return       | -2.1206765   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 200          |
|    time_elapsed         | 29227        |
|    total_timesteps      | 51200        |
| train/                  |              |
|    active_example       | 166          |
|    approx_kl            | -0.011038989 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0644      |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 39800        |
|    policy_gradient_loss | 0.00328      |
|    value_loss           | 0.00331      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0234      |
|    negative_advantag... | 0.16998358  |
|    positive_advantag... | 0.38904008  |
|    prob_ratio           | 860174.2    |
|    rollout_return       | -1.6918483  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00991     |
|    train_loss/Sum       | 31.7        |
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | -184        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 201         |
|    time_elapsed         | 29388       |
|    total_timesteps      | 51456       |
| train/                  |             |
|    active_example       | 125         |
|    approx_kl            | 0.013772972 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0634     |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 40000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00424     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.02         |
|    negative_advantag... | 0.10210217   |
|    positive_advantag... | 0.4274682    |
|    prob_ratio           | 501960.7     |
|    rollout_return       | -1.5130174   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 202          |
|    time_elapsed         | 29550        |
|    total_timesteps      | 51712        |
| train/                  |              |
|    active_example       | 243          |
|    approx_kl            | -0.010988802 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.046       |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 40200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00303      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0231       |
|    negative_advantag... | 0.16402504   |
|    positive_advantag... | 0.36785      |
|    prob_ratio           | 1270115.6    |
|    rollout_return       | -1.6541679   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 184          |
|    ep_rew_mean          | -183         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 203          |
|    time_elapsed         | 29711        |
|    total_timesteps      | 51968        |
| train/                  |              |
|    active_example       | 176          |
|    approx_kl            | 0.0097797215 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0704      |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 40400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0022       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.028        |
|    negative_advantag... | 0.19104074   |
|    positive_advantag... | 0.37808427   |
|    prob_ratio           | 976740.94    |
|    rollout_return       | -2.0990863   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 184          |
|    ep_rew_mean          | -183         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 204          |
|    time_elapsed         | 29873        |
|    total_timesteps      | 52224        |
| train/                  |              |
|    active_example       | 121          |
|    approx_kl            | -0.015417665 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0872      |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 40600        |
|    policy_gradient_loss | 0.0012       |
|    value_loss           | 0.00121      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0261      |
|    negative_advantag... | 0.17145027  |
|    positive_advantag... | 0.33458507  |
|    prob_ratio           | 1039785.8   |
|    rollout_return       | -1.9715059  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 205         |
|    time_elapsed         | 30035       |
|    total_timesteps      | 52480       |
| train/                  |             |
|    active_example       | 241         |
|    approx_kl            | -0.03386326 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0657     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 40800       |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 0.00309     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0331      |
|    negative_advantag... | 0.2511502   |
|    positive_advantag... | 0.29640862  |
|    prob_ratio           | 377663.56   |
|    rollout_return       | -1.7673732  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.22        |
|    collect_rollout/Mean | 4.23        |
|    collect_rollout/Sum  | 4.23        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 206         |
|    time_elapsed         | 30197       |
|    total_timesteps      | 52736       |
| train/                  |             |
|    active_example       | 168         |
|    approx_kl            | 0.010035634 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0714     |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 41000       |
|    policy_gradient_loss | 0.00422     |
|    value_loss           | 0.00195     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0335      |
|    negative_advantag... | 0.21888909  |
|    positive_advantag... | 0.33343533  |
|    prob_ratio           | 1027640.0   |
|    rollout_return       | -2.3765955  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | -184        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 207         |
|    time_elapsed         | 30359       |
|    total_timesteps      | 52992       |
| train/                  |             |
|    active_example       | 124         |
|    approx_kl            | 0.029006675 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0827     |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 41200       |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 0.00225     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0229        |
|    negative_advantag... | 0.17128576    |
|    positive_advantag... | 0.29129255    |
|    prob_ratio           | 830588.0      |
|    rollout_return       | -1.7258137    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.23          |
|    collect_rollout/Mean | 4.24          |
|    collect_rollout/Sum  | 4.24          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 184           |
|    ep_rew_mean          | -183          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 208           |
|    time_elapsed         | 30521         |
|    total_timesteps      | 53248         |
| train/                  |               |
|    active_example       | 235           |
|    approx_kl            | -0.0027978122 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0553       |
|    explained_variance   | 0.895         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 41400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00476       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0254        |
|    negative_advantag... | 0.14653637    |
|    positive_advantag... | 0.31510472    |
|    prob_ratio           | 1722968.8     |
|    rollout_return       | -1.8605323    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 184           |
|    ep_rew_mean          | -183          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 209           |
|    time_elapsed         | 30683         |
|    total_timesteps      | 53504         |
| train/                  |               |
|    active_example       | 151           |
|    approx_kl            | -0.0016496778 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0483       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 41600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00207       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0263      |
|    negative_advantag... | 0.17585668  |
|    positive_advantag... | 0.32527623  |
|    prob_ratio           | 948092.25   |
|    rollout_return       | -1.9061956  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 210         |
|    time_elapsed         | 30844       |
|    total_timesteps      | 53760       |
| train/                  |             |
|    active_example       | 133         |
|    approx_kl            | 0.021386534 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0628     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 41800       |
|    policy_gradient_loss | 0.000748    |
|    value_loss           | 0.00261     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0236        |
|    negative_advantag... | 0.16767707    |
|    positive_advantag... | 0.3047841     |
|    prob_ratio           | 763463.25     |
|    rollout_return       | -1.8350273    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 185           |
|    ep_rew_mean          | -184          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 211           |
|    time_elapsed         | 31006         |
|    total_timesteps      | 54016         |
| train/                  |               |
|    active_example       | 241           |
|    approx_kl            | -0.0075514913 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.057        |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 42000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00173       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0217       |
|    negative_advantag... | 0.15657286   |
|    positive_advantag... | 0.25957954   |
|    prob_ratio           | 1253176.2    |
|    rollout_return       | -1.8448803   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 184          |
|    ep_rew_mean          | -183         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 212          |
|    time_elapsed         | 31168        |
|    total_timesteps      | 54272        |
| train/                  |              |
|    active_example       | 185          |
|    approx_kl            | 0.0017954558 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0662      |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 42200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.004        |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0248       |
|    negative_advantag... | 0.17991681   |
|    positive_advantag... | 0.3273099    |
|    prob_ratio           | 2081816.4    |
|    rollout_return       | -2.210388    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 213          |
|    time_elapsed         | 31330        |
|    total_timesteps      | 54528        |
| train/                  |              |
|    active_example       | 131          |
|    approx_kl            | 0.0019859374 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0586      |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 42400        |
|    policy_gradient_loss | 0.00427      |
|    value_loss           | 0.00349      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0205      |
|    negative_advantag... | 0.14800934  |
|    positive_advantag... | 0.30636582  |
|    prob_ratio           | 1591366.8   |
|    rollout_return       | -1.6685643  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.0098      |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 214         |
|    time_elapsed         | 31492       |
|    total_timesteps      | 54784       |
| train/                  |             |
|    active_example       | 236         |
|    approx_kl            | 0.009986997 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0437     |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 42600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.0018      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0213      |
|    negative_advantag... | 0.16111946  |
|    positive_advantag... | 0.32847053  |
|    prob_ratio           | 2466171.2   |
|    rollout_return       | -2.037549   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | -183        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 215         |
|    time_elapsed         | 31654       |
|    total_timesteps      | 55040       |
| train/                  |             |
|    active_example       | 160         |
|    approx_kl            | 0.005356297 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0534     |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 42800       |
|    policy_gradient_loss | 0.000344    |
|    value_loss           | 0.000622    |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0215         |
|    negative_advantag... | 0.1434422      |
|    positive_advantag... | 0.3015776      |
|    prob_ratio           | 1167137.9      |
|    rollout_return       | -1.8921465     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.22           |
|    collect_rollout/Mean | 4.23           |
|    collect_rollout/Sum  | 4.23           |
|    train_action_adv/... | 0.00945        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.6           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00979        |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 184            |
|    ep_rew_mean          | -183           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 216            |
|    time_elapsed         | 31816          |
|    total_timesteps      | 55296          |
| train/                  |                |
|    active_example       | 131            |
|    approx_kl            | -1.6391277e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0581        |
|    explained_variance   | 0.975          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 43000          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00189        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0244        |
|    negative_advantag... | 0.17767136    |
|    positive_advantag... | 0.25039512    |
|    prob_ratio           | 1046021.2     |
|    rollout_return       | -1.8003746    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 185           |
|    ep_rew_mean          | -184          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 217           |
|    time_elapsed         | 31978         |
|    total_timesteps      | 55552         |
| train/                  |               |
|    active_example       | 229           |
|    approx_kl            | 1.8149614e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0505       |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 43200         |
|    policy_gradient_loss | 0.000415      |
|    value_loss           | 0.00261       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0228       |
|    negative_advantag... | 0.1632989    |
|    positive_advantag... | 0.3242209    |
|    prob_ratio           | 3057036.2    |
|    rollout_return       | -2.2282922   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | -184         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 218          |
|    time_elapsed         | 32140        |
|    total_timesteps      | 55808        |
| train/                  |              |
|    active_example       | 156          |
|    approx_kl            | -0.007849917 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0589      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 43400        |
|    policy_gradient_loss | 0.000197     |
|    value_loss           | 0.000919     |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.024      |
|    negative_advantag... | 0.17655058 |
|    positive_advantag... | 0.2549534  |
|    prob_ratio           | 1984056.5  |
|    rollout_return       | -1.9106686 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.18       |
|    collect_rollout/Sum  | 4.18       |
|    train_action_adv/... | 0.00945    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.5       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00979    |
|    train_loss/Sum       | 31.3       |
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | -183       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 219        |
|    time_elapsed         | 32302      |
|    total_timesteps      | 56064      |
| train/                  |            |
|    active_example       | 117        |
|    approx_kl            | 0.02024313 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0527    |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 43600      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00212    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0204       |
|    negative_advantag... | 0.15083757   |
|    positive_advantag... | 0.30545175   |
|    prob_ratio           | 2491491.0    |
|    rollout_return       | -2.1548421   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 220          |
|    time_elapsed         | 32463        |
|    total_timesteps      | 56320        |
| train/                  |              |
|    active_example       | 241          |
|    approx_kl            | 0.0017296076 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0442      |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 43800        |
|    policy_gradient_loss | 0.000765     |
|    value_loss           | 0.00415      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0189       |
|    negative_advantag... | 0.12960774   |
|    positive_advantag... | 0.28980654   |
|    prob_ratio           | 2022361.5    |
|    rollout_return       | -1.809136    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 221          |
|    time_elapsed         | 32626        |
|    total_timesteps      | 56576        |
| train/                  |              |
|    active_example       | 169          |
|    approx_kl            | 0.0061740875 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0417      |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 44000        |
|    policy_gradient_loss | 0.00216      |
|    value_loss           | 0.00168      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0205        |
|    negative_advantag... | 0.1475883     |
|    positive_advantag... | 0.3073534     |
|    prob_ratio           | 3077624.8     |
|    rollout_return       | -2.2318323    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 222           |
|    time_elapsed         | 32788         |
|    total_timesteps      | 56832         |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | -0.0031253994 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0687       |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 44200         |
|    policy_gradient_loss | 0.000929      |
|    value_loss           | 0.000734      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0195       |
|    negative_advantag... | 0.12096664   |
|    positive_advantag... | 0.30922902   |
|    prob_ratio           | 1598314.1    |
|    rollout_return       | -1.7760708   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 223          |
|    time_elapsed         | 32950        |
|    total_timesteps      | 57088        |
| train/                  |              |
|    active_example       | 242          |
|    approx_kl            | 9.536743e-07 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0506      |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 44400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00217      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.018       |
|    negative_advantag... | 0.12526196  |
|    positive_advantag... | 0.28157416  |
|    prob_ratio           | 976341.44   |
|    rollout_return       | -1.9413851  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | -181        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 224         |
|    time_elapsed         | 33112       |
|    total_timesteps      | 57344       |
| train/                  |             |
|    active_example       | 169         |
|    approx_kl            | 0.028256819 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0407     |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 44600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00426     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.018         |
|    negative_advantag... | 0.1143151     |
|    positive_advantag... | 0.3402748     |
|    prob_ratio           | 1208364.4     |
|    rollout_return       | -2.1582248    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 225           |
|    time_elapsed         | 33273         |
|    total_timesteps      | 57600         |
| train/                  |               |
|    active_example       | 135           |
|    approx_kl            | 0.00026401877 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0537       |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 44800         |
|    policy_gradient_loss | 0.000583      |
|    value_loss           | 0.00563       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0164       |
|    negative_advantag... | 0.1070333    |
|    positive_advantag... | 0.327166     |
|    prob_ratio           | 888101.6     |
|    rollout_return       | -1.6641834   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 226          |
|    time_elapsed         | 33435        |
|    total_timesteps      | 57856        |
| train/                  |              |
|    active_example       | 245          |
|    approx_kl            | -0.022618651 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0356      |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 45000        |
|    policy_gradient_loss | 0.00298      |
|    value_loss           | 0.00181      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0443       |
|    negative_advantag... | 0.37963653   |
|    positive_advantag... | 0.23780493   |
|    prob_ratio           | 849192.6     |
|    rollout_return       | -2.460699    |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00978      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 227          |
|    time_elapsed         | 33598        |
|    total_timesteps      | 58112        |
| train/                  |              |
|    active_example       | 190          |
|    approx_kl            | -0.018285707 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.069       |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.01         |
|    loss                 | 1.47         |
|    n_updates            | 45200        |
|    policy_gradient_loss | 0.0371       |
|    value_loss           | 0.000994     |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0196        |
|    negative_advantag... | 0.11417259    |
|    positive_advantag... | 0.29686284    |
|    prob_ratio           | 1389447.2     |
|    rollout_return       | -1.7881972    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 183           |
|    ep_rew_mean          | -182          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 228           |
|    time_elapsed         | 33760         |
|    total_timesteps      | 58368         |
| train/                  |               |
|    active_example       | 134           |
|    approx_kl            | -0.0030809343 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0416       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 45400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00115       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0166        |
|    negative_advantag... | 0.11415404    |
|    positive_advantag... | 0.28533822    |
|    prob_ratio           | 876729.8      |
|    rollout_return       | -1.3943378    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 229           |
|    time_elapsed         | 33922         |
|    total_timesteps      | 58624         |
| train/                  |               |
|    active_example       | 242           |
|    approx_kl            | -6.324053e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0415       |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 45600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00165       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0192       |
|    negative_advantag... | 0.115546495  |
|    positive_advantag... | 0.30371138   |
|    prob_ratio           | 1721732.8    |
|    rollout_return       | -2.078041    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00953      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00987      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | -181         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 230          |
|    time_elapsed         | 34084        |
|    total_timesteps      | 58880        |
| train/                  |              |
|    active_example       | 153          |
|    approx_kl            | -0.013103798 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0465      |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 45800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.000568     |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0193        |
|    negative_advantag... | 0.13750096    |
|    positive_advantag... | 0.2645695     |
|    prob_ratio           | 879745.94     |
|    rollout_return       | -1.7266918    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 231           |
|    time_elapsed         | 34245         |
|    total_timesteps      | 59136         |
| train/                  |               |
|    active_example       | 119           |
|    approx_kl            | 0.00027820468 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0488       |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 46000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00118       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.027        |
|    negative_advantag... | 0.20923136   |
|    positive_advantag... | 0.21145236   |
|    prob_ratio           | 2412694.0    |
|    rollout_return       | -1.8520217   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 232          |
|    time_elapsed         | 34407        |
|    total_timesteps      | 59392        |
| train/                  |              |
|    active_example       | 235          |
|    approx_kl            | -0.001599133 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0486      |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.01         |
|    loss                 | 0.202        |
|    n_updates            | 46200        |
|    policy_gradient_loss | 0.000516     |
|    value_loss           | 0.00211      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0262       |
|    negative_advantag... | 0.18707064   |
|    positive_advantag... | 0.26056635   |
|    prob_ratio           | 1822025.6    |
|    rollout_return       | -2.0901558   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 233          |
|    time_elapsed         | 34569        |
|    total_timesteps      | 59648        |
| train/                  |              |
|    active_example       | 144          |
|    approx_kl            | 0.0064822137 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0571      |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 46400        |
|    policy_gradient_loss | 0.000218     |
|    value_loss           | 0.00132      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0204        |
|    negative_advantag... | 0.12988833    |
|    positive_advantag... | 0.25872505    |
|    prob_ratio           | 1065077.9     |
|    rollout_return       | -1.8757985    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00954       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 182           |
|    ep_rew_mean          | -181          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 234           |
|    time_elapsed         | 34731         |
|    total_timesteps      | 59904         |
| train/                  |               |
|    active_example       | 120           |
|    approx_kl            | -0.0057468414 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0414       |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 46600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0045        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=60000, episode_reward=-202.40 +/- 64.57
Episode length: 203.40 +/- 64.57
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0222      |
|    negative_advantag... | 0.15820564  |
|    positive_advantag... | 0.26351333  |
|    prob_ratio           | 2920615.8   |
|    rollout_return       | -2.20534    |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 6.1         |
|    collect_rollout/Sum  | 6.1         |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | -202        |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | -181        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 235         |
|    time_elapsed         | 34895       |
|    total_timesteps      | 60160       |
| train/                  |             |
|    active_example       | 248         |
|    approx_kl            | 0.014489025 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0473     |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 46800       |
|    policy_gradient_loss | 0.00187     |
|    value_loss           | 0.000885    |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0205      |
|    negative_advantag... | 0.14643167  |
|    positive_advantag... | 0.2549945   |
|    prob_ratio           | 1147123.4   |
|    rollout_return       | -2.1061606  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | -181        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 236         |
|    time_elapsed         | 35057       |
|    total_timesteps      | 60416       |
| train/                  |             |
|    active_example       | 165         |
|    approx_kl            | 0.004394859 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0459     |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 47000       |
|    policy_gradient_loss | 0.00191     |
|    value_loss           | 0.004       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0181       |
|    negative_advantag... | 0.1249442    |
|    positive_advantag... | 0.26333717   |
|    prob_ratio           | 398949.25    |
|    rollout_return       | -1.7318561   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00953      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | -181         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 237          |
|    time_elapsed         | 35220        |
|    total_timesteps      | 60672        |
| train/                  |              |
|    active_example       | 114          |
|    approx_kl            | 0.0023483336 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 47200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00302      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0251      |
|    negative_advantag... | 0.17624502  |
|    positive_advantag... | 0.22475117  |
|    prob_ratio           | 1140274.1   |
|    rollout_return       | -1.7999272  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | -180        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 238         |
|    time_elapsed         | 35382       |
|    total_timesteps      | 60928       |
| train/                  |             |
|    active_example       | 245         |
|    approx_kl            | 0.019003972 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0451     |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 47400       |
|    policy_gradient_loss | 0.000798    |
|    value_loss           | 0.00269     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0175        |
|    negative_advantag... | 0.118594      |
|    positive_advantag... | 0.24162097    |
|    prob_ratio           | 925829.2      |
|    rollout_return       | -1.802946     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 181           |
|    ep_rew_mean          | -180          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 239           |
|    time_elapsed         | 35543         |
|    total_timesteps      | 61184         |
| train/                  |               |
|    active_example       | 161           |
|    approx_kl            | -0.0037697554 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0476       |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 47600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00317       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0208       |
|    negative_advantag... | 0.14524455   |
|    positive_advantag... | 0.21188468   |
|    prob_ratio           | 928930.2     |
|    rollout_return       | -1.9593555   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | -180         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 240          |
|    time_elapsed         | 35705        |
|    total_timesteps      | 61440        |
| train/                  |              |
|    active_example       | 125          |
|    approx_kl            | -0.012952372 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0332      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 47800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00117      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0152      |
|    negative_advantag... | 0.091538355 |
|    positive_advantag... | 0.25291508  |
|    prob_ratio           | 562393.06   |
|    rollout_return       | -1.5942537  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | -180        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 241         |
|    time_elapsed         | 35867       |
|    total_timesteps      | 61696       |
| train/                  |             |
|    active_example       | 245         |
|    approx_kl            | 0.014623076 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0393     |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 48000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00229     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0237         |
|    negative_advantag... | 0.1519353      |
|    positive_advantag... | 0.24685383     |
|    prob_ratio           | 1128451.1      |
|    rollout_return       | -2.1575313     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00948        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.6           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 180            |
|    ep_rew_mean          | -179           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 242            |
|    time_elapsed         | 36029          |
|    total_timesteps      | 61952          |
| train/                  |                |
|    active_example       | 153            |
|    approx_kl            | -0.00014173985 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0546        |
|    explained_variance   | 0.99           |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 48200          |
|    policy_gradient_loss | 0.00252        |
|    value_loss           | 0.000531       |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0144       |
|    negative_advantag... | 0.085073     |
|    positive_advantag... | 0.27915096   |
|    prob_ratio           | 619612.56    |
|    rollout_return       | -1.7071655   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | -180         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 243          |
|    time_elapsed         | 36191        |
|    total_timesteps      | 62208        |
| train/                  |              |
|    active_example       | 133          |
|    approx_kl            | 0.0081962645 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0356      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.01         |
|    loss                 | 0.194        |
|    n_updates            | 48400        |
|    policy_gradient_loss | 0.00081      |
|    value_loss           | 0.00166      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.021        |
|    negative_advantag... | 0.15903056   |
|    positive_advantag... | 0.26788354   |
|    prob_ratio           | 640227.8     |
|    rollout_return       | -2.0795026   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | -180         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 244          |
|    time_elapsed         | 36353        |
|    total_timesteps      | 62464        |
| train/                  |              |
|    active_example       | 241          |
|    approx_kl            | 0.0003502965 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0454      |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 48600        |
|    policy_gradient_loss | 0.00224      |
|    value_loss           | 0.00092      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0168     |
|    negative_advantag... | 0.10885328 |
|    positive_advantag... | 0.27708438 |
|    prob_ratio           | 1204015.1  |
|    rollout_return       | -1.7875354 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.21       |
|    collect_rollout/Mean | 4.22       |
|    collect_rollout/Sum  | 4.22       |
|    train_action_adv/... | 0.00947    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.3       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00981    |
|    train_loss/Sum       | 31.4       |
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | -179       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 245        |
|    time_elapsed         | 36515      |
|    total_timesteps      | 62720      |
| train/                  |            |
|    active_example       | 146        |
|    approx_kl            | 0.01244095 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0311    |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 48800      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00087    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0193       |
|    negative_advantag... | 0.13053335   |
|    positive_advantag... | 0.2613222    |
|    prob_ratio           | 2274520.0    |
|    rollout_return       | -1.9379879   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00944      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 180          |
|    ep_rew_mean          | -179         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 246          |
|    time_elapsed         | 36677        |
|    total_timesteps      | 62976        |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | 0.0036305487 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0468      |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 49000        |
|    policy_gradient_loss | 0.00399      |
|    value_loss           | 0.00694      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0244        |
|    negative_advantag... | 0.18991509    |
|    positive_advantag... | 0.22801483    |
|    prob_ratio           | 983202.06     |
|    rollout_return       | -2.1797245    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00992       |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 178           |
|    ep_rew_mean          | -177          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 247           |
|    time_elapsed         | 36839         |
|    total_timesteps      | 63232         |
| train/                  |               |
|    active_example       | 249           |
|    approx_kl            | 5.2511692e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0433       |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 49200         |
|    policy_gradient_loss | 0.00058       |
|    value_loss           | 0.00283       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0188       |
|    negative_advantag... | 0.15134872   |
|    positive_advantag... | 0.2276359    |
|    prob_ratio           | 1461419.2    |
|    rollout_return       | -2.0096412   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 178          |
|    ep_rew_mean          | -177         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 248          |
|    time_elapsed         | 37001        |
|    total_timesteps      | 63488        |
| train/                  |              |
|    active_example       | 157          |
|    approx_kl            | -0.008020133 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.03        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 49400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00481      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0221         |
|    negative_advantag... | 0.17807193     |
|    positive_advantag... | 0.19612733     |
|    prob_ratio           | 469796.6       |
|    rollout_return       | -1.6137342     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.17           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00978        |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 177            |
|    ep_rew_mean          | -176           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 249            |
|    time_elapsed         | 37163          |
|    total_timesteps      | 63744          |
| train/                  |                |
|    active_example       | 121            |
|    approx_kl            | -0.00035092235 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0371        |
|    explained_variance   | 0.953          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 49600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00212        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0194     |
|    negative_advantag... | 0.13032609 |
|    positive_advantag... | 0.24104126 |
|    prob_ratio           | 812363.94  |
|    rollout_return       | -1.8565277 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.2        |
|    collect_rollout/Sum  | 4.2        |
|    train_action_adv/... | 0.00947    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.3       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00986    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 250        |
|    time_elapsed         | 37325      |
|    total_timesteps      | 64000      |
| train/                  |            |
|    active_example       | 246        |
|    approx_kl            | 0.02041009 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0494    |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 49800      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.000785   |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0221       |
|    negative_advantag... | 0.15996821   |
|    positive_advantag... | 0.22657491   |
|    prob_ratio           | 1590063.6    |
|    rollout_return       | -1.7167162   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 176          |
|    ep_rew_mean          | -175         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 251          |
|    time_elapsed         | 37488        |
|    total_timesteps      | 64256        |
| train/                  |              |
|    active_example       | 177          |
|    approx_kl            | -0.080862045 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0383      |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 50000        |
|    policy_gradient_loss | 0.000708     |
|    value_loss           | 0.0104       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0175        |
|    negative_advantag... | 0.1264123     |
|    positive_advantag... | 0.27065808    |
|    prob_ratio           | 1452979.6     |
|    rollout_return       | -1.8434213    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 174           |
|    ep_rew_mean          | -173          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 252           |
|    time_elapsed         | 37650         |
|    total_timesteps      | 64512         |
| train/                  |               |
|    active_example       | 129           |
|    approx_kl            | -7.987022e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0445       |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 50200         |
|    policy_gradient_loss | 6.65e-05      |
|    value_loss           | 0.00136       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0155      |
|    negative_advantag... | 0.0972631   |
|    positive_advantag... | 0.23601872  |
|    prob_ratio           | 184304.06   |
|    rollout_return       | -1.5286678  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00956     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00989     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 173         |
|    ep_rew_mean          | -172        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 253         |
|    time_elapsed         | 37812       |
|    total_timesteps      | 64768       |
| train/                  |             |
|    active_example       | 244         |
|    approx_kl            | -0.00162521 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0337     |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 50400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.004       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0143         |
|    negative_advantag... | 0.09453263     |
|    positive_advantag... | 0.2804873      |
|    prob_ratio           | 1194055.5      |
|    rollout_return       | -1.9287407     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.17           |
|    collect_rollout/Mean | 4.17           |
|    collect_rollout/Sum  | 4.17           |
|    train_action_adv/... | 0.00948        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 172            |
|    ep_rew_mean          | -171           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 254            |
|    time_elapsed         | 37975          |
|    total_timesteps      | 65024          |
| train/                  |                |
|    active_example       | 153            |
|    approx_kl            | -0.00050869584 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0448        |
|    explained_variance   | 0.978          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 50600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00112        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0175       |
|    negative_advantag... | 0.12471588   |
|    positive_advantag... | 0.2444839    |
|    prob_ratio           | 916510.4     |
|    rollout_return       | -1.8229585   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00953      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 172          |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 255          |
|    time_elapsed         | 38138        |
|    total_timesteps      | 65280        |
| train/                  |              |
|    active_example       | 132          |
|    approx_kl            | 0.0018407106 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0342      |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 50800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0031       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0161       |
|    negative_advantag... | 0.10801458   |
|    positive_advantag... | 0.2302473    |
|    prob_ratio           | 648767.5     |
|    rollout_return       | -1.7457762   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.22         |
|    collect_rollout/Mean | 4.23         |
|    collect_rollout/Sum  | 4.23         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 172          |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 256          |
|    time_elapsed         | 38299        |
|    total_timesteps      | 65536        |
| train/                  |              |
|    active_example       | 246          |
|    approx_kl            | -0.007980764 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0477      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 51000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00156      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0192        |
|    negative_advantag... | 0.11240235    |
|    positive_advantag... | 0.16949223    |
|    prob_ratio           | 596516.4      |
|    rollout_return       | -1.7043489    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00991       |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 174           |
|    ep_rew_mean          | -173          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 257           |
|    time_elapsed         | 38461         |
|    total_timesteps      | 65792         |
| train/                  |               |
|    active_example       | 136           |
|    approx_kl            | 0.00024029613 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 51200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00206       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0174       |
|    negative_advantag... | 0.107281655  |
|    positive_advantag... | 0.21211326   |
|    prob_ratio           | 278874.47    |
|    rollout_return       | -1.7364933   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 174          |
|    ep_rew_mean          | -173         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 258          |
|    time_elapsed         | 38622        |
|    total_timesteps      | 66048        |
| train/                  |              |
|    active_example       | 146          |
|    approx_kl            | 0.0008855611 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.027       |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 51400        |
|    policy_gradient_loss | 0.00179      |
|    value_loss           | 0.0045       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0142      |
|    negative_advantag... | 0.097988956 |
|    positive_advantag... | 0.24111266  |
|    prob_ratio           | 594790.06   |
|    rollout_return       | -2.0264874  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 173         |
|    ep_rew_mean          | -172        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 259         |
|    time_elapsed         | 38784       |
|    total_timesteps      | 66304       |
| train/                  |             |
|    active_example       | 246         |
|    approx_kl            | -0.00525856 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0376     |
|    explained_variance   | 0.908       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 51600       |
|    policy_gradient_loss | 0.000207    |
|    value_loss           | 0.00644     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0186        |
|    negative_advantag... | 0.1275579     |
|    positive_advantag... | 0.21285234    |
|    prob_ratio           | 2628809.5     |
|    rollout_return       | -2.158993     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 173           |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 260           |
|    time_elapsed         | 38947         |
|    total_timesteps      | 66560         |
| train/                  |               |
|    active_example       | 155           |
|    approx_kl            | -0.0005415678 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0328       |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 51800         |
|    policy_gradient_loss | 0.00334       |
|    value_loss           | 0.00926       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0125        |
|    negative_advantag... | 0.0759251     |
|    positive_advantag... | 0.18876247    |
|    prob_ratio           | 1336697.0     |
|    rollout_return       | -1.6980984    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 173           |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 261           |
|    time_elapsed         | 39109         |
|    total_timesteps      | 66816         |
| train/                  |               |
|    active_example       | 115           |
|    approx_kl            | -7.033348e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0181       |
|    explained_variance   | 0.804         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 52000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00432       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0163      |
|    negative_advantag... | 0.10665908  |
|    positive_advantag... | 0.2526185   |
|    prob_ratio           | 567151.94   |
|    rollout_return       | -2.147917   |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 159         |
|    train_epoch/Sum      | 159         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 173         |
|    ep_rew_mean          | -172        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 262         |
|    time_elapsed         | 39272       |
|    total_timesteps      | 67072       |
| train/                  |             |
|    active_example       | 248         |
|    approx_kl            | 0.004687935 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0431     |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 52200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00273     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0165         |
|    negative_advantag... | 0.11228237     |
|    positive_advantag... | 0.21451491     |
|    prob_ratio           | 2138328.8      |
|    rollout_return       | -1.8986259     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.21           |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00951        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 173            |
|    ep_rew_mean          | -172           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 263            |
|    time_elapsed         | 39433          |
|    total_timesteps      | 67328          |
| train/                  |                |
|    active_example       | 153            |
|    approx_kl            | -0.00015494227 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0372        |
|    explained_variance   | 0.963          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 52400          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00352        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0144       |
|    negative_advantag... | 0.10608075   |
|    positive_advantag... | 0.2143881    |
|    prob_ratio           | 1081166.5    |
|    rollout_return       | -1.9539963   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00991      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 172          |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 264          |
|    time_elapsed         | 39595        |
|    total_timesteps      | 67584        |
| train/                  |              |
|    active_example       | 121          |
|    approx_kl            | 5.045533e-05 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 52600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00184      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0182        |
|    negative_advantag... | 0.13474527    |
|    positive_advantag... | 0.22560658    |
|    prob_ratio           | 688380.75     |
|    rollout_return       | -2.0318818    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 173           |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 265           |
|    time_elapsed         | 39756         |
|    total_timesteps      | 67840         |
| train/                  |               |
|    active_example       | 250           |
|    approx_kl            | -3.874302e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0226       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 52800         |
|    policy_gradient_loss | 0.000404      |
|    value_loss           | 0.00076       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0169       |
|    negative_advantag... | 0.13481738   |
|    positive_advantag... | 0.19229248   |
|    prob_ratio           | 1814873.0    |
|    rollout_return       | -1.7129502   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 173          |
|    ep_rew_mean          | -172         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 266          |
|    time_elapsed         | 39918        |
|    total_timesteps      | 68096        |
| train/                  |              |
|    active_example       | 158          |
|    approx_kl            | 0.0013631433 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0485      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 53000        |
|    policy_gradient_loss | 0.000163     |
|    value_loss           | 0.00119      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0176        |
|    negative_advantag... | 0.12563185    |
|    positive_advantag... | 0.20341139    |
|    prob_ratio           | 966019.1      |
|    rollout_return       | -2.04396      |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 173           |
|    ep_rew_mean          | -172          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 267           |
|    time_elapsed         | 40081         |
|    total_timesteps      | 68352         |
| train/                  |               |
|    active_example       | 141           |
|    approx_kl            | -0.0050120354 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0436       |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 53200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00339       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0165     |
|    negative_advantag... | 0.12959523 |
|    positive_advantag... | 0.22925247 |
|    prob_ratio           | 647464.4   |
|    rollout_return       | -1.469464  |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.0095     |
|    train_action_adv/Sum | 30.4       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.3       |
|    train_epoch/Mean     | 157        |
|    train_epoch/Sum      | 157        |
|    train_loss/Mean      | 0.00984    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 268        |
|    time_elapsed         | 40242      |
|    total_timesteps      | 68608      |
| train/                  |            |
|    active_example       | 236        |
|    approx_kl            | 0.0583221  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0442    |
|    explained_variance   | 0.948      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 53400      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.004      |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0181        |
|    negative_advantag... | 0.14455906    |
|    positive_advantag... | 0.1978044     |
|    prob_ratio           | 801968.3      |
|    rollout_return       | -1.9990329    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 171           |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 269           |
|    time_elapsed         | 40404         |
|    total_timesteps      | 68864         |
| train/                  |               |
|    active_example       | 146           |
|    approx_kl            | 0.00019946694 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0347       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 53600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00118       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0136       |
|    negative_advantag... | 0.08889736   |
|    positive_advantag... | 0.20137618   |
|    prob_ratio           | 436384.62    |
|    rollout_return       | -1.7819399   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00981      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 172          |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 270          |
|    time_elapsed         | 40566        |
|    total_timesteps      | 69120        |
| train/                  |              |
|    active_example       | 119          |
|    approx_kl            | -0.018321306 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0288      |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 53800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00159      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0156       |
|    negative_advantag... | 0.11020294   |
|    positive_advantag... | 0.18024634   |
|    prob_ratio           | 1198849.5    |
|    rollout_return       | -1.8185805   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.16         |
|    collect_rollout/Sum  | 4.16         |
|    train_action_adv/... | 0.00944      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0099       |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 172          |
|    ep_rew_mean          | -171         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 271          |
|    time_elapsed         | 40728        |
|    total_timesteps      | 69376        |
| train/                  |              |
|    active_example       | 246          |
|    approx_kl            | 0.0001450777 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 54000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00233      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0179       |
|    negative_advantag... | 0.1287094    |
|    positive_advantag... | 0.21021658   |
|    prob_ratio           | 829424.8     |
|    rollout_return       | -1.8096625   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 171          |
|    ep_rew_mean          | -170         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 272          |
|    time_elapsed         | 40889        |
|    total_timesteps      | 69632        |
| train/                  |              |
|    active_example       | 153          |
|    approx_kl            | 0.0107783675 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.047       |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 54200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00188      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0158        |
|    negative_advantag... | 0.11385764    |
|    positive_advantag... | 0.2547951     |
|    prob_ratio           | 194121.0      |
|    rollout_return       | -2.080086     |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 171           |
|    ep_rew_mean          | -170          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 273           |
|    time_elapsed         | 41050         |
|    total_timesteps      | 69888         |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | -0.0021335185 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0455       |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 54400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00177       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=70000, episode_reward=-204.20 +/- 74.94
Episode length: 205.20 +/- 74.94
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0171      |
|    negative_advantag... | 0.11710779  |
|    positive_advantag... | 0.18482585  |
|    prob_ratio           | 364618.25   |
|    rollout_return       | -1.9454689  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 6.16        |
|    collect_rollout/Sum  | 6.16        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.0099      |
|    train_loss/Sum       | 31.7        |
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | -204        |
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | -169        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 274         |
|    time_elapsed         | 41214       |
|    total_timesteps      | 70144       |
| train/                  |             |
|    active_example       | 246         |
|    approx_kl            | 0.009941727 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.033      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 54600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00197     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0172       |
|    negative_advantag... | 0.11550638   |
|    positive_advantag... | 0.20572422   |
|    prob_ratio           | 1029469.25   |
|    rollout_return       | -2.1164842   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 169          |
|    ep_rew_mean          | -168         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 275          |
|    time_elapsed         | 41376        |
|    total_timesteps      | 70400        |
| train/                  |              |
|    active_example       | 142          |
|    approx_kl            | -0.008176893 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0483      |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 54800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00272      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0144        |
|    negative_advantag... | 0.11387951    |
|    positive_advantag... | 0.20319085    |
|    prob_ratio           | 786577.4      |
|    rollout_return       | -2.0645556    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 167           |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 276           |
|    time_elapsed         | 41538         |
|    total_timesteps      | 70656         |
| train/                  |               |
|    active_example       | 128           |
|    approx_kl            | -7.236004e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0399       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 55000         |
|    policy_gradient_loss | 0.000176      |
|    value_loss           | 0.00299       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0161         |
|    negative_advantag... | 0.11797141     |
|    positive_advantag... | 0.21122783     |
|    prob_ratio           | 139099.14      |
|    rollout_return       | -2.073989      |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.23           |
|    collect_rollout/Mean | 4.24           |
|    collect_rollout/Sum  | 4.24           |
|    train_action_adv/... | 0.00948        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 167            |
|    ep_rew_mean          | -166           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 277            |
|    time_elapsed         | 41701          |
|    total_timesteps      | 70912          |
| train/                  |                |
|    active_example       | 245            |
|    approx_kl            | -0.00021450222 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0325        |
|    explained_variance   | 0.948          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 55200          |
|    policy_gradient_loss | 0.000884       |
|    value_loss           | 0.00542        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0154       |
|    negative_advantag... | 0.13270281   |
|    positive_advantag... | 0.22999254   |
|    prob_ratio           | 25696.572    |
|    rollout_return       | -1.5050976   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 167          |
|    ep_rew_mean          | -166         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 278          |
|    time_elapsed         | 41862        |
|    total_timesteps      | 71168        |
| train/                  |              |
|    active_example       | 152          |
|    approx_kl            | -0.001305744 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0322      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 55400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00586      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0179       |
|    negative_advantag... | 0.1455832    |
|    positive_advantag... | 0.19658484   |
|    prob_ratio           | 812364.4     |
|    rollout_return       | -1.9231899   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00976      |
|    train_loss/Sum       | 31.2         |
| rollout/                |              |
|    ep_len_mean          | 168          |
|    ep_rew_mean          | -167         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 279          |
|    time_elapsed         | 42024        |
|    total_timesteps      | 71424        |
| train/                  |              |
|    active_example       | 132          |
|    approx_kl            | 0.0015772879 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0414      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 55600        |
|    policy_gradient_loss | 0.000505     |
|    value_loss           | 0.00276      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00705     |
|    negative_advantag... | 0.038676377 |
|    positive_advantag... | 0.100425236 |
|    prob_ratio           | 434761.84   |
|    rollout_return       | -1.2715898  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00978     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | -166        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 280         |
|    time_elapsed         | 42186       |
|    total_timesteps      | 71680       |
| train/                  |             |
|    active_example       | 255         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0121     |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 55800       |
|    policy_gradient_loss | 5.91e-05    |
|    value_loss           | 0.00312     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.017       |
|    negative_advantag... | 0.12486608  |
|    positive_advantag... | 0.22909893  |
|    prob_ratio           | 525711.0    |
|    rollout_return       | -1.906982   |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | -167        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 281         |
|    time_elapsed         | 42348       |
|    total_timesteps      | 71936       |
| train/                  |             |
|    active_example       | 154         |
|    approx_kl            | 0.008714825 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0393     |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 56000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00191     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.017       |
|    negative_advantag... | 0.11492056  |
|    positive_advantag... | 0.1924623   |
|    prob_ratio           | 576026.3    |
|    rollout_return       | -1.6732025  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -165        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 282         |
|    time_elapsed         | 42510       |
|    total_timesteps      | 72192       |
| train/                  |             |
|    active_example       | 132         |
|    approx_kl            | 0.009466827 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0344     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 56200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00237     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0149         |
|    negative_advantag... | 0.1126052      |
|    positive_advantag... | 0.23964144     |
|    prob_ratio           | 1553909.8      |
|    rollout_return       | -1.9590229     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00943        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00986        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 166            |
|    ep_rew_mean          | -165           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 283            |
|    time_elapsed         | 42672          |
|    total_timesteps      | 72448          |
| train/                  |                |
|    active_example       | 250            |
|    approx_kl            | -3.0100346e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0343        |
|    explained_variance   | 0.965          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 56400          |
|    policy_gradient_loss | 0.000448       |
|    value_loss           | 0.00318        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0145     |
|    negative_advantag... | 0.11683691 |
|    positive_advantag... | 0.19816318 |
|    prob_ratio           | 645628.06  |
|    rollout_return       | -1.5577725 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.00944    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.4       |
|    train_epoch/Mean     | 157        |
|    train_epoch/Sum      | 157        |
|    train_loss/Mean      | 0.0098     |
|    train_loss/Sum       | 31.3       |
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | -165       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 284        |
|    time_elapsed         | 42834      |
|    total_timesteps      | 72704      |
| train/                  |            |
|    active_example       | 172        |
|    approx_kl            | 0.01430624 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0275    |
|    explained_variance   | 0.907      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 56600      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00837    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0122         |
|    negative_advantag... | 0.077240184    |
|    positive_advantag... | 0.269088       |
|    prob_ratio           | 349383.8       |
|    rollout_return       | -2.0689821     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.17           |
|    collect_rollout/Mean | 4.17           |
|    collect_rollout/Sum  | 4.17           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00979        |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 166            |
|    ep_rew_mean          | -165           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 285            |
|    time_elapsed         | 42995          |
|    total_timesteps      | 72960          |
| train/                  |                |
|    active_example       | 120            |
|    approx_kl            | -0.00033289194 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0412        |
|    explained_variance   | 0.949          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 56800          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00158        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0148       |
|    negative_advantag... | 0.09481403   |
|    positive_advantag... | 0.21469776   |
|    prob_ratio           | 383147.88    |
|    rollout_return       | -1.5578746   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 167          |
|    ep_rew_mean          | -166         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 286          |
|    time_elapsed         | 43156        |
|    total_timesteps      | 73216        |
| train/                  |              |
|    active_example       | 248          |
|    approx_kl            | -0.004440531 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0423      |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 57000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00132      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0157      |
|    negative_advantag... | 0.10955866  |
|    positive_advantag... | 0.20132038  |
|    prob_ratio           | 859482.06   |
|    rollout_return       | -1.9548886  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00953     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -165        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 287         |
|    time_elapsed         | 43318       |
|    total_timesteps      | 73472       |
| train/                  |             |
|    active_example       | 181         |
|    approx_kl            | -0.01201576 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0381     |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 57200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00349     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0151      |
|    negative_advantag... | 0.11339769  |
|    positive_advantag... | 0.19539145  |
|    prob_ratio           | 761117.75   |
|    rollout_return       | -1.7617362  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.21        |
|    collect_rollout/Sum  | 4.21        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -165        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 288         |
|    time_elapsed         | 43481       |
|    total_timesteps      | 73728       |
| train/                  |             |
|    active_example       | 110         |
|    approx_kl            | -0.01719299 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.031      |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 57400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00558     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0168        |
|    negative_advantag... | 0.1018425     |
|    positive_advantag... | 0.22782558    |
|    prob_ratio           | 1308861.1     |
|    rollout_return       | -1.9103743    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 57.9          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00995       |
|    train_loss/Sum       | 31.8          |
| rollout/                |               |
|    ep_len_mean          | 166           |
|    ep_rew_mean          | -165          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 289           |
|    time_elapsed         | 43642         |
|    total_timesteps      | 73984         |
| train/                  |               |
|    active_example       | 249           |
|    approx_kl            | 0.00022122264 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0356       |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 57600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00177       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0124         |
|    negative_advantag... | 0.09166898     |
|    positive_advantag... | 0.21114357     |
|    prob_ratio           | 1009060.6      |
|    rollout_return       | -1.7437761     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00941        |
|    train_action_adv/Sum | 30.1           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 167            |
|    ep_rew_mean          | -166           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 290            |
|    time_elapsed         | 43804          |
|    total_timesteps      | 74240          |
| train/                  |                |
|    active_example       | 166            |
|    approx_kl            | -5.8948994e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0343        |
|    explained_variance   | 0.944          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 57800          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00327        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0179      |
|    negative_advantag... | 0.11982894  |
|    positive_advantag... | 0.2579641   |
|    prob_ratio           | 516311.03   |
|    rollout_return       | -2.0635138  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00944     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 57.9        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -166        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 291         |
|    time_elapsed         | 43965       |
|    total_timesteps      | 74496       |
| train/                  |             |
|    active_example       | 117         |
|    approx_kl            | 0.035738006 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0445     |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 58000       |
|    policy_gradient_loss | 0.00089     |
|    value_loss           | 0.00234     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0143         |
|    negative_advantag... | 0.105297       |
|    positive_advantag... | 0.22310162     |
|    prob_ratio           | 3045767.8      |
|    rollout_return       | -1.6727539     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00988        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 166            |
|    ep_rew_mean          | -165           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 292            |
|    time_elapsed         | 44128          |
|    total_timesteps      | 74752          |
| train/                  |                |
|    active_example       | 246            |
|    approx_kl            | -4.1097403e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0408        |
|    explained_variance   | 0.976          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 58200          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00168        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0116         |
|    negative_advantag... | 0.07111114     |
|    positive_advantag... | 0.22093971     |
|    prob_ratio           | 1107474.6      |
|    rollout_return       | -1.8645343     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00946        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.1           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 165            |
|    ep_rew_mean          | -164           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 293            |
|    time_elapsed         | 44289          |
|    total_timesteps      | 75008          |
| train/                  |                |
|    active_example       | 157            |
|    approx_kl            | -0.00043570995 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0318        |
|    explained_variance   | 0.936          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 58400          |
|    policy_gradient_loss | 0.000832       |
|    value_loss           | 0.00714        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0124         |
|    negative_advantag... | 0.09513927     |
|    positive_advantag... | 0.22251701     |
|    prob_ratio           | 1176469.0      |
|    rollout_return       | -2.0326574     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00944        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0099         |
|    train_loss/Sum       | 31.7           |
| rollout/                |                |
|    ep_len_mean          | 165            |
|    ep_rew_mean          | -164           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 294            |
|    time_elapsed         | 44451          |
|    total_timesteps      | 75264          |
| train/                  |                |
|    active_example       | 131            |
|    approx_kl            | -0.00022694468 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0248        |
|    explained_variance   | 0.954          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 58600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00204        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0116       |
|    negative_advantag... | 0.08805175   |
|    positive_advantag... | 0.18325691   |
|    prob_ratio           | 186325.08    |
|    rollout_return       | -1.4383185   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 295          |
|    time_elapsed         | 44613        |
|    total_timesteps      | 75520        |
| train/                  |              |
|    active_example       | 249          |
|    approx_kl            | 0.0067343414 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0282      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 58800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00247      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0165        |
|    negative_advantag... | 0.10226185    |
|    positive_advantag... | 0.21103905    |
|    prob_ratio           | 922159.44     |
|    rollout_return       | -2.0715115    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 296           |
|    time_elapsed         | 44774         |
|    total_timesteps      | 75776         |
| train/                  |               |
|    active_example       | 145           |
|    approx_kl            | 4.1127205e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0274       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 59000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000631      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.014         |
|    negative_advantag... | 0.09836398    |
|    positive_advantag... | 0.19501565    |
|    prob_ratio           | 858265.06     |
|    rollout_return       | -1.7413286    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 297           |
|    time_elapsed         | 44936         |
|    total_timesteps      | 76032         |
| train/                  |               |
|    active_example       | 104           |
|    approx_kl            | 4.8279762e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 59200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0151        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0171        |
|    negative_advantag... | 0.12849848    |
|    positive_advantag... | 0.20027114    |
|    prob_ratio           | 1100234.8     |
|    rollout_return       | -1.9027023    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 298           |
|    time_elapsed         | 45098         |
|    total_timesteps      | 76288         |
| train/                  |               |
|    active_example       | 248           |
|    approx_kl            | -0.0043560565 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.034        |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 59400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00204       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0137      |
|    negative_advantag... | 0.102054395 |
|    positive_advantag... | 0.2128871   |
|    prob_ratio           | 520974.44   |
|    rollout_return       | -1.9304607  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.22        |
|    collect_rollout/Mean | 4.23        |
|    collect_rollout/Sum  | 4.23        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 299         |
|    time_elapsed         | 45260       |
|    total_timesteps      | 76544       |
| train/                  |             |
|    active_example       | 183         |
|    approx_kl            | 0.01767695  |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0347     |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 59600       |
|    policy_gradient_loss | 0.000115    |
|    value_loss           | 0.00677     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0151        |
|    negative_advantag... | 0.11085105    |
|    positive_advantag... | 0.20473535    |
|    prob_ratio           | 571193.7      |
|    rollout_return       | -2.0355673    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 300           |
|    time_elapsed         | 45422         |
|    total_timesteps      | 76800         |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | 2.9802322e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0341       |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 59800         |
|    policy_gradient_loss | 0.000363      |
|    value_loss           | 0.00182       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0114        |
|    negative_advantag... | 0.07353799    |
|    positive_advantag... | 0.2204662     |
|    prob_ratio           | 1010767.1     |
|    rollout_return       | -1.951859     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 301           |
|    time_elapsed         | 45584         |
|    total_timesteps      | 77056         |
| train/                  |               |
|    active_example       | 250           |
|    approx_kl            | -0.0018256009 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0263       |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 60000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00158       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0117        |
|    negative_advantag... | 0.07768363    |
|    positive_advantag... | 0.21823439    |
|    prob_ratio           | 1085120.5     |
|    rollout_return       | -1.4570812    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 302           |
|    time_elapsed         | 45746         |
|    total_timesteps      | 77312         |
| train/                  |               |
|    active_example       | 155           |
|    approx_kl            | -0.0012103617 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0285       |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 60200         |
|    policy_gradient_loss | 0.00357       |
|    value_loss           | 0.00241       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0145        |
|    negative_advantag... | 0.10182986    |
|    positive_advantag... | 0.21406882    |
|    prob_ratio           | 1332718.1     |
|    rollout_return       | -1.9131541    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 303           |
|    time_elapsed         | 45908         |
|    total_timesteps      | 77568         |
| train/                  |               |
|    active_example       | 127           |
|    approx_kl            | -0.0003517568 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0402       |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 60400         |
|    policy_gradient_loss | 0.000563      |
|    value_loss           | 0.00208       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.013         |
|    negative_advantag... | 0.08231204    |
|    positive_advantag... | 0.21907479    |
|    prob_ratio           | 328208.03     |
|    rollout_return       | -1.6467062    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 304           |
|    time_elapsed         | 46070         |
|    total_timesteps      | 77824         |
| train/                  |               |
|    active_example       | 248           |
|    approx_kl            | 0.00023904443 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.027        |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 60600         |
|    policy_gradient_loss | 0.00169       |
|    value_loss           | 0.00173       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0127        |
|    negative_advantag... | 0.10114109    |
|    positive_advantag... | 0.21772619    |
|    prob_ratio           | 572688.9      |
|    rollout_return       | -1.8550043    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00977       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 305           |
|    time_elapsed         | 46231         |
|    total_timesteps      | 78080         |
| train/                  |               |
|    active_example       | 160           |
|    approx_kl            | -0.0007597208 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 60800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00484       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0137        |
|    negative_advantag... | 0.09133939    |
|    positive_advantag... | 0.20510593    |
|    prob_ratio           | 1438983.5     |
|    rollout_return       | -1.8601904    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 306           |
|    time_elapsed         | 46393         |
|    total_timesteps      | 78336         |
| train/                  |               |
|    active_example       | 115           |
|    approx_kl            | 0.00040751696 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0329       |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 61000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0026        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0145        |
|    negative_advantag... | 0.10224348    |
|    positive_advantag... | 0.17590122    |
|    prob_ratio           | 860024.94     |
|    rollout_return       | -1.5862019    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 307           |
|    time_elapsed         | 46554         |
|    total_timesteps      | 78592         |
| train/                  |               |
|    active_example       | 248           |
|    approx_kl            | -6.824732e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0319       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 61200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00372       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0151        |
|    negative_advantag... | 0.1106799     |
|    positive_advantag... | 0.16189834    |
|    prob_ratio           | 1435398.8     |
|    rollout_return       | -1.9070076    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 308           |
|    time_elapsed         | 46716         |
|    total_timesteps      | 78848         |
| train/                  |               |
|    active_example       | 157           |
|    approx_kl            | 2.9802322e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 61400         |
|    policy_gradient_loss | 0.000992      |
|    value_loss           | 0.00282       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0156       |
|    negative_advantag... | 0.10566007   |
|    positive_advantag... | 0.1896916    |
|    prob_ratio           | 1432882.1    |
|    rollout_return       | -1.9147855   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | -163         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 309          |
|    time_elapsed         | 46878        |
|    total_timesteps      | 79104        |
| train/                  |              |
|    active_example       | 134          |
|    approx_kl            | 0.0046401024 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0336      |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.01         |
|    loss                 | 0.345        |
|    n_updates            | 61600        |
|    policy_gradient_loss | 0.00116      |
|    value_loss           | 0.00171      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00904     |
|    negative_advantag... | 0.07444706  |
|    positive_advantag... | 0.10381469  |
|    prob_ratio           | 285288.5    |
|    rollout_return       | -1.2733322  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00944     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 310         |
|    time_elapsed         | 47040       |
|    total_timesteps      | 79360       |
| train/                  |             |
|    active_example       | 254         |
|    approx_kl            | 0.001244247 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0163     |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 61800       |
|    policy_gradient_loss | 0.000245    |
|    value_loss           | 0.00256     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0138       |
|    negative_advantag... | 0.09933307   |
|    positive_advantag... | 0.1770928    |
|    prob_ratio           | 2152285.5    |
|    rollout_return       | -1.6843954   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 311          |
|    time_elapsed         | 47202        |
|    total_timesteps      | 79616        |
| train/                  |              |
|    active_example       | 151          |
|    approx_kl            | 0.0032334328 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 62000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00124      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0137       |
|    negative_advantag... | 0.10339878   |
|    positive_advantag... | 0.21117355   |
|    prob_ratio           | 975371.94    |
|    rollout_return       | -1.9648659   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 312          |
|    time_elapsed         | 47363        |
|    total_timesteps      | 79872        |
| train/                  |              |
|    active_example       | 129          |
|    approx_kl            | 0.0075627714 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0351      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 62200        |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 0.00109      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=80000, episode_reward=-172.00 +/- 14.45
Episode length: 173.00 +/- 14.45
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0148      |
|    negative_advantag... | 0.09364885  |
|    positive_advantag... | 0.17564812  |
|    prob_ratio           | 1043043.94  |
|    rollout_return       | -1.881159   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 5.81        |
|    collect_rollout/Sum  | 5.81        |
|    train_action_adv/... | 0.00955     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.4        |
| eval/                   |             |
|    mean_ep_length       | 173         |
|    mean_reward          | -172        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 313         |
|    time_elapsed         | 47526       |
|    total_timesteps      | 80128       |
| train/                  |             |
|    active_example       | 249         |
|    approx_kl            | -0.01300776 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0347     |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 62400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00303     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0133         |
|    negative_advantag... | 0.08330411     |
|    positive_advantag... | 0.1998209      |
|    prob_ratio           | 2019263.2      |
|    rollout_return       | -2.1122065     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.00948        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.1           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00986        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 165            |
|    ep_rew_mean          | -164           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 314            |
|    time_elapsed         | 47688          |
|    total_timesteps      | 80384          |
| train/                  |                |
|    active_example       | 154            |
|    approx_kl            | -0.00030982494 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0336        |
|    explained_variance   | 0.977          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 62600          |
|    policy_gradient_loss | 0.000312       |
|    value_loss           | 0.00196        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0106       |
|    negative_advantag... | 0.06645324   |
|    positive_advantag... | 0.18051952   |
|    prob_ratio           | 721980.94    |
|    rollout_return       | -1.9674498   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 315          |
|    time_elapsed         | 47850        |
|    total_timesteps      | 80640        |
| train/                  |              |
|    active_example       | 132          |
|    approx_kl            | 0.0070668757 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0174      |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 62800        |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 0.00153      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0131        |
|    negative_advantag... | 0.07475231    |
|    positive_advantag... | 0.20382218    |
|    prob_ratio           | 1304133.9     |
|    rollout_return       | -1.8490654    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00978       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 316           |
|    time_elapsed         | 48011         |
|    total_timesteps      | 80896         |
| train/                  |               |
|    active_example       | 250           |
|    approx_kl            | 0.00093096495 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0326       |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 63000         |
|    policy_gradient_loss | 0.00087       |
|    value_loss           | 0.00153       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0125      |
|    negative_advantag... | 0.08875555  |
|    positive_advantag... | 0.160522    |
|    prob_ratio           | 19301.527   |
|    rollout_return       | -1.8131344  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 317         |
|    time_elapsed         | 48173       |
|    total_timesteps      | 81152       |
| train/                  |             |
|    active_example       | 173         |
|    approx_kl            | 0.033368975 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0295     |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 63200       |
|    policy_gradient_loss | 0.000635    |
|    value_loss           | 0.00261     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00751       |
|    negative_advantag... | 0.056644157   |
|    positive_advantag... | 0.21150045    |
|    prob_ratio           | 1219387.2     |
|    rollout_return       | -1.7841319    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 318           |
|    time_elapsed         | 48335         |
|    total_timesteps      | 81408         |
| train/                  |               |
|    active_example       | 112           |
|    approx_kl            | -0.0097518265 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 63400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000803      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.017          |
|    negative_advantag... | 0.120043494    |
|    positive_advantag... | 0.16882387     |
|    prob_ratio           | 2214517.0      |
|    rollout_return       | -2.138537      |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.1           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0099         |
|    train_loss/Sum       | 31.7           |
| rollout/                |                |
|    ep_len_mean          | 166            |
|    ep_rew_mean          | -165           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 319            |
|    time_elapsed         | 48497          |
|    total_timesteps      | 81664          |
| train/                  |                |
|    active_example       | 250            |
|    approx_kl            | -0.00032693148 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0275        |
|    explained_variance   | 0.925          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 63600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00734        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0105        |
|    negative_advantag... | 0.07285486    |
|    positive_advantag... | 0.18234056    |
|    prob_ratio           | 443004.7      |
|    rollout_return       | -1.0800114    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 166           |
|    ep_rew_mean          | -165          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 320           |
|    time_elapsed         | 48658         |
|    total_timesteps      | 81920         |
| train/                  |               |
|    active_example       | 96            |
|    approx_kl            | 0.00014153123 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.865         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 63800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00369       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0147      |
|    negative_advantag... | 0.101377144 |
|    positive_advantag... | 0.1606151   |
|    prob_ratio           | 86580.05    |
|    rollout_return       | -1.8009514  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.16        |
|    collect_rollout/Mean | 4.16        |
|    collect_rollout/Sum  | 4.16        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0181      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 321         |
|    time_elapsed         | 48819       |
|    total_timesteps      | 82176       |
| train/                  |             |
|    active_example       | 101         |
|    approx_kl            | 0.009105548 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0216     |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 64000       |
|    policy_gradient_loss | 0.000221    |
|    value_loss           | 0.0207      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0133       |
|    negative_advantag... | 0.10117858   |
|    positive_advantag... | 0.1610483    |
|    prob_ratio           | 1161599.9    |
|    rollout_return       | -2.0139828   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | -163         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 322          |
|    time_elapsed         | 48980        |
|    total_timesteps      | 82432        |
| train/                  |              |
|    active_example       | 250          |
|    approx_kl            | 6.854534e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.026       |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 64200        |
|    policy_gradient_loss | 0.00169      |
|    value_loss           | 0.00402      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0152      |
|    negative_advantag... | 0.10840018  |
|    positive_advantag... | 0.1641585   |
|    prob_ratio           | 647828.9    |
|    rollout_return       | -1.3241292  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.23        |
|    collect_rollout/Mean | 4.25        |
|    collect_rollout/Sum  | 4.25        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00978     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | -165        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 323         |
|    time_elapsed         | 49142       |
|    total_timesteps      | 82688       |
| train/                  |             |
|    active_example       | 108         |
|    approx_kl            | 0.008042201 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0364     |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 64400       |
|    policy_gradient_loss | 0.000875    |
|    value_loss           | 0.00336     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0112       |
|    negative_advantag... | 0.08238125   |
|    positive_advantag... | 0.14482583   |
|    prob_ratio           | 479527.56    |
|    rollout_return       | -1.8927647   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 166          |
|    ep_rew_mean          | -165         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 324          |
|    time_elapsed         | 49304        |
|    total_timesteps      | 82944        |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | -0.053699523 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0242      |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 64600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00174      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0101       |
|    negative_advantag... | 0.071177356  |
|    positive_advantag... | 0.21173285   |
|    prob_ratio           | 792841.5     |
|    rollout_return       | -1.880213    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 325          |
|    time_elapsed         | 49465        |
|    total_timesteps      | 83200        |
| train/                  |              |
|    active_example       | 247          |
|    approx_kl            | -0.008920312 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0303      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 64800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00174      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0146      |
|    negative_advantag... | 0.1133181   |
|    positive_advantag... | 0.13822499  |
|    prob_ratio           | 746872.5    |
|    rollout_return       | -2.0911202  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00977     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 326         |
|    time_elapsed         | 49627       |
|    total_timesteps      | 83456       |
| train/                  |             |
|    active_example       | 148         |
|    approx_kl            | 0.020179749 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0268     |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 65000       |
|    policy_gradient_loss | 0.00034     |
|    value_loss           | 0.00178     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0159      |
|    negative_advantag... | 0.109687895 |
|    positive_advantag... | 0.13283183  |
|    prob_ratio           | 650315.25   |
|    rollout_return       | -1.9629954  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 327         |
|    time_elapsed         | 49789       |
|    total_timesteps      | 83712       |
| train/                  |             |
|    active_example       | 123         |
|    approx_kl            | 0.012604564 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0253     |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 65200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00157     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0136        |
|    negative_advantag... | 0.10640771    |
|    positive_advantag... | 0.15581892    |
|    prob_ratio           | 723245.7      |
|    rollout_return       | -1.9737442    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.23          |
|    collect_rollout/Mean | 4.25          |
|    collect_rollout/Sum  | 4.25          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 328           |
|    time_elapsed         | 49951         |
|    total_timesteps      | 83968         |
| train/                  |               |
|    active_example       | 249           |
|    approx_kl            | 0.00010609627 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0284       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0.116         |
|    n_updates            | 65400         |
|    policy_gradient_loss | 0.000372      |
|    value_loss           | 0.00183       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0115        |
|    negative_advantag... | 0.07654446    |
|    positive_advantag... | 0.15593606    |
|    prob_ratio           | 481716.3      |
|    rollout_return       | -1.8744873    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 329           |
|    time_elapsed         | 50113         |
|    total_timesteps      | 84224         |
| train/                  |               |
|    active_example       | 165           |
|    approx_kl            | -0.0062242746 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 65600         |
|    policy_gradient_loss | 0.000873      |
|    value_loss           | 0.002         |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0136        |
|    negative_advantag... | 0.10230908    |
|    positive_advantag... | 0.17716362    |
|    prob_ratio           | 869327.5      |
|    rollout_return       | -2.0226235    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 330           |
|    time_elapsed         | 50275         |
|    total_timesteps      | 84480         |
| train/                  |               |
|    active_example       | 124           |
|    approx_kl            | -0.0023583174 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0237       |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 65800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00298       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0128        |
|    negative_advantag... | 0.0949508     |
|    positive_advantag... | 0.16118206    |
|    prob_ratio           | 833110.56     |
|    rollout_return       | -1.8020748    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00992       |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 331           |
|    time_elapsed         | 50436         |
|    total_timesteps      | 84736         |
| train/                  |               |
|    active_example       | 251           |
|    approx_kl            | -0.0047320426 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0319       |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 66000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00206       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00988      |
|    negative_advantag... | 0.07365486   |
|    positive_advantag... | 0.18437262   |
|    prob_ratio           | 1305628.4    |
|    rollout_return       | -1.7553415   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 332          |
|    time_elapsed         | 50598        |
|    total_timesteps      | 84992        |
| train/                  |              |
|    active_example       | 140          |
|    approx_kl            | 0.0009689629 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 66200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00147      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0131       |
|    negative_advantag... | 0.08851355   |
|    positive_advantag... | 0.19396701   |
|    prob_ratio           | 962904.2     |
|    rollout_return       | -2.0474615   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 333          |
|    time_elapsed         | 50760        |
|    total_timesteps      | 85248        |
| train/                  |              |
|    active_example       | 126          |
|    approx_kl            | 0.0006707907 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 66400        |
|    policy_gradient_loss | 0.00192      |
|    value_loss           | 0.00155      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
---------------------------------------------
| HPO/                    |                 |
|    margin               | 0.0147          |
|    negative_advantag... | 0.11042656      |
|    positive_advantag... | 0.16611646      |
|    prob_ratio           | 1468645.1       |
|    rollout_return       | -1.8574988      |
| Time/                   |                 |
|    collect_computeV/... | 0.0125          |
|    collect_computeV/Sum | 3.2             |
|    collect_rollout/Mean | 4.22            |
|    collect_rollout/Sum  | 4.22            |
|    train_action_adv/... | 0.0095          |
|    train_action_adv/Sum | 30.4            |
|    train_computeV/Mean  | 0.0182          |
|    train_computeV/Sum   | 58.3            |
|    train_epoch/Mean     | 158             |
|    train_epoch/Sum      | 158             |
|    train_loss/Mean      | 0.00988         |
|    train_loss/Sum       | 31.6            |
| rollout/                |                 |
|    ep_len_mean          | 163             |
|    ep_rew_mean          | -162            |
| time/                   |                 |
|    fps                  | 1               |
|    iterations           | 334             |
|    time_elapsed         | 50922           |
|    total_timesteps      | 85504           |
| train/                  |                 |
|    active_example       | 249             |
|    approx_kl            | -1.50203705e-05 |
|    clip_range           | 0.2             |
|    entropy_loss         | -0.017          |
|    explained_variance   | 0.958           |
|    learning_rate        | 0.01            |
|    loss                 | 0               |
|    n_updates            | 66600           |
|    policy_gradient_loss | 0.000149        |
|    value_loss           | 0.00299         |
---------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0168     |
|    negative_advantag... | 0.133768   |
|    positive_advantag... | 0.14404497 |
|    prob_ratio           | 1007037.94 |
|    rollout_return       | -1.7582604 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.18       |
|    collect_rollout/Sum  | 4.18       |
|    train_action_adv/... | 0.00944    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.2       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00986    |
|    train_loss/Sum       | 31.6       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 335        |
|    time_elapsed         | 51084      |
|    total_timesteps      | 85760      |
| train/                  |            |
|    active_example       | 150        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.032     |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 66800      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00212    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0136      |
|    negative_advantag... | 0.104711846 |
|    positive_advantag... | 0.1531018   |
|    prob_ratio           | 864462.3    |
|    rollout_return       | -1.7643739  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00957     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00988     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | -160        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 336         |
|    time_elapsed         | 51246       |
|    total_timesteps      | 86016       |
| train/                  |             |
|    active_example       | 131         |
|    approx_kl            | 0.02444917  |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0239     |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 67000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00162     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0158       |
|    negative_advantag... | 0.13348159   |
|    positive_advantag... | 0.14390124   |
|    prob_ratio           | 1252506.4    |
|    rollout_return       | -1.9732509   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00978      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 337          |
|    time_elapsed         | 51407        |
|    total_timesteps      | 86272        |
| train/                  |              |
|    active_example       | 248          |
|    approx_kl            | 0.0073696077 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0307      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.01         |
|    loss                 | 0.0947       |
|    n_updates            | 67200        |
|    policy_gradient_loss | 0.000855     |
|    value_loss           | 0.00178      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0155        |
|    negative_advantag... | 0.13073684    |
|    positive_advantag... | 0.114888236   |
|    prob_ratio           | 1421225.9     |
|    rollout_return       | -1.5477784    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00978       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 338           |
|    time_elapsed         | 51569         |
|    total_timesteps      | 86528         |
| train/                  |               |
|    active_example       | 165           |
|    approx_kl            | -0.0058806837 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0246       |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 67400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00356       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.015         |
|    negative_advantag... | 0.11601679    |
|    positive_advantag... | 0.1675185     |
|    prob_ratio           | 490558.72     |
|    rollout_return       | -1.9336493    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 339           |
|    time_elapsed         | 51731         |
|    total_timesteps      | 86784         |
| train/                  |               |
|    active_example       | 112           |
|    approx_kl            | -0.0017927289 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0225       |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 67600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00507       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0136       |
|    negative_advantag... | 0.090286255  |
|    positive_advantag... | 0.18242876   |
|    prob_ratio           | 2411702.0    |
|    rollout_return       | -1.9716638   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 340          |
|    time_elapsed         | 51893        |
|    total_timesteps      | 87040        |
| train/                  |              |
|    active_example       | 252          |
|    approx_kl            | 0.0020832568 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0287      |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 67800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00237      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0121       |
|    negative_advantag... | 0.094912045  |
|    positive_advantag... | 0.14975607   |
|    prob_ratio           | 839363.94    |
|    rollout_return       | -1.5598679   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.23         |
|    collect_rollout/Mean | 4.25         |
|    collect_rollout/Sum  | 4.25         |
|    train_action_adv/... | 0.00953      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00987      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 341          |
|    time_elapsed         | 52055        |
|    total_timesteps      | 87296        |
| train/                  |              |
|    active_example       | 169          |
|    approx_kl            | 0.0002348721 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0327      |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 68000        |
|    policy_gradient_loss | 0.00081      |
|    value_loss           | 0.00405      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0126        |
|    negative_advantag... | 0.084536344   |
|    positive_advantag... | 0.16433088    |
|    prob_ratio           | 778392.06     |
|    rollout_return       | -1.858732     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 342           |
|    time_elapsed         | 52217         |
|    total_timesteps      | 87552         |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | 0.00034990907 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0387       |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 68200         |
|    policy_gradient_loss | 0.00315       |
|    value_loss           | 0.00106       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0141        |
|    negative_advantag... | 0.106798984   |
|    positive_advantag... | 0.13501757    |
|    prob_ratio           | 1129449.9     |
|    rollout_return       | -1.8644947    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00941       |
|    train_action_adv/Sum | 30.1          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 343           |
|    time_elapsed         | 52379         |
|    total_timesteps      | 87808         |
| train/                  |               |
|    active_example       | 252           |
|    approx_kl            | 0.00020995736 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0302       |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 68400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00533       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0104         |
|    negative_advantag... | 0.08771232     |
|    positive_advantag... | 0.1612331      |
|    prob_ratio           | 932732.25      |
|    rollout_return       | -1.6657299     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0181         |
|    train_computeV/Sum   | 58             |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00987        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 344            |
|    time_elapsed         | 52540          |
|    total_timesteps      | 88064          |
| train/                  |                |
|    active_example       | 133            |
|    approx_kl            | -0.00033333898 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0187        |
|    explained_variance   | 0.929          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 68600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00613        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.015         |
|    negative_advantag... | 0.11975987    |
|    positive_advantag... | 0.10086525    |
|    prob_ratio           | 68228.9       |
|    rollout_return       | -1.6674647    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00978       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 345           |
|    time_elapsed         | 52701         |
|    total_timesteps      | 88320         |
| train/                  |               |
|    active_example       | 123           |
|    approx_kl            | 5.2511692e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 68800         |
|    policy_gradient_loss | 0.0019        |
|    value_loss           | 0.00257       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0162       |
|    negative_advantag... | 0.13457957   |
|    positive_advantag... | 0.13024499   |
|    prob_ratio           | 539448.25    |
|    rollout_return       | -1.7793697   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 346          |
|    time_elapsed         | 52864        |
|    total_timesteps      | 88576        |
| train/                  |              |
|    active_example       | 250          |
|    approx_kl            | -0.017242342 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0342      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 69000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00409      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0108       |
|    negative_advantag... | 0.05581174   |
|    positive_advantag... | 0.17229378   |
|    prob_ratio           | 42708.16     |
|    rollout_return       | -1.7564797   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00952      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 347          |
|    time_elapsed         | 53026        |
|    total_timesteps      | 88832        |
| train/                  |              |
|    active_example       | 167          |
|    approx_kl            | -0.011416882 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0373      |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 69200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00662      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00636       |
|    negative_advantag... | 0.038301803   |
|    positive_advantag... | 0.12896389    |
|    prob_ratio           | 495196.03     |
|    rollout_return       | -1.6830405    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 348           |
|    time_elapsed         | 53187         |
|    total_timesteps      | 89088         |
| train/                  |               |
|    active_example       | 94            |
|    approx_kl            | 1.1920929e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.813         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 69400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00807       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0126         |
|    negative_advantag... | 0.090353966    |
|    positive_advantag... | 0.14304449     |
|    prob_ratio           | 682667.4       |
|    rollout_return       | -2.0442505     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.22           |
|    collect_rollout/Mean | 4.22           |
|    collect_rollout/Sum  | 4.22           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00989        |
|    train_loss/Sum       | 31.7           |
| rollout/                |                |
|    ep_len_mean          | 162            |
|    ep_rew_mean          | -161           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 349            |
|    time_elapsed         | 53349          |
|    total_timesteps      | 89344          |
| train/                  |                |
|    active_example       | 250            |
|    approx_kl            | -0.00016698241 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0252        |
|    explained_variance   | 0.882          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 69600          |
|    policy_gradient_loss | 0.0005         |
|    value_loss           | 0.00581        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0134        |
|    negative_advantag... | 0.09482796    |
|    positive_advantag... | 0.15960568    |
|    prob_ratio           | 1392721.8     |
|    rollout_return       | -2.0784335    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 350           |
|    time_elapsed         | 53511         |
|    total_timesteps      | 89600         |
| train/                  |               |
|    active_example       | 144           |
|    approx_kl            | -0.0005969703 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 69800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00441       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0121       |
|    negative_advantag... | 0.089870594  |
|    positive_advantag... | 0.15835209   |
|    prob_ratio           | 596516.56    |
|    rollout_return       | -1.9759218   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 351          |
|    time_elapsed         | 53673        |
|    total_timesteps      | 89856        |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | 0.0033394992 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.033       |
|    explained_variance   | 0.778        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 70000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00971      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=90000, episode_reward=-150.60 +/- 13.87
Episode length: 151.60 +/- 13.87
New best mean reward!
----------------------------------------
| HPO/                    |            |
|    margin               | 0.013      |
|    negative_advantag... | 0.09724355 |
|    positive_advantag... | 0.17246354 |
|    prob_ratio           | 951223.4   |
|    rollout_return       | -1.9935306 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 5.69       |
|    collect_rollout/Sum  | 5.69       |
|    train_action_adv/... | 0.00945    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.4       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00982    |
|    train_loss/Sum       | 31.4       |
| eval/                   |            |
|    mean_ep_length       | 152        |
|    mean_reward          | -151       |
| rollout/                |            |
|    ep_len_mean          | 161        |
|    ep_rew_mean          | -160       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 352        |
|    time_elapsed         | 53836      |
|    total_timesteps      | 90112      |
| train/                  |            |
|    active_example       | 251        |
|    approx_kl            | 0.04512568 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0319    |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 70200      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00486    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00935       |
|    negative_advantag... | 0.064229      |
|    positive_advantag... | 0.19500948    |
|    prob_ratio           | 893903.25     |
|    rollout_return       | -1.8086314    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 353           |
|    time_elapsed         | 53998         |
|    total_timesteps      | 90368         |
| train/                  |               |
|    active_example       | 152           |
|    approx_kl            | -0.0024874806 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0303       |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 70400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0055        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0106         |
|    negative_advantag... | 0.06202507     |
|    positive_advantag... | 0.16678368     |
|    prob_ratio           | 487827.72      |
|    rollout_return       | -1.8947725     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.00945        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0181         |
|    train_computeV/Sum   | 58             |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00987        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 354            |
|    time_elapsed         | 54159          |
|    total_timesteps      | 90624          |
| train/                  |                |
|    active_example       | 113            |
|    approx_kl            | -5.9604645e-08 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0236        |
|    explained_variance   | 0.953          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 70600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00292        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0147       |
|    negative_advantag... | 0.09810295   |
|    positive_advantag... | 0.16592064   |
|    prob_ratio           | 576058.6     |
|    rollout_return       | -1.9840722   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.0099       |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 355          |
|    time_elapsed         | 54320        |
|    total_timesteps      | 90880        |
| train/                  |              |
|    active_example       | 250          |
|    approx_kl            | 0.0064931065 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0282      |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.01         |
|    loss                 | 0.104        |
|    n_updates            | 70800        |
|    policy_gradient_loss | 0.00321      |
|    value_loss           | 0.00226      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.011       |
|    negative_advantag... | 0.07277104  |
|    positive_advantag... | 0.19478783  |
|    prob_ratio           | 996949.44   |
|    rollout_return       | -1.7520502  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | -160        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 356         |
|    time_elapsed         | 54483       |
|    total_timesteps      | 91136       |
| train/                  |             |
|    active_example       | 149         |
|    approx_kl            | 0.016896933 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0277     |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 71000       |
|    policy_gradient_loss | 0.00249     |
|    value_loss           | 0.00263     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0113        |
|    negative_advantag... | 0.079654045   |
|    positive_advantag... | 0.19182959    |
|    prob_ratio           | 875858.56     |
|    rollout_return       | -1.9258825    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 357           |
|    time_elapsed         | 54645         |
|    total_timesteps      | 91392         |
| train/                  |               |
|    active_example       | 124           |
|    approx_kl            | -0.0016446114 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0215       |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 71200         |
|    policy_gradient_loss | 0.000861      |
|    value_loss           | 0.000868      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0131      |
|    negative_advantag... | 0.10249384  |
|    positive_advantag... | 0.17529918  |
|    prob_ratio           | 1166431.6   |
|    rollout_return       | -1.5821943  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 358         |
|    time_elapsed         | 54806       |
|    total_timesteps      | 91648       |
| train/                  |             |
|    active_example       | 248         |
|    approx_kl            | 0.010732561 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0269     |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 71400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00184     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0124         |
|    negative_advantag... | 0.09571672     |
|    positive_advantag... | 0.17008442     |
|    prob_ratio           | 2283897.2      |
|    rollout_return       | -2.0818985     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00983        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 359            |
|    time_elapsed         | 54968          |
|    total_timesteps      | 91904          |
| train/                  |                |
|    active_example       | 147            |
|    approx_kl            | -0.00064730644 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0207        |
|    explained_variance   | 0.902          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 71600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00672        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0129       |
|    negative_advantag... | 0.08955583   |
|    positive_advantag... | 0.15056142   |
|    prob_ratio           | 806380.3     |
|    rollout_return       | -1.9842007   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 360          |
|    time_elapsed         | 55130        |
|    total_timesteps      | 92160        |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | 0.0003553629 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0252      |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 71800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00353      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0119        |
|    negative_advantag... | 0.08928339    |
|    positive_advantag... | 0.15870498    |
|    prob_ratio           | 1114672.2     |
|    rollout_return       | -1.731841     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.16          |
|    collect_rollout/Sum  | 4.16          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 57.8          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00995       |
|    train_loss/Sum       | 31.9          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 361           |
|    time_elapsed         | 55291         |
|    total_timesteps      | 92416         |
| train/                  |               |
|    active_example       | 248           |
|    approx_kl            | 2.3245811e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0223       |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 72000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00234       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0154       |
|    negative_advantag... | 0.106090955  |
|    positive_advantag... | 0.1470732    |
|    prob_ratio           | 554589.44    |
|    rollout_return       | -1.7335924   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00978      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 362          |
|    time_elapsed         | 55453        |
|    total_timesteps      | 92672        |
| train/                  |              |
|    active_example       | 135          |
|    approx_kl            | -0.016662717 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0305      |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 72200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00106      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0143       |
|    negative_advantag... | 0.10602181   |
|    positive_advantag... | 0.12993525   |
|    prob_ratio           | 1146610.1    |
|    rollout_return       | -1.9827814   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.23         |
|    collect_rollout/Sum  | 4.23         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 363          |
|    time_elapsed         | 55616        |
|    total_timesteps      | 92928        |
| train/                  |              |
|    active_example       | 122          |
|    approx_kl            | 0.0017056167 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 72400        |
|    policy_gradient_loss | 0.00189      |
|    value_loss           | 0.00136      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0147        |
|    negative_advantag... | 0.112974465   |
|    positive_advantag... | 0.14356874    |
|    prob_ratio           | 1949884.5     |
|    rollout_return       | -2.0297415    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 364           |
|    time_elapsed         | 55778         |
|    total_timesteps      | 93184         |
| train/                  |               |
|    active_example       | 251           |
|    approx_kl            | -0.0017097369 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0292       |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 72600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0013        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0137      |
|    negative_advantag... | 0.09316253  |
|    positive_advantag... | 0.15080239  |
|    prob_ratio           | 914307.5    |
|    rollout_return       | -1.7889079  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 365         |
|    time_elapsed         | 55940       |
|    total_timesteps      | 93440       |
| train/                  |             |
|    active_example       | 147         |
|    approx_kl            | -0.03481832 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0269     |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 72800       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00324     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00969       |
|    negative_advantag... | 0.07169381    |
|    positive_advantag... | 0.1664117     |
|    prob_ratio           | 1169849.8     |
|    rollout_return       | -1.7553489    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 366           |
|    time_elapsed         | 56102         |
|    total_timesteps      | 93696         |
| train/                  |               |
|    active_example       | 122           |
|    approx_kl            | -0.0018068552 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 73000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00278       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0128       |
|    negative_advantag... | 0.07967755   |
|    positive_advantag... | 0.14583078   |
|    prob_ratio           | 577746.0     |
|    rollout_return       | -1.739743    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 367          |
|    time_elapsed         | 56263        |
|    total_timesteps      | 93952        |
| train/                  |              |
|    active_example       | 252          |
|    approx_kl            | -0.020927131 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0238      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 73200        |
|    policy_gradient_loss | 0.000424     |
|    value_loss           | 0.00134      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0104        |
|    negative_advantag... | 0.08466312    |
|    positive_advantag... | 0.12623543    |
|    prob_ratio           | 1366054.1     |
|    rollout_return       | -1.4946542    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 368           |
|    time_elapsed         | 56425         |
|    total_timesteps      | 94208         |
| train/                  |               |
|    active_example       | 134           |
|    approx_kl            | 1.1920929e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0249       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 73400         |
|    policy_gradient_loss | 0.00096       |
|    value_loss           | 0.00281       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0094     |
|    negative_advantag... | 0.07899403 |
|    positive_advantag... | 0.1501662  |
|    prob_ratio           | 1688691.2  |
|    rollout_return       | -2.0191855 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.2        |
|    collect_rollout/Mean | 4.21       |
|    collect_rollout/Sum  | 4.21       |
|    train_action_adv/... | 0.00948    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.3       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00983    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 369        |
|    time_elapsed         | 56587      |
|    total_timesteps      | 94464      |
| train/                  |            |
|    active_example       | 127        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0209    |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 73600      |
|    policy_gradient_loss | 0.00133    |
|    value_loss           | 0.000791   |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00861     |
|    negative_advantag... | 0.073223256 |
|    positive_advantag... | 0.101659596 |
|    prob_ratio           | 368632.72   |
|    rollout_return       | -1.6613256  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.23        |
|    collect_rollout/Mean | 4.24        |
|    collect_rollout/Sum  | 4.24        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.1        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00988     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 370         |
|    time_elapsed         | 56749       |
|    total_timesteps      | 94720       |
| train/                  |             |
|    active_example       | 249         |
|    approx_kl            | -0.01970303 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0217     |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 73800       |
|    policy_gradient_loss | 0.000104    |
|    value_loss           | 0.00747     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0147        |
|    negative_advantag... | 0.097668506   |
|    positive_advantag... | 0.14834718    |
|    prob_ratio           | 972516.5      |
|    rollout_return       | -2.1542325    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00954       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 371           |
|    time_elapsed         | 56911         |
|    total_timesteps      | 94976         |
| train/                  |               |
|    active_example       | 148           |
|    approx_kl            | -5.620718e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0273       |
|    explained_variance   | 0.878         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 74000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.01          |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0124       |
|    negative_advantag... | 0.104849994  |
|    positive_advantag... | 0.117122725  |
|    prob_ratio           | 1438246.9    |
|    rollout_return       | -1.9404161   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 372          |
|    time_elapsed         | 57073        |
|    total_timesteps      | 95232        |
| train/                  |              |
|    active_example       | 120          |
|    approx_kl            | 0.0039545894 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0212      |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 74200        |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 0.00311      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0119       |
|    negative_advantag... | 0.09588992   |
|    positive_advantag... | 0.10700099   |
|    prob_ratio           | 287245.62    |
|    rollout_return       | -1.8353968   |
| Time/                   |              |
|    collect_computeV/... | 0.0123       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.16         |
|    collect_rollout/Sum  | 4.16         |
|    train_action_adv/... | 0.00943      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 373          |
|    time_elapsed         | 57234        |
|    total_timesteps      | 95488        |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | -0.028766066 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0317      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 74400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00264      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0121        |
|    negative_advantag... | 0.080571      |
|    positive_advantag... | 0.12784702    |
|    prob_ratio           | 198160.6      |
|    rollout_return       | -1.957458     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 374           |
|    time_elapsed         | 57396         |
|    total_timesteps      | 95744         |
| train/                  |               |
|    active_example       | 148           |
|    approx_kl            | 0.00072202086 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0234       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 74600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00196       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00972      |
|    negative_advantag... | 0.059338264  |
|    positive_advantag... | 0.15132593   |
|    prob_ratio           | 496841.38    |
|    rollout_return       | -1.8136451   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 375          |
|    time_elapsed         | 57558        |
|    total_timesteps      | 96000        |
| train/                  |              |
|    active_example       | 113          |
|    approx_kl            | 0.0032268167 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0268      |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 74800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00681      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00929       |
|    negative_advantag... | 0.07863516    |
|    positive_advantag... | 0.096462585   |
|    prob_ratio           | 348154.84     |
|    rollout_return       | -1.5654328    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 376           |
|    time_elapsed         | 57719         |
|    total_timesteps      | 96256         |
| train/                  |               |
|    active_example       | 251           |
|    approx_kl            | 0.00032651424 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 75000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00326       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0129        |
|    negative_advantag... | 0.081936315   |
|    positive_advantag... | 0.13044658    |
|    prob_ratio           | 335692.9      |
|    rollout_return       | -2.1228542    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 377           |
|    time_elapsed         | 57881         |
|    total_timesteps      | 96512         |
| train/                  |               |
|    active_example       | 149           |
|    approx_kl            | -0.0068496466 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0199       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 75200         |
|    policy_gradient_loss | 0.00149       |
|    value_loss           | 0.00141       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00924      |
|    negative_advantag... | 0.06726459   |
|    positive_advantag... | 0.15752062   |
|    prob_ratio           | 884095.06    |
|    rollout_return       | -1.9203912   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 378          |
|    time_elapsed         | 58043        |
|    total_timesteps      | 96768        |
| train/                  |              |
|    active_example       | 114          |
|    approx_kl            | 0.0028047562 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0268      |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 75400        |
|    policy_gradient_loss | 0.00221      |
|    value_loss           | 0.00303      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00778        |
|    negative_advantag... | 0.049866047    |
|    positive_advantag... | 0.12187229     |
|    prob_ratio           | 405775.84      |
|    rollout_return       | -2.14329       |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00947        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00982        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 379            |
|    time_elapsed         | 58205          |
|    total_timesteps      | 97024          |
| train/                  |                |
|    active_example       | 253            |
|    approx_kl            | -2.5510788e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0159        |
|    explained_variance   | 0.98           |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 75600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00133        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0091       |
|    negative_advantag... | 0.05860995   |
|    positive_advantag... | 0.12699556   |
|    prob_ratio           | 442436.8     |
|    rollout_return       | -1.8325418   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00986      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 380          |
|    time_elapsed         | 58367        |
|    total_timesteps      | 97280        |
| train/                  |              |
|    active_example       | 167          |
|    approx_kl            | 0.0005803704 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0241      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 75800        |
|    policy_gradient_loss | 0.000147     |
|    value_loss           | 0.00572      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0128        |
|    negative_advantag... | 0.08881786    |
|    positive_advantag... | 0.14504942    |
|    prob_ratio           | 623555.4      |
|    rollout_return       | -2.2751815    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 381           |
|    time_elapsed         | 58529         |
|    total_timesteps      | 97536         |
| train/                  |               |
|    active_example       | 125           |
|    approx_kl            | -0.0029947162 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 76000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00237       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0106        |
|    negative_advantag... | 0.07679617    |
|    positive_advantag... | 0.14572369    |
|    prob_ratio           | 158601.95     |
|    rollout_return       | -1.9187994    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 382           |
|    time_elapsed         | 58690         |
|    total_timesteps      | 97792         |
| train/                  |               |
|    active_example       | 251           |
|    approx_kl            | 1.5735626e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 76200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00261       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0122        |
|    negative_advantag... | 0.087797105   |
|    positive_advantag... | 0.11462478    |
|    prob_ratio           | 261483.1      |
|    rollout_return       | -1.9506656    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 383           |
|    time_elapsed         | 58853         |
|    total_timesteps      | 98048         |
| train/                  |               |
|    active_example       | 149           |
|    approx_kl            | -3.874302e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.025        |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 76400         |
|    policy_gradient_loss | 0.0023        |
|    value_loss           | 0.00155       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00983    |
|    negative_advantag... | 0.07755003 |
|    positive_advantag... | 0.11844624 |
|    prob_ratio           | 784510.9   |
|    rollout_return       | -2.1684892 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.18       |
|    collect_rollout/Sum  | 4.18       |
|    train_action_adv/... | 0.00951    |
|    train_action_adv/Sum | 30.4       |
|    train_computeV/Mean  | 0.0181     |
|    train_computeV/Sum   | 58         |
|    train_epoch/Mean     | 157        |
|    train_epoch/Sum      | 157        |
|    train_loss/Mean      | 0.00986    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 384        |
|    time_elapsed         | 59014      |
|    total_timesteps      | 98304      |
| train/                  |            |
|    active_example       | 110        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0203    |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 76600      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00128    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0101      |
|    negative_advantag... | 0.08655348  |
|    positive_advantag... | 0.14860295  |
|    prob_ratio           | 1080506.9   |
|    rollout_return       | -2.2002404  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00989     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 385         |
|    time_elapsed         | 59177       |
|    total_timesteps      | 98560       |
| train/                  |             |
|    active_example       | 253         |
|    approx_kl            | 0.003359139 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0115     |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.01        |
|    loss                 | 0.29        |
|    n_updates            | 76800       |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 0.0026      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0128        |
|    negative_advantag... | 0.1102037     |
|    positive_advantag... | 0.11967914    |
|    prob_ratio           | 53130.83      |
|    rollout_return       | -1.8372521    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 386           |
|    time_elapsed         | 59339         |
|    total_timesteps      | 98816         |
| train/                  |               |
|    active_example       | 148           |
|    approx_kl            | 5.9604645e-08 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0241       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 77000         |
|    policy_gradient_loss | 0.00119       |
|    value_loss           | 0.00322       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0106        |
|    negative_advantag... | 0.086101875   |
|    positive_advantag... | 0.08911311    |
|    prob_ratio           | 402688.44     |
|    rollout_return       | -1.9708911    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 387           |
|    time_elapsed         | 59501         |
|    total_timesteps      | 99072         |
| train/                  |               |
|    active_example       | 124           |
|    approx_kl            | -4.172325e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0158       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 77200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00112       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.011       |
|    negative_advantag... | 0.08968399  |
|    positive_advantag... | 0.10848015  |
|    prob_ratio           | 871180.5    |
|    rollout_return       | -2.0512948  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 388         |
|    time_elapsed         | 59662       |
|    total_timesteps      | 99328       |
| train/                  |             |
|    active_example       | 252         |
|    approx_kl            | 0.000254035 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0232     |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 77400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.000741    |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00824      |
|    negative_advantag... | 0.06865974   |
|    positive_advantag... | 0.11803952   |
|    prob_ratio           | 819672.4     |
|    rollout_return       | -1.6366528   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 389          |
|    time_elapsed         | 59823        |
|    total_timesteps      | 99584        |
| train/                  |              |
|    active_example       | 150          |
|    approx_kl            | 0.0002450049 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.016       |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 77600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00374      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0116        |
|    negative_advantag... | 0.08467269    |
|    positive_advantag... | 0.12335473    |
|    prob_ratio           | 771616.56     |
|    rollout_return       | -2.0372236    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 390           |
|    time_elapsed         | 59985         |
|    total_timesteps      | 99840         |
| train/                  |               |
|    active_example       | 127           |
|    approx_kl            | 5.9604645e-08 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 77800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000718      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=100000, episode_reward=-167.60 +/- 31.61
Episode length: 168.60 +/- 31.61
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00806       |
|    negative_advantag... | 0.06846008    |
|    positive_advantag... | 0.083590716   |
|    prob_ratio           | 1016658.8     |
|    rollout_return       | -1.6465087    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 5.79          |
|    collect_rollout/Sum  | 5.79          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.7          |
| eval/                   |               |
|    mean_ep_length       | 169           |
|    mean_reward          | -168          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 391           |
|    time_elapsed         | 60149         |
|    total_timesteps      | 100096        |
| train/                  |               |
|    active_example       | 252           |
|    approx_kl            | 0.00010856986 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00699      |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 78000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00298       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0102       |
|    negative_advantag... | 0.06933051   |
|    positive_advantag... | 0.12854062   |
|    prob_ratio           | 402678.97    |
|    rollout_return       | -1.8371478   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 392          |
|    time_elapsed         | 60310        |
|    total_timesteps      | 100352       |
| train/                  |              |
|    active_example       | 137          |
|    approx_kl            | 0.0006887913 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 78200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00145      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0116         |
|    negative_advantag... | 0.097116515    |
|    positive_advantag... | 0.08219992     |
|    prob_ratio           | 768590.94      |
|    rollout_return       | -1.8039948     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00953        |
|    train_action_adv/Sum | 30.5           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 158            |
|    ep_rew_mean          | -157           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 393            |
|    time_elapsed         | 60472          |
|    total_timesteps      | 100608         |
| train/                  |                |
|    active_example       | 148            |
|    approx_kl            | -0.00011187792 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0169        |
|    explained_variance   | 0.99           |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 78400          |
|    policy_gradient_loss | 6.26e-05       |
|    value_loss           | 0.00131        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0121       |
|    negative_advantag... | 0.08779103   |
|    positive_advantag... | 0.13420127   |
|    prob_ratio           | 714558.3     |
|    rollout_return       | -2.0733132   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.7         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00977      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 394          |
|    time_elapsed         | 60634        |
|    total_timesteps      | 100864       |
| train/                  |              |
|    active_example       | 251          |
|    approx_kl            | 0.0009215176 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0186      |
|    explained_variance   | 0.908        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 78600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00765      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
---------------------------------------------
| HPO/                    |                 |
|    margin               | 0.0105          |
|    negative_advantag... | 0.06836581      |
|    positive_advantag... | 0.14032568      |
|    prob_ratio           | 1111289.8       |
|    rollout_return       | -1.9313948      |
| Time/                   |                 |
|    collect_computeV/... | 0.0124          |
|    collect_computeV/Sum | 3.19            |
|    collect_rollout/Mean | 4.19            |
|    collect_rollout/Sum  | 4.19            |
|    train_action_adv/... | 0.00947         |
|    train_action_adv/Sum | 30.3            |
|    train_computeV/Mean  | 0.0183          |
|    train_computeV/Sum   | 58.6            |
|    train_epoch/Mean     | 158             |
|    train_epoch/Sum      | 158             |
|    train_loss/Mean      | 0.00977         |
|    train_loss/Sum       | 31.3            |
| rollout/                |                 |
|    ep_len_mean          | 158             |
|    ep_rew_mean          | -157            |
| time/                   |                 |
|    fps                  | 1               |
|    iterations           | 395             |
|    time_elapsed         | 60796           |
|    total_timesteps      | 101120          |
| train/                  |                 |
|    active_example       | 154             |
|    approx_kl            | -0.000110566616 |
|    clip_range           | 0.2             |
|    entropy_loss         | -0.014          |
|    explained_variance   | 0.953           |
|    learning_rate        | 0.01            |
|    loss                 | 0               |
|    n_updates            | 78800           |
|    policy_gradient_loss | 0               |
|    value_loss           | 0.0026          |
---------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0116      |
|    negative_advantag... | 0.08547488  |
|    positive_advantag... | 0.12374394  |
|    prob_ratio           | 364562.9    |
|    rollout_return       | -1.8317215  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.21        |
|    collect_rollout/Mean | 4.22        |
|    collect_rollout/Sum  | 4.22        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 396         |
|    time_elapsed         | 60958       |
|    total_timesteps      | 101376      |
| train/                  |             |
|    active_example       | 115         |
|    approx_kl            | 0.012626752 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0291     |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 79000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00135     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0102       |
|    negative_advantag... | 0.0730096    |
|    positive_advantag... | 0.09075995   |
|    prob_ratio           | 1009636.4    |
|    rollout_return       | -1.8280598   |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.22         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 397          |
|    time_elapsed         | 61120        |
|    total_timesteps      | 101632       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | -0.025845975 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 79200        |
|    policy_gradient_loss | 9.16e-05     |
|    value_loss           | 0.00252      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00979        |
|    negative_advantag... | 0.07629576     |
|    positive_advantag... | 0.14729816     |
|    prob_ratio           | 862057.5       |
|    rollout_return       | -1.646217      |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.00951        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 159            |
|    ep_rew_mean          | -158           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 398            |
|    time_elapsed         | 61283          |
|    total_timesteps      | 101888         |
| train/                  |                |
|    active_example       | 147            |
|    approx_kl            | -7.1525574e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0152        |
|    explained_variance   | 0.944          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 79400          |
|    policy_gradient_loss | 0.000905       |
|    value_loss           | 0.00162        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00847     |
|    negative_advantag... | 0.054877754 |
|    positive_advantag... | 0.18070823  |
|    prob_ratio           | 921135.94   |
|    rollout_return       | -2.071174   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 399         |
|    time_elapsed         | 61444       |
|    total_timesteps      | 102144      |
| train/                  |             |
|    active_example       | 120         |
|    approx_kl            | 0.009179875 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0253     |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 79600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00113     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0116       |
|    negative_advantag... | 0.09200656   |
|    positive_advantag... | 0.11982972   |
|    prob_ratio           | 843083.6     |
|    rollout_return       | -1.9243828   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.0094       |
|    train_action_adv/Sum | 30.1         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 400          |
|    time_elapsed         | 61606        |
|    total_timesteps      | 102400       |
| train/                  |              |
|    active_example       | 246          |
|    approx_kl            | 0.0064540505 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 79800        |
|    policy_gradient_loss | 0.000272     |
|    value_loss           | 0.00457      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00939     |
|    negative_advantag... | 0.07525816  |
|    positive_advantag... | 0.10421456  |
|    prob_ratio           | 125780.72   |
|    rollout_return       | -1.81322    |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00948     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 401         |
|    time_elapsed         | 61768       |
|    total_timesteps      | 102656      |
| train/                  |             |
|    active_example       | 126         |
|    approx_kl            | 0.012951568 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0226     |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 80000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00353     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00978       |
|    negative_advantag... | 0.07216657    |
|    positive_advantag... | 0.15121242    |
|    prob_ratio           | 78023.56      |
|    rollout_return       | -1.9810092    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00953       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00993       |
|    train_loss/Sum       | 31.8          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 402           |
|    time_elapsed         | 61930         |
|    total_timesteps      | 102912        |
| train/                  |               |
|    active_example       | 128           |
|    approx_kl            | 3.4213066e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 80200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00147       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0101        |
|    negative_advantag... | 0.07315836    |
|    positive_advantag... | 0.09059173    |
|    prob_ratio           | 561048.06     |
|    rollout_return       | -1.7037085    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00989       |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 403           |
|    time_elapsed         | 62091         |
|    total_timesteps      | 103168        |
| train/                  |               |
|    active_example       | 254           |
|    approx_kl            | 3.5762787e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 80400         |
|    policy_gradient_loss | 0.000199      |
|    value_loss           | 0.00179       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0132      |
|    negative_advantag... | 0.09951473  |
|    positive_advantag... | 0.122477494 |
|    prob_ratio           | 1339363.8   |
|    rollout_return       | -1.973949   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 404         |
|    time_elapsed         | 62253       |
|    total_timesteps      | 103424      |
| train/                  |             |
|    active_example       | 146         |
|    approx_kl            | 0.017209947 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0238     |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 80600       |
|    policy_gradient_loss | 0.000325    |
|    value_loss           | 0.00286     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0122       |
|    negative_advantag... | 0.09300477   |
|    positive_advantag... | 0.13021807   |
|    prob_ratio           | 516881.8     |
|    rollout_return       | -2.1226525   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 405          |
|    time_elapsed         | 62415        |
|    total_timesteps      | 103680       |
| train/                  |              |
|    active_example       | 118          |
|    approx_kl            | -0.013649166 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 80800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00742      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00945       |
|    negative_advantag... | 0.07707701    |
|    positive_advantag... | 0.13489579    |
|    prob_ratio           | 817170.6      |
|    rollout_return       | -1.9269408    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 406           |
|    time_elapsed         | 62578         |
|    total_timesteps      | 103936        |
| train/                  |               |
|    active_example       | 250           |
|    approx_kl            | -0.0020878017 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 81000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00245       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0148       |
|    negative_advantag... | 0.1170268    |
|    positive_advantag... | 0.11307092   |
|    prob_ratio           | 431852.4     |
|    rollout_return       | -2.0652642   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00992      |
|    train_loss/Sum       | 31.8         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 407          |
|    time_elapsed         | 62740        |
|    total_timesteps      | 104192       |
| train/                  |              |
|    active_example       | 142          |
|    approx_kl            | 0.0065448284 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0217      |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 81200        |
|    policy_gradient_loss | 0.00419      |
|    value_loss           | 0.00601      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0107        |
|    negative_advantag... | 0.08331483    |
|    positive_advantag... | 0.11648998    |
|    prob_ratio           | 1591795.9     |
|    rollout_return       | -1.9560698    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 408           |
|    time_elapsed         | 62902         |
|    total_timesteps      | 104448        |
| train/                  |               |
|    active_example       | 121           |
|    approx_kl            | -0.0017615259 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 81400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00119       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0106     |
|    negative_advantag... | 0.0707418  |
|    positive_advantag... | 0.15123102 |
|    prob_ratio           | 1225021.5  |
|    rollout_return       | -2.0406594 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.17       |
|    collect_rollout/Mean | 4.17       |
|    collect_rollout/Sum  | 4.17       |
|    train_action_adv/... | 0.00948    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.2       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00985    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | -157       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 409        |
|    time_elapsed         | 63064      |
|    total_timesteps      | 104704     |
| train/                  |            |
|    active_example       | 252        |
|    approx_kl            | 0.04927337 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0177    |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 81600      |
|    policy_gradient_loss | 0.0004     |
|    value_loss           | 0.00102    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0129       |
|    negative_advantag... | 0.10326475   |
|    positive_advantag... | 0.11255559   |
|    prob_ratio           | 794158.94    |
|    rollout_return       | -1.5402896   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 410          |
|    time_elapsed         | 63226        |
|    total_timesteps      | 104960       |
| train/                  |              |
|    active_example       | 146          |
|    approx_kl            | 0.0023680478 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0206      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 81800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00366      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0115        |
|    negative_advantag... | 0.07648517    |
|    positive_advantag... | 0.11917901    |
|    prob_ratio           | 817945.94     |
|    rollout_return       | -1.8269055    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 411           |
|    time_elapsed         | 63388         |
|    total_timesteps      | 105216        |
| train/                  |               |
|    active_example       | 117           |
|    approx_kl            | -0.0050581098 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.024        |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 82000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000885      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0113       |
|    negative_advantag... | 0.085159644  |
|    positive_advantag... | 0.1328482    |
|    prob_ratio           | 1136327.4    |
|    rollout_return       | -2.0069122   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 412          |
|    time_elapsed         | 63550        |
|    total_timesteps      | 105472       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | 0.0009227097 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.02        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 82200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00219      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0056        |
|    negative_advantag... | 0.040691562   |
|    positive_advantag... | 0.08821474    |
|    prob_ratio           | 392364.72     |
|    rollout_return       | -1.2322346    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00941       |
|    train_action_adv/Sum | 30.1          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00976       |
|    train_loss/Sum       | 31.2          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 413           |
|    time_elapsed         | 63712         |
|    total_timesteps      | 105728        |
| train/                  |               |
|    active_example       | 127           |
|    approx_kl            | -0.0012619495 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0131       |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 82400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00322       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00981     |
|    negative_advantag... | 0.07226771  |
|    positive_advantag... | 0.18548518  |
|    prob_ratio           | 571965.25   |
|    rollout_return       | -2.0637336  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 414         |
|    time_elapsed         | 63874       |
|    total_timesteps      | 105984      |
| train/                  |             |
|    active_example       | 128         |
|    approx_kl            | 0.001735568 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0251     |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 82600       |
|    policy_gradient_loss | 0.00517     |
|    value_loss           | 0.00194     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0114        |
|    negative_advantag... | 0.087342784   |
|    positive_advantag... | 0.1128721     |
|    prob_ratio           | 137898.1      |
|    rollout_return       | -1.8999082    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 415           |
|    time_elapsed         | 64036         |
|    total_timesteps      | 106240        |
| train/                  |               |
|    active_example       | 253           |
|    approx_kl            | 3.6418438e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 82800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00412       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00913       |
|    negative_advantag... | 0.06082561    |
|    positive_advantag... | 0.115834706   |
|    prob_ratio           | 1076886.5     |
|    rollout_return       | -1.7544452    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 416           |
|    time_elapsed         | 64198         |
|    total_timesteps      | 106496        |
| train/                  |               |
|    active_example       | 131           |
|    approx_kl            | 3.9815903e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 83000         |
|    policy_gradient_loss | 0.00143       |
|    value_loss           | 0.00126       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0115       |
|    negative_advantag... | 0.08786744   |
|    positive_advantag... | 0.117659934  |
|    prob_ratio           | 784978.5     |
|    rollout_return       | -1.7633524   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 417          |
|    time_elapsed         | 64360        |
|    total_timesteps      | 106752       |
| train/                  |              |
|    active_example       | 116          |
|    approx_kl            | -0.019062907 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0224      |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 83200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00456      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0123         |
|    negative_advantag... | 0.09510212     |
|    positive_advantag... | 0.080913536    |
|    prob_ratio           | 403244.0       |
|    rollout_return       | -1.8844472     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.00945        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 158            |
|    ep_rew_mean          | -157           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 418            |
|    time_elapsed         | 64521          |
|    total_timesteps      | 107008         |
| train/                  |                |
|    active_example       | 251            |
|    approx_kl            | -0.00087067485 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.02          |
|    explained_variance   | 0.972          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 83400          |
|    policy_gradient_loss | 0.000231       |
|    value_loss           | 0.00144        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0117        |
|    negative_advantag... | 0.09387729    |
|    positive_advantag... | 0.08038055    |
|    prob_ratio           | 26549.578     |
|    rollout_return       | -1.784628     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 419           |
|    time_elapsed         | 64684         |
|    total_timesteps      | 107264        |
| train/                  |               |
|    active_example       | 133           |
|    approx_kl            | 3.2782555e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0252       |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 83600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00134       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0132       |
|    negative_advantag... | 0.11546063   |
|    positive_advantag... | 0.09862144   |
|    prob_ratio           | 499682.4     |
|    rollout_return       | -1.7647055   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 420          |
|    time_elapsed         | 64846        |
|    total_timesteps      | 107520       |
| train/                  |              |
|    active_example       | 127          |
|    approx_kl            | 0.0009507537 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0174      |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 83800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00108      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00913       |
|    negative_advantag... | 0.0786442     |
|    positive_advantag... | 0.102547504   |
|    prob_ratio           | 409660.56     |
|    rollout_return       | -1.8400371    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 421           |
|    time_elapsed         | 65007         |
|    total_timesteps      | 107776        |
| train/                  |               |
|    active_example       | 254           |
|    approx_kl            | -4.529953e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 84000         |
|    policy_gradient_loss | 0.000565      |
|    value_loss           | 0.00308       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0128       |
|    negative_advantag... | 0.0944048    |
|    positive_advantag... | 0.122314006  |
|    prob_ratio           | 610821.94    |
|    rollout_return       | -1.7319492   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 422          |
|    time_elapsed         | 65169        |
|    total_timesteps      | 108032       |
| train/                  |              |
|    active_example       | 142          |
|    approx_kl            | 2.130866e-05 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 84200        |
|    policy_gradient_loss | 0.000574     |
|    value_loss           | 0.00448      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0109      |
|    negative_advantag... | 0.07990167  |
|    positive_advantag... | 0.14132884  |
|    prob_ratio           | 1316209.9   |
|    rollout_return       | -1.7927022  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 159         |
|    train_epoch/Sum      | 159         |
|    train_loss/Mean      | 0.00992     |
|    train_loss/Sum       | 31.7        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 423         |
|    time_elapsed         | 65331       |
|    total_timesteps      | 108288      |
| train/                  |             |
|    active_example       | 118         |
|    approx_kl            | 0.038061947 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0316     |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 84400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00284     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00792        |
|    negative_advantag... | 0.04342655     |
|    positive_advantag... | 0.1523158      |
|    prob_ratio           | 563086.75      |
|    rollout_return       | -1.7303075     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00952        |
|    train_action_adv/Sum | 30.5           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00986        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 158            |
|    ep_rew_mean          | -157           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 424            |
|    time_elapsed         | 65494          |
|    total_timesteps      | 108544         |
| train/                  |                |
|    active_example       | 252            |
|    approx_kl            | -5.9604645e-07 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0123        |
|    explained_variance   | 0.971          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 84600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00192        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0109       |
|    negative_advantag... | 0.07980341   |
|    positive_advantag... | 0.10287246   |
|    prob_ratio           | 160680.25    |
|    rollout_return       | -1.7395406   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00951      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | -157         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 425          |
|    time_elapsed         | 65656        |
|    total_timesteps      | 108800       |
| train/                  |              |
|    active_example       | 144          |
|    approx_kl            | 0.0016180873 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0239      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 84800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00209      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00831       |
|    negative_advantag... | 0.072252214   |
|    positive_advantag... | 0.11098999    |
|    prob_ratio           | 368211.38     |
|    rollout_return       | -1.7674714    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 426           |
|    time_elapsed         | 65818         |
|    total_timesteps      | 109056        |
| train/                  |               |
|    active_example       | 131           |
|    approx_kl            | 6.3180923e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.976         |
|    learning_rate        | 0.01          |
|    loss                 | 0.0635        |
|    n_updates            | 85000         |
|    policy_gradient_loss | 0.000939      |
|    value_loss           | 0.000924      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0134        |
|    negative_advantag... | 0.10653303    |
|    positive_advantag... | 0.08544036    |
|    prob_ratio           | 1250892.0     |
|    rollout_return       | -1.9676191    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 427           |
|    time_elapsed         | 65980         |
|    total_timesteps      | 109312        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | -4.172325e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 85200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000999      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0117      |
|    negative_advantag... | 0.08546844  |
|    positive_advantag... | 0.0839652   |
|    prob_ratio           | 688286.0    |
|    rollout_return       | -1.6851027  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.17        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00982     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 428         |
|    time_elapsed         | 66141       |
|    total_timesteps      | 109568      |
| train/                  |             |
|    active_example       | 189         |
|    approx_kl            | 0.023658395 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0237     |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 85400       |
|    policy_gradient_loss | 0.000318    |
|    value_loss           | 0.00484     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0119        |
|    negative_advantag... | 0.097289026   |
|    positive_advantag... | 0.1320274     |
|    prob_ratio           | 796848.06     |
|    rollout_return       | -1.9189224    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00951       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 429           |
|    time_elapsed         | 66303         |
|    total_timesteps      | 109824        |
| train/                  |               |
|    active_example       | 124           |
|    approx_kl            | 1.7881393e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0275       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 85600         |
|    policy_gradient_loss | 1.52e-05      |
|    value_loss           | 0.000935      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=110000, episode_reward=-169.40 +/- 26.72
Episode length: 170.40 +/- 26.72
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00949       |
|    negative_advantag... | 0.07305363    |
|    positive_advantag... | 0.107942544   |
|    prob_ratio           | 446239.84     |
|    rollout_return       | -1.7265177    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 5.77          |
|    collect_rollout/Sum  | 5.77          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| eval/                   |               |
|    mean_ep_length       | 170           |
|    mean_reward          | -169          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 430           |
|    time_elapsed         | 66466         |
|    total_timesteps      | 110080        |
| train/                  |               |
|    active_example       | 251           |
|    approx_kl            | 0.00023037195 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 85800         |
|    policy_gradient_loss | 0.000176      |
|    value_loss           | 0.00282       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00725     |
|    negative_advantag... | 0.0603142   |
|    positive_advantag... | 0.1317758   |
|    prob_ratio           | 304456.44   |
|    rollout_return       | -1.87643    |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 431         |
|    time_elapsed         | 66628       |
|    total_timesteps      | 110336      |
| train/                  |             |
|    active_example       | 168         |
|    approx_kl            | 0.028809339 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0129     |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0738      |
|    n_updates            | 86000       |
|    policy_gradient_loss | 0.000264    |
|    value_loss           | 0.00864     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0118      |
|    negative_advantag... | 0.10234992  |
|    positive_advantag... | 0.10643915  |
|    prob_ratio           | 607135.06   |
|    rollout_return       | -1.8766115  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00975     |
|    train_loss/Sum       | 31.2        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 432         |
|    time_elapsed         | 66790       |
|    total_timesteps      | 110592      |
| train/                  |             |
|    active_example       | 113         |
|    approx_kl            | 0.040883154 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.019      |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 86200       |
|    policy_gradient_loss | 0.00197     |
|    value_loss           | 0.00437     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0122      |
|    negative_advantag... | 0.0960068   |
|    positive_advantag... | 0.09323159  |
|    prob_ratio           | 699758.25   |
|    rollout_return       | -2.1390195  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | -157        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 433         |
|    time_elapsed         | 66952       |
|    total_timesteps      | 110848      |
| train/                  |             |
|    active_example       | 255         |
|    approx_kl            | 0.034684673 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0165     |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 86400       |
|    policy_gradient_loss | 0.000148    |
|    value_loss           | 0.00221     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00966       |
|    negative_advantag... | 0.078078695   |
|    positive_advantag... | 0.104597166   |
|    prob_ratio           | 295693.8      |
|    rollout_return       | -1.5562522    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 434           |
|    time_elapsed         | 67113         |
|    total_timesteps      | 111104        |
| train/                  |               |
|    active_example       | 131           |
|    approx_kl            | 5.9604645e-08 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0182       |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 86600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0022        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0108        |
|    negative_advantag... | 0.07661989    |
|    positive_advantag... | 0.104688734   |
|    prob_ratio           | 467930.7      |
|    rollout_return       | -1.9473151    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 435           |
|    time_elapsed         | 67276         |
|    total_timesteps      | 111360        |
| train/                  |               |
|    active_example       | 116           |
|    approx_kl            | 1.7881393e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 86800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00517       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0101       |
|    negative_advantag... | 0.07653983   |
|    positive_advantag... | 0.12240572   |
|    prob_ratio           | 443148.84    |
|    rollout_return       | -1.9030076   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | -155         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 436          |
|    time_elapsed         | 67438        |
|    total_timesteps      | 111616       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | 0.0019786358 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 87000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00191      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00921     |
|    negative_advantag... | 0.072695844 |
|    positive_advantag... | 0.11931727  |
|    prob_ratio           | 384129.53   |
|    rollout_return       | -1.5293856  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.0098      |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | -155        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 437         |
|    time_elapsed         | 67600       |
|    total_timesteps      | 111872      |
| train/                  |             |
|    active_example       | 167         |
|    approx_kl            | 0.022858351 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0294     |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 87200       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.0074      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0094      |
|    negative_advantag... | 0.075322084 |
|    positive_advantag... | 0.074306935 |
|    prob_ratio           | 773552.75   |
|    rollout_return       | -1.6277118  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00951     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00989     |
|    train_loss/Sum       | 31.7        |
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | -155        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 438         |
|    time_elapsed         | 67762       |
|    total_timesteps      | 112128      |
| train/                  |             |
|    active_example       | 148         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00961    |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 87400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00267     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00773     |
|    negative_advantag... | 0.053271163 |
|    positive_advantag... | 0.12706089  |
|    prob_ratio           | 1081732.6   |
|    rollout_return       | -1.9343598  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00985     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | -154        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 439         |
|    time_elapsed         | 67923       |
|    total_timesteps      | 112384      |
| train/                  |             |
|    active_example       | 254         |
|    approx_kl            | 0.004387617 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0169     |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0923      |
|    n_updates            | 87600       |
|    policy_gradient_loss | 0.000536    |
|    value_loss           | 0.00714     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00848       |
|    negative_advantag... | 0.061165642   |
|    positive_advantag... | 0.11455703    |
|    prob_ratio           | 223919.42     |
|    rollout_return       | -1.8499256    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0181        |
|    train_computeV/Sum   | 58            |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0099        |
|    train_loss/Sum       | 31.7          |
| rollout/                |               |
|    ep_len_mean          | 155           |
|    ep_rew_mean          | -154          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 440           |
|    time_elapsed         | 68086         |
|    total_timesteps      | 112640        |
| train/                  |               |
|    active_example       | 140           |
|    approx_kl            | 1.1384487e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0166       |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 87800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00256       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00816       |
|    negative_advantag... | 0.06611041    |
|    positive_advantag... | 0.10502245    |
|    prob_ratio           | 439064.38     |
|    rollout_return       | -1.6564155    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 154           |
|    ep_rew_mean          | -153          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 441           |
|    time_elapsed         | 68247         |
|    total_timesteps      | 112896        |
| train/                  |               |
|    active_example       | 112           |
|    approx_kl            | 0.00011819601 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 88000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00319       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00726      |
|    negative_advantag... | 0.05294515   |
|    positive_advantag... | 0.1164885    |
|    prob_ratio           | 213625.5     |
|    rollout_return       | -1.8113061   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00944      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00994      |
|    train_loss/Sum       | 31.8         |
| rollout/                |              |
|    ep_len_mean          | 154          |
|    ep_rew_mean          | -153         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 442          |
|    time_elapsed         | 68410        |
|    total_timesteps      | 113152       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | -8.34465e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 88200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00113      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0105       |
|    negative_advantag... | 0.087598436  |
|    positive_advantag... | 0.106620446  |
|    prob_ratio           | 555153.06    |
|    rollout_return       | -1.805552    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00953      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 155          |
|    ep_rew_mean          | -154         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 443          |
|    time_elapsed         | 68572        |
|    total_timesteps      | 113408       |
| train/                  |              |
|    active_example       | 141          |
|    approx_kl            | 4.810095e-05 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0131      |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 88400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00145      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00916       |
|    negative_advantag... | 0.07643745    |
|    positive_advantag... | 0.09498834    |
|    prob_ratio           | 694401.06     |
|    rollout_return       | -1.4619689    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 155           |
|    ep_rew_mean          | -154          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 444           |
|    time_elapsed         | 68734         |
|    total_timesteps      | 113664        |
| train/                  |               |
|    active_example       | 108           |
|    approx_kl            | 5.9604645e-08 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 88600         |
|    policy_gradient_loss | 0.000918      |
|    value_loss           | 0.00255       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0111        |
|    negative_advantag... | 0.08078186    |
|    positive_advantag... | 0.09531247    |
|    prob_ratio           | 607040.25     |
|    rollout_return       | -1.905509     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 155           |
|    ep_rew_mean          | -154          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 445           |
|    time_elapsed         | 68897         |
|    total_timesteps      | 113920        |
| train/                  |               |
|    active_example       | 254           |
|    approx_kl            | 1.1920929e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0161       |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 88800         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00149       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00831       |
|    negative_advantag... | 0.07062489    |
|    positive_advantag... | 0.12310566    |
|    prob_ratio           | 775396.56     |
|    rollout_return       | -1.8581362    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 156           |
|    ep_rew_mean          | -155          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 446           |
|    time_elapsed         | 69058         |
|    total_timesteps      | 114176        |
| train/                  |               |
|    active_example       | 141           |
|    approx_kl            | -0.0065743476 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00998      |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 89000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00299       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00534      |
|    negative_advantag... | 0.043707967  |
|    positive_advantag... | 0.05539362   |
|    prob_ratio           | 612366.75    |
|    rollout_return       | -1.0890191   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0181       |
|    train_computeV/Sum   | 58           |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0099       |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | -155         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 447          |
|    time_elapsed         | 69220        |
|    total_timesteps      | 114432       |
| train/                  |              |
|    active_example       | 197          |
|    approx_kl            | 0.0013114214 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00593     |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 89200        |
|    policy_gradient_loss | 0.00263      |
|    value_loss           | 0.00332      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00538       |
|    negative_advantag... | 0.035884336   |
|    positive_advantag... | 0.04997504    |
|    prob_ratio           | 762898.2      |
|    rollout_return       | -1.405533     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 448           |
|    time_elapsed         | 69382         |
|    total_timesteps      | 114688        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | 0.00038677454 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 89400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00367       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00614     |
|    negative_advantag... | 0.045035493 |
|    positive_advantag... | 0.08006224  |
|    prob_ratio           | 359484.44   |
|    rollout_return       | -1.7334659  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.22        |
|    collect_rollout/Mean | 4.24        |
|    collect_rollout/Sum  | 4.24        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 449         |
|    time_elapsed         | 69544       |
|    total_timesteps      | 114944      |
| train/                  |             |
|    active_example       | 83          |
|    approx_kl            | 0.00452441  |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0102     |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 89600       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00758     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00944      |
|    negative_advantag... | 0.07781081   |
|    positive_advantag... | 0.09988458   |
|    prob_ratio           | 1059136.1    |
|    rollout_return       | -2.045379    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00978      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 450          |
|    time_elapsed         | 69706        |
|    total_timesteps      | 115200       |
| train/                  |              |
|    active_example       | 133          |
|    approx_kl            | -0.016991407 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0141      |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 89800        |
|    policy_gradient_loss | 0.000955     |
|    value_loss           | 0.00603      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00696       |
|    negative_advantag... | 0.05737594    |
|    positive_advantag... | 0.044010855   |
|    prob_ratio           | 622020.2      |
|    rollout_return       | -1.922088     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00943       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.7          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 451           |
|    time_elapsed         | 69868         |
|    total_timesteps      | 115456        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | 2.4437904e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.825         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 90000         |
|    policy_gradient_loss | 0.000369      |
|    value_loss           | 0.00987       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00929       |
|    negative_advantag... | 0.07788525    |
|    positive_advantag... | 0.12092342    |
|    prob_ratio           | 738508.56     |
|    rollout_return       | -2.0203815    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0184        |
|    train_computeV/Sum   | 58.8          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 452           |
|    time_elapsed         | 70031         |
|    total_timesteps      | 115712        |
| train/                  |               |
|    active_example       | 138           |
|    approx_kl            | 0.00076293945 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 90200         |
|    policy_gradient_loss | 0.000846      |
|    value_loss           | 0.00417       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0104         |
|    negative_advantag... | 0.08050291     |
|    positive_advantag... | 0.1268995      |
|    prob_ratio           | 188223.34      |
|    rollout_return       | -2.2193975     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.17           |
|    collect_rollout/Mean | 4.16           |
|    collect_rollout/Sum  | 4.16           |
|    train_action_adv/... | 0.00953        |
|    train_action_adv/Sum | 30.5           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 159            |
|    ep_rew_mean          | -158           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 453            |
|    time_elapsed         | 70193          |
|    total_timesteps      | 115968         |
| train/                  |                |
|    active_example       | 118            |
|    approx_kl            | -8.3595514e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0213        |
|    explained_variance   | 0.879          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 90400          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00365        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00949       |
|    negative_advantag... | 0.06939267    |
|    positive_advantag... | 0.10388865    |
|    prob_ratio           | 922611.8      |
|    rollout_return       | -2.0458565    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 454           |
|    time_elapsed         | 70355         |
|    total_timesteps      | 116224        |
| train/                  |               |
|    active_example       | 254           |
|    approx_kl            | 0.00022619963 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0156       |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 90600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00161       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0109         |
|    negative_advantag... | 0.08387364     |
|    positive_advantag... | 0.13163425     |
|    prob_ratio           | 1235464.5      |
|    rollout_return       | -2.0477128     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00945        |
|    train_action_adv/Sum | 30.2           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00983        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 455            |
|    time_elapsed         | 70517          |
|    total_timesteps      | 116480         |
| train/                  |                |
|    active_example       | 133            |
|    approx_kl            | -0.00012427568 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0121        |
|    explained_variance   | 0.959          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 90800          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00326        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00742       |
|    negative_advantag... | 0.04884942    |
|    positive_advantag... | 0.090974845   |
|    prob_ratio           | 758948.0      |
|    rollout_return       | -1.8583541    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 456           |
|    time_elapsed         | 70679         |
|    total_timesteps      | 116736        |
| train/                  |               |
|    active_example       | 116           |
|    approx_kl            | 1.9073486e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 91000         |
|    policy_gradient_loss | 0.00554       |
|    value_loss           | 0.00411       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0107       |
|    negative_advantag... | 0.08144599   |
|    positive_advantag... | 0.13761665   |
|    prob_ratio           | 939231.7     |
|    rollout_return       | -2.1014514   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 457          |
|    time_elapsed         | 70841        |
|    total_timesteps      | 116992       |
| train/                  |              |
|    active_example       | 254          |
|    approx_kl            | -0.006655693 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0218      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 91200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00403      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00877       |
|    negative_advantag... | 0.06280763    |
|    positive_advantag... | 0.1369592     |
|    prob_ratio           | 1110669.2     |
|    rollout_return       | -1.8047682    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00949       |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 458           |
|    time_elapsed         | 71003         |
|    total_timesteps      | 117248        |
| train/                  |               |
|    active_example       | 148           |
|    approx_kl            | -0.0013555884 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0155       |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 91400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00261       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0103         |
|    negative_advantag... | 0.088671215    |
|    positive_advantag... | 0.1364461      |
|    prob_ratio           | 1094933.6      |
|    rollout_return       | -2.1649148     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00947        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00978        |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 459            |
|    time_elapsed         | 71165          |
|    total_timesteps      | 117504         |
| train/                  |                |
|    active_example       | 124            |
|    approx_kl            | -1.1920929e-07 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0165        |
|    explained_variance   | 0.969          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 91600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00328        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0113     |
|    negative_advantag... | 0.08961762 |
|    positive_advantag... | 0.07647617 |
|    prob_ratio           | 50987.477  |
|    rollout_return       | -1.5534629 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.17       |
|    collect_rollout/Mean | 4.17       |
|    collect_rollout/Sum  | 4.17       |
|    train_action_adv/... | 0.00958    |
|    train_action_adv/Sum | 30.7       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.2       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00992    |
|    train_loss/Sum       | 31.7       |
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 460        |
|    time_elapsed         | 71327      |
|    total_timesteps      | 117760     |
| train/                  |            |
|    active_example       | 252        |
|    approx_kl            | 0.0736582  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0208    |
|    explained_variance   | 0.924      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 91800      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00437    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.011        |
|    negative_advantag... | 0.070037104  |
|    positive_advantag... | 0.09871302   |
|    prob_ratio           | 91215.66     |
|    rollout_return       | -2.107153    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00944      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 461          |
|    time_elapsed         | 71489        |
|    total_timesteps      | 118016       |
| train/                  |              |
|    active_example       | 151          |
|    approx_kl            | -0.006517455 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 92000        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00124      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00941        |
|    negative_advantag... | 0.06621912     |
|    positive_advantag... | 0.07712072     |
|    prob_ratio           | 1241484.1      |
|    rollout_return       | -1.8432807     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.23           |
|    collect_rollout/Mean | 4.24           |
|    collect_rollout/Sum  | 4.24           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 462            |
|    time_elapsed         | 71651          |
|    total_timesteps      | 118272         |
| train/                  |                |
|    active_example       | 68             |
|    approx_kl            | -5.9723854e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0231        |
|    explained_variance   | 0.709          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 92200          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00717        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00692        |
|    negative_advantag... | 0.056487463    |
|    positive_advantag... | 0.11704773     |
|    prob_ratio           | 15891.758      |
|    rollout_return       | -1.8953344     |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.23           |
|    collect_rollout/Mean | 4.24           |
|    collect_rollout/Sum  | 4.24           |
|    train_action_adv/... | 0.00948        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.2           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00987        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 463            |
|    time_elapsed         | 71814          |
|    total_timesteps      | 118528         |
| train/                  |                |
|    active_example       | 253            |
|    approx_kl            | -0.00049999356 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0167        |
|    explained_variance   | 0.891          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 92400          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00391        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0117        |
|    negative_advantag... | 0.08650498    |
|    positive_advantag... | 0.098553725   |
|    prob_ratio           | 817018.25     |
|    rollout_return       | -1.8697494    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0184        |
|    train_computeV/Sum   | 58.8          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 464           |
|    time_elapsed         | 71977         |
|    total_timesteps      | 118784        |
| train/                  |               |
|    active_example       | 145           |
|    approx_kl            | 0.00022900105 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 92600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0093        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0116     |
|    negative_advantag... | 0.09425903 |
|    positive_advantag... | 0.08468632 |
|    prob_ratio           | 505784.3   |
|    rollout_return       | -1.9339471 |
| Time/                   |            |
|    collect_computeV/... | 0.0126     |
|    collect_computeV/Sum | 3.21       |
|    collect_rollout/Mean | 4.22       |
|    collect_rollout/Sum  | 4.22       |
|    train_action_adv/... | 0.00954    |
|    train_action_adv/Sum | 30.5       |
|    train_computeV/Mean  | 0.0186     |
|    train_computeV/Sum   | 59.4       |
|    train_epoch/Mean     | 159        |
|    train_epoch/Sum      | 159        |
|    train_loss/Mean      | 0.00985    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | -159       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 465        |
|    time_elapsed         | 72140      |
|    total_timesteps      | 119040     |
| train/                  |            |
|    active_example       | 115        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0128    |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 92800      |
|    policy_gradient_loss | 1.49e-05   |
|    value_loss           | 0.0042     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0103         |
|    negative_advantag... | 0.095160514    |
|    positive_advantag... | 0.083609074    |
|    prob_ratio           | 652361.8       |
|    rollout_return       | -2.007945      |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0184         |
|    train_computeV/Sum   | 58.8           |
|    train_epoch/Mean     | 159            |
|    train_epoch/Sum      | 159            |
|    train_loss/Mean      | 0.00986        |
|    train_loss/Sum       | 31.6           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 466            |
|    time_elapsed         | 72303          |
|    total_timesteps      | 119296         |
| train/                  |                |
|    active_example       | 252            |
|    approx_kl            | -4.9471855e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0109        |
|    explained_variance   | 0.95           |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 93000          |
|    policy_gradient_loss | 0.00143        |
|    value_loss           | 0.00322        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00885      |
|    negative_advantag... | 0.07792877   |
|    positive_advantag... | 0.09347755   |
|    prob_ratio           | 36982.684    |
|    rollout_return       | -1.3761827   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0184       |
|    train_computeV/Sum   | 58.7         |
|    train_epoch/Mean     | 159          |
|    train_epoch/Sum      | 159          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 467          |
|    time_elapsed         | 72466        |
|    total_timesteps      | 119552       |
| train/                  |              |
|    active_example       | 153          |
|    approx_kl            | -0.042312294 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0163      |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 93200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00302      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00856      |
|    negative_advantag... | 0.07369226   |
|    positive_advantag... | 0.13037032   |
|    prob_ratio           | 521650.47    |
|    rollout_return       | -2.0446472   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 468          |
|    time_elapsed         | 72628        |
|    total_timesteps      | 119808       |
| train/                  |              |
|    active_example       | 119          |
|    approx_kl            | 0.0011316836 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 93400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00441      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=120000, episode_reward=-139.40 +/- 11.11
Episode length: 140.40 +/- 11.11
New best mean reward!
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00605       |
|    negative_advantag... | 0.047330536   |
|    positive_advantag... | 0.099993706   |
|    prob_ratio           | 810859.6      |
|    rollout_return       | -1.3269625    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 5.56          |
|    collect_rollout/Sum  | 5.56          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.7          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| eval/                   |               |
|    mean_ep_length       | 140           |
|    mean_reward          | -139          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 469           |
|    time_elapsed         | 72792         |
|    total_timesteps      | 120064        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | -0.0010253191 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0138       |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 93600         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00478       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00718        |
|    negative_advantag... | 0.05206815     |
|    positive_advantag... | 0.11648661     |
|    prob_ratio           | 453598.38      |
|    rollout_return       | -1.9221228     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.6           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 470            |
|    time_elapsed         | 72954          |
|    total_timesteps      | 120320         |
| train/                  |                |
|    active_example       | 140            |
|    approx_kl            | 0.000120311975 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00617       |
|    explained_variance   | 0.947          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 93800          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00265        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00738       |
|    negative_advantag... | 0.057522368   |
|    positive_advantag... | 0.11687222    |
|    prob_ratio           | 71759.16      |
|    rollout_return       | -1.7376893    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 471           |
|    time_elapsed         | 73117         |
|    total_timesteps      | 120576        |
| train/                  |               |
|    active_example       | 106           |
|    approx_kl            | 2.1338463e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00883      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 94000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00256       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0107        |
|    negative_advantag... | 0.08677371    |
|    positive_advantag... | 0.088538885   |
|    prob_ratio           | 763187.94     |
|    rollout_return       | -1.9280686    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 472           |
|    time_elapsed         | 73280         |
|    total_timesteps      | 120832        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | -0.0002156496 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0158       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 94200         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00148       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0103       |
|    negative_advantag... | 0.07620291   |
|    positive_advantag... | 0.11509317   |
|    prob_ratio           | 338309.88    |
|    rollout_return       | -1.9212695   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 473          |
|    time_elapsed         | 73441        |
|    total_timesteps      | 121088       |
| train/                  |              |
|    active_example       | 143          |
|    approx_kl            | 0.0044691265 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0294      |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 94400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00113      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00919       |
|    negative_advantag... | 0.07574101    |
|    positive_advantag... | 0.08898559    |
|    prob_ratio           | 1100378.2     |
|    rollout_return       | -1.8244543    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 474           |
|    time_elapsed         | 73603         |
|    total_timesteps      | 121344        |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | -0.0060818195 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 94600         |
|    policy_gradient_loss | 0.00273       |
|    value_loss           | 0.00211       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00653        |
|    negative_advantag... | 0.046670686    |
|    positive_advantag... | 0.092235796    |
|    prob_ratio           | 367628.7       |
|    rollout_return       | -2.049747      |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 475            |
|    time_elapsed         | 73765          |
|    total_timesteps      | 121600         |
| train/                  |                |
|    active_example       | 256            |
|    approx_kl            | -2.3186207e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.013         |
|    explained_variance   | 0.864          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 94800          |
|    policy_gradient_loss | 0.0027         |
|    value_loss           | 0.00447        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00878       |
|    negative_advantag... | 0.066091694   |
|    positive_advantag... | 0.094630994   |
|    prob_ratio           | 665452.7      |
|    rollout_return       | -1.6682057    |
| Time/                   |               |
|    collect_computeV/... | 0.0127        |
|    collect_computeV/Sum | 3.25          |
|    collect_rollout/Mean | 4.26          |
|    collect_rollout/Sum  | 4.26          |
|    train_action_adv/... | 0.00942       |
|    train_action_adv/Sum | 30.1          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.0098        |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 476           |
|    time_elapsed         | 73927         |
|    total_timesteps      | 121856        |
| train/                  |               |
|    active_example       | 141           |
|    approx_kl            | -2.115965e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 95000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00125       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0104       |
|    negative_advantag... | 0.09170252   |
|    positive_advantag... | 0.072027974  |
|    prob_ratio           | 782721.94    |
|    rollout_return       | -1.7433468   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 477          |
|    time_elapsed         | 74089        |
|    total_timesteps      | 122112       |
| train/                  |              |
|    active_example       | 115          |
|    approx_kl            | 9.983778e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00462     |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 95200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0018       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00766    |
|    negative_advantag... | 0.06775193 |
|    positive_advantag... | 0.07720938 |
|    prob_ratio           | 447643.44  |
|    rollout_return       | -1.7050056 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.00948    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.7       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00977    |
|    train_loss/Sum       | 31.3       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 478        |
|    time_elapsed         | 74251      |
|    total_timesteps      | 122368     |
| train/                  |            |
|    active_example       | 251        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0107    |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 95400      |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00485    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00616        |
|    negative_advantag... | 0.052613907    |
|    positive_advantag... | 0.12301114     |
|    prob_ratio           | 511333.97      |
|    rollout_return       | -1.840914      |
| Time/                   |                |
|    collect_computeV/... | 0.0126         |
|    collect_computeV/Sum | 3.21           |
|    collect_rollout/Mean | 4.23           |
|    collect_rollout/Sum  | 4.23           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.7           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00976        |
|    train_loss/Sum       | 31.2           |
| rollout/                |                |
|    ep_len_mean          | 163            |
|    ep_rew_mean          | -162           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 479            |
|    time_elapsed         | 74414          |
|    total_timesteps      | 122624         |
| train/                  |                |
|    active_example       | 109            |
|    approx_kl            | -3.7908554e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0111        |
|    explained_variance   | 0.979          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 95600          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00109        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00796      |
|    negative_advantag... | 0.0690979    |
|    positive_advantag... | 0.082835786  |
|    prob_ratio           | 319875.1     |
|    rollout_return       | -1.770233    |
| Time/                   |              |
|    collect_computeV/... | 0.0127       |
|    collect_computeV/Sum | 3.25         |
|    collect_rollout/Mean | 4.26         |
|    collect_rollout/Sum  | 4.26         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 480          |
|    time_elapsed         | 74576        |
|    total_timesteps      | 122880       |
| train/                  |              |
|    active_example       | 121          |
|    approx_kl            | 7.688999e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0061      |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 95800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00145      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0086        |
|    negative_advantag... | 0.057768356   |
|    positive_advantag... | 0.097641975   |
|    prob_ratio           | 1272239.6     |
|    rollout_return       | -2.0994523    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.0095        |
|    train_action_adv/Sum | 30.4          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 481           |
|    time_elapsed         | 74739         |
|    total_timesteps      | 123136        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | 1.1920929e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 96000         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00091       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00911     |
|    negative_advantag... | 0.08204947  |
|    positive_advantag... | 0.09293104  |
|    prob_ratio           | 492557.28   |
|    rollout_return       | -1.8856734  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.23        |
|    collect_rollout/Mean | 4.25        |
|    collect_rollout/Sum  | 4.25        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.7        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00977     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 482         |
|    time_elapsed         | 74902       |
|    total_timesteps      | 123392      |
| train/                  |             |
|    active_example       | 136         |
|    approx_kl            | 0.008441448 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.017      |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 96200       |
|    policy_gradient_loss | 0.000823    |
|    value_loss           | 0.0027      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00984      |
|    negative_advantag... | 0.07775221   |
|    positive_advantag... | 0.0763885    |
|    prob_ratio           | 621645.44    |
|    rollout_return       | -2.1654806   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00979      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 483          |
|    time_elapsed         | 75064        |
|    total_timesteps      | 123648       |
| train/                  |              |
|    active_example       | 115          |
|    approx_kl            | 0.0013750792 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0184      |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 96400        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00227      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0108     |
|    negative_advantag... | 0.08952135 |
|    positive_advantag... | 0.06725606 |
|    prob_ratio           | 636134.25  |
|    rollout_return       | -2.151018  |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.00944    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.5       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00979    |
|    train_loss/Sum       | 31.3       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 484        |
|    time_elapsed         | 75226      |
|    total_timesteps      | 123904     |
| train/                  |            |
|    active_example       | 255        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00884   |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 96600      |
|    policy_gradient_loss | 0.00143    |
|    value_loss           | 0.00721    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00797      |
|    negative_advantag... | 0.07325092   |
|    positive_advantag... | 0.1094835    |
|    prob_ratio           | 667276.4     |
|    rollout_return       | -1.9443649   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00943      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 485          |
|    time_elapsed         | 75388        |
|    total_timesteps      | 124160       |
| train/                  |              |
|    active_example       | 136          |
|    approx_kl            | 0.0005913973 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 96800        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00181      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00978        |
|    negative_advantag... | 0.07878979     |
|    positive_advantag... | 0.09302663     |
|    prob_ratio           | 761040.0       |
|    rollout_return       | -1.4870647     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.00947        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00984        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 163            |
|    ep_rew_mean          | -162           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 486            |
|    time_elapsed         | 75550          |
|    total_timesteps      | 124416         |
| train/                  |                |
|    active_example       | 117            |
|    approx_kl            | 1.32620335e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0191        |
|    explained_variance   | 0.944          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 97000          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00322        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00954       |
|    negative_advantag... | 0.07461566    |
|    positive_advantag... | 0.09182972    |
|    prob_ratio           | 338279.75     |
|    rollout_return       | -1.9352715    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 157           |
|    train_epoch/Sum      | 157           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 487           |
|    time_elapsed         | 75712         |
|    total_timesteps      | 124672        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | 0.00012660027 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0175       |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 97200         |
|    policy_gradient_loss | 0.000287      |
|    value_loss           | 0.00158       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00732        |
|    negative_advantag... | 0.057938665    |
|    positive_advantag... | 0.092783995    |
|    prob_ratio           | 11231.46       |
|    rollout_return       | -1.6533291     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00941        |
|    train_action_adv/Sum | 30.1           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.3           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00982        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 162            |
|    ep_rew_mean          | -161           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 488            |
|    time_elapsed         | 75873          |
|    total_timesteps      | 124928         |
| train/                  |                |
|    active_example       | 121            |
|    approx_kl            | -0.00021889806 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0219        |
|    explained_variance   | 0.981          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 97400          |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00102        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.006       |
|    negative_advantag... | 0.032230567 |
|    positive_advantag... | 0.13921493  |
|    prob_ratio           | 143557.36   |
|    rollout_return       | -2.160023   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00953     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 489         |
|    time_elapsed         | 76036       |
|    total_timesteps      | 125184      |
| train/                  |             |
|    active_example       | 120         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0119     |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 97600       |
|    policy_gradient_loss | 0.0012      |
|    value_loss           | 0.00156     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00808       |
|    negative_advantag... | 0.061732367   |
|    positive_advantag... | 0.109771535   |
|    prob_ratio           | 50402.72      |
|    rollout_return       | -1.6249559    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 490           |
|    time_elapsed         | 76197         |
|    total_timesteps      | 125440        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | -7.987022e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 97800         |
|    policy_gradient_loss | 0.000167      |
|    value_loss           | 0.00183       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00736     |
|    negative_advantag... | 0.059314154 |
|    positive_advantag... | 0.08127184  |
|    prob_ratio           | 13383.478   |
|    rollout_return       | -1.8420761  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.2         |
|    collect_rollout/Sum  | 4.2         |
|    train_action_adv/... | 0.00956     |
|    train_action_adv/Sum | 30.6        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00984     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 491         |
|    time_elapsed         | 76360       |
|    total_timesteps      | 125696      |
| train/                  |             |
|    active_example       | 99          |
|    approx_kl            | 0.007439509 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0217     |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 98000       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00158     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00762      |
|    negative_advantag... | 0.066035375  |
|    positive_advantag... | 0.10158184   |
|    prob_ratio           | 3267.0378    |
|    rollout_return       | -1.4831603   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.7         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00973      |
|    train_loss/Sum       | 31.1         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 492          |
|    time_elapsed         | 76522        |
|    total_timesteps      | 125952       |
| train/                  |              |
|    active_example       | 111          |
|    approx_kl            | -0.000153929 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0143      |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 98200        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00388      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00871     |
|    negative_advantag... | 0.065593235 |
|    positive_advantag... | 0.06135993  |
|    prob_ratio           | 407930.16   |
|    rollout_return       | -1.3592012  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00945     |
|    train_action_adv/Sum | 30.2        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | -161        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 493         |
|    time_elapsed         | 76684       |
|    total_timesteps      | 126208      |
| train/                  |             |
|    active_example       | 255         |
|    approx_kl            | 0.013687439 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0224     |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 98400       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00295     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0096       |
|    negative_advantag... | 0.07508562   |
|    positive_advantag... | 0.10382065   |
|    prob_ratio           | 463393.47    |
|    rollout_return       | -2.042447    |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00989      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 494          |
|    time_elapsed         | 76846        |
|    total_timesteps      | 126464       |
| train/                  |              |
|    active_example       | 149          |
|    approx_kl            | -0.010580778 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0216      |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 98600        |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00212      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00923     |
|    negative_advantag... | 0.07836513  |
|    positive_advantag... | 0.06907628  |
|    prob_ratio           | 556564.8    |
|    rollout_return       | -1.7599862  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00981     |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 495         |
|    time_elapsed         | 77008       |
|    total_timesteps      | 126720      |
| train/                  |             |
|    active_example       | 132         |
|    approx_kl            | 0.038018823 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0119     |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 98800       |
|    policy_gradient_loss | 2.77e-05    |
|    value_loss           | 0.00143     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00985       |
|    negative_advantag... | 0.08024208    |
|    positive_advantag... | 0.100402825   |
|    prob_ratio           | 490214.16     |
|    rollout_return       | -1.9677643    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0184        |
|    train_computeV/Sum   | 58.9          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00979       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 496           |
|    time_elapsed         | 77171         |
|    total_timesteps      | 126976        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | 7.1525574e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 99000         |
|    policy_gradient_loss | 0.001         |
|    value_loss           | 0.00247       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00851      |
|    negative_advantag... | 0.06649494   |
|    positive_advantag... | 0.112411365  |
|    prob_ratio           | 952880.5     |
|    rollout_return       | -1.8941818   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00943      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00988      |
|    train_loss/Sum       | 31.6         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 497          |
|    time_elapsed         | 77333        |
|    total_timesteps      | 127232       |
| train/                  |              |
|    active_example       | 150          |
|    approx_kl            | 4.261732e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0138      |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 99200        |
|    policy_gradient_loss | 0.000593     |
|    value_loss           | 0.00163      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.012         |
|    negative_advantag... | 0.08979883    |
|    positive_advantag... | 0.110396534   |
|    prob_ratio           | 338094.44     |
|    rollout_return       | -1.9254788    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00977       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 498           |
|    time_elapsed         | 77494         |
|    total_timesteps      | 127488        |
| train/                  |               |
|    active_example       | 135           |
|    approx_kl            | -0.0037475228 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0226       |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 99400         |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00137       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00707       |
|    negative_advantag... | 0.061182972   |
|    positive_advantag... | 0.08577027    |
|    prob_ratio           | 446147.8      |
|    rollout_return       | -1.3768039    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 165           |
|    ep_rew_mean          | -164          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 499           |
|    time_elapsed         | 77656         |
|    total_timesteps      | 127744        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | 2.9802322e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00844      |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 99600         |
|    policy_gradient_loss | 0.000738      |
|    value_loss           | 0.00344       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0102      |
|    negative_advantag... | 0.076513916 |
|    positive_advantag... | 0.10288065  |
|    prob_ratio           | 318670.4    |
|    rollout_return       | -1.7242845  |
| Time/                   |             |
|    collect_computeV/... | 0.0126      |
|    collect_computeV/Sum | 3.22        |
|    collect_rollout/Mean | 4.23        |
|    collect_rollout/Sum  | 4.23        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00986     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 500         |
|    time_elapsed         | 77819       |
|    total_timesteps      | 128000      |
| train/                  |             |
|    active_example       | 144         |
|    approx_kl            | -0.00953491 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0189     |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 99800       |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00211     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00996      |
|    negative_advantag... | 0.073162526  |
|    positive_advantag... | 0.10771665   |
|    prob_ratio           | 925429.94    |
|    rollout_return       | -1.9425725   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 501          |
|    time_elapsed         | 77982        |
|    total_timesteps      | 128256       |
| train/                  |              |
|    active_example       | 133          |
|    approx_kl            | 0.0008646846 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0319      |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 100000       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00126      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00749     |
|    negative_advantag... | 0.049184266 |
|    positive_advantag... | 0.09870651  |
|    prob_ratio           | 1113518.1   |
|    rollout_return       | -1.843405   |
| Time/                   |             |
|    collect_computeV/... | 0.0123      |
|    collect_computeV/Sum | 3.16        |
|    collect_rollout/Mean | 4.16        |
|    collect_rollout/Sum  | 4.16        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.3        |
|    train_epoch/Mean     | 157         |
|    train_epoch/Sum      | 157         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 502         |
|    time_elapsed         | 78143       |
|    total_timesteps      | 128512      |
| train/                  |             |
|    active_example       | 255         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.019      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 100200      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00248     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00598        |
|    negative_advantag... | 0.04685739     |
|    positive_advantag... | 0.10857241     |
|    prob_ratio           | 738790.06      |
|    rollout_return       | -1.927719      |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0181         |
|    train_computeV/Sum   | 58.1           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00985        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 166            |
|    ep_rew_mean          | -165           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 503            |
|    time_elapsed         | 78305          |
|    total_timesteps      | 128768         |
| train/                  |                |
|    active_example       | 120            |
|    approx_kl            | -4.7683716e-07 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00904       |
|    explained_variance   | 0.976          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 100400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00169        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0105      |
|    negative_advantag... | 0.0757272   |
|    positive_advantag... | 0.098686926 |
|    prob_ratio           | 1154063.0   |
|    rollout_return       | -1.9570853  |
| Time/                   |             |
|    collect_computeV/... | 0.0127      |
|    collect_computeV/Sum | 3.24        |
|    collect_rollout/Mean | 4.26        |
|    collect_rollout/Sum  | 4.26        |
|    train_action_adv/... | 0.00941     |
|    train_action_adv/Sum | 30.1        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.7        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00976     |
|    train_loss/Sum       | 31.2        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 504         |
|    time_elapsed         | 78467       |
|    total_timesteps      | 129024      |
| train/                  |             |
|    active_example       | 122         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0149     |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 100600      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00157     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0105     |
|    negative_advantag... | 0.0943105  |
|    positive_advantag... | 0.09346301 |
|    prob_ratio           | 363552.16  |
|    rollout_return       | -2.2069623 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.2        |
|    collect_rollout/Sum  | 4.2        |
|    train_action_adv/... | 0.00945    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0182     |
|    train_computeV/Sum   | 58.4       |
|    train_epoch/Mean     | 157        |
|    train_epoch/Sum      | 157        |
|    train_loss/Mean      | 0.00985    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 165        |
|    ep_rew_mean          | -164       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 505        |
|    time_elapsed         | 78628      |
|    total_timesteps      | 129280     |
| train/                  |            |
|    active_example       | 255        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0145    |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 100800     |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00614    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00877     |
|    negative_advantag... | 0.06784546  |
|    positive_advantag... | 0.10486951  |
|    prob_ratio           | 1148463.6   |
|    rollout_return       | -2.0724773  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00946     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00979     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 506         |
|    time_elapsed         | 78790       |
|    total_timesteps      | 129536      |
| train/                  |             |
|    active_example       | 134         |
|    approx_kl            | 0.007886559 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0188     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 101000      |
|    policy_gradient_loss | 0.0011      |
|    value_loss           | 0.00135     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00919      |
|    negative_advantag... | 0.057704914  |
|    positive_advantag... | 0.10176789   |
|    prob_ratio           | 817335.8     |
|    rollout_return       | -2.064828    |
| Time/                   |              |
|    collect_computeV/... | 0.0123       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.17         |
|    collect_rollout/Sum  | 4.17         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -164         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 507          |
|    time_elapsed         | 78952        |
|    total_timesteps      | 129792       |
| train/                  |              |
|    active_example       | 119          |
|    approx_kl            | -0.007583916 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0197      |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 101200       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00143      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=130000, episode_reward=-133.60 +/- 11.69
Episode length: 134.60 +/- 11.69
New best mean reward!
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00689      |
|    negative_advantag... | 0.06136285   |
|    positive_advantag... | 0.101078615  |
|    prob_ratio           | 1036922.9    |
|    rollout_return       | -1.6390696   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 5.43         |
|    collect_rollout/Sum  | 5.43         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0184       |
|    train_computeV/Sum   | 58.7         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00977      |
|    train_loss/Sum       | 31.3         |
| eval/                   |              |
|    mean_ep_length       | 135          |
|    mean_reward          | -134         |
| rollout/                |              |
|    ep_len_mean          | 166          |
|    ep_rew_mean          | -165         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 508          |
|    time_elapsed         | 79116        |
|    total_timesteps      | 130048       |
| train/                  |              |
|    active_example       | 251          |
|    approx_kl            | 0.0064929724 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00758     |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 101400       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00351      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0102        |
|    negative_advantag... | 0.0792711     |
|    positive_advantag... | 0.113580555   |
|    prob_ratio           | 913456.25     |
|    rollout_return       | -1.7212089    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 166           |
|    ep_rew_mean          | -165          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 509           |
|    time_elapsed         | 79278         |
|    total_timesteps      | 130304        |
| train/                  |               |
|    active_example       | 144           |
|    approx_kl            | -0.0056013167 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0292       |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 101600        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00293       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00314     |
|    negative_advantag... | 0.021999136 |
|    positive_advantag... | 0.036770422 |
|    prob_ratio           | 389233.3    |
|    rollout_return       | -1.5049047  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00978     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | -167        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 510         |
|    time_elapsed         | 79441       |
|    total_timesteps      | 130560      |
| train/                  |             |
|    active_example       | 43          |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00154    |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 101800      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00321     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00916       |
|    negative_advantag... | 0.07014172    |
|    positive_advantag... | 0.09778807    |
|    prob_ratio           | 841193.06     |
|    rollout_return       | -1.8161792    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 167           |
|    ep_rew_mean          | -166          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 511           |
|    time_elapsed         | 79603         |
|    total_timesteps      | 130816        |
| train/                  |               |
|    active_example       | 252           |
|    approx_kl            | 0.00031736493 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 102000        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00323       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00897      |
|    negative_advantag... | 0.060313128  |
|    positive_advantag... | 0.0840033    |
|    prob_ratio           | 243908.45    |
|    rollout_return       | -1.9619626   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00943      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 512          |
|    time_elapsed         | 79765        |
|    total_timesteps      | 131072       |
| train/                  |              |
|    active_example       | 138          |
|    approx_kl            | 7.867813e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 102200       |
|    policy_gradient_loss | 0.000123     |
|    value_loss           | 0.00253      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00781       |
|    negative_advantag... | 0.059357896   |
|    positive_advantag... | 0.10898222    |
|    prob_ratio           | 563803.2      |
|    rollout_return       | -2.246999     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00955       |
|    train_action_adv/Sum | 30.6          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.5          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 513           |
|    time_elapsed         | 79927         |
|    total_timesteps      | 131328        |
| train/                  |               |
|    active_example       | 122           |
|    approx_kl            | -0.0028392076 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 102400        |
|    policy_gradient_loss | 0.0065        |
|    value_loss           | 0.00262       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00918        |
|    negative_advantag... | 0.07508162     |
|    positive_advantag... | 0.05105121     |
|    prob_ratio           | 63154.336      |
|    rollout_return       | -1.8619735     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00946        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0182         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00981        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 162            |
|    ep_rew_mean          | -161           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 514            |
|    time_elapsed         | 80089          |
|    total_timesteps      | 131584         |
| train/                  |                |
|    active_example       | 254            |
|    approx_kl            | -2.6226044e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0155        |
|    explained_variance   | 0.887          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 102600         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00467        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0082        |
|    negative_advantag... | 0.05758265    |
|    positive_advantag... | 0.11173391    |
|    prob_ratio           | 796091.94     |
|    rollout_return       | -1.9350407    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.7          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 515           |
|    time_elapsed         | 80252         |
|    total_timesteps      | 131840        |
| train/                  |               |
|    active_example       | 146           |
|    approx_kl            | -0.0028738603 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0161       |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 102800        |
|    policy_gradient_loss | 0.00154       |
|    value_loss           | 0.00425       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00682    |
|    negative_advantag... | 0.04089422 |
|    positive_advantag... | 0.07533626 |
|    prob_ratio           | 583060.5   |
|    rollout_return       | -1.5833217 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.00943    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.6       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00986    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 516        |
|    time_elapsed         | 80414      |
|    total_timesteps      | 132096     |
| train/                  |            |
|    active_example       | 148        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00788   |
|    explained_variance   | 0.928      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 103000     |
|    policy_gradient_loss | 0.000227   |
|    value_loss           | 0.00322    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00937      |
|    negative_advantag... | 0.0651338    |
|    positive_advantag... | 0.08986627   |
|    prob_ratio           | 750885.75    |
|    rollout_return       | -1.9766247   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 517          |
|    time_elapsed         | 80577        |
|    total_timesteps      | 132352       |
| train/                  |              |
|    active_example       | 254          |
|    approx_kl            | -0.007367015 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0137      |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 103200       |
|    policy_gradient_loss | 0.000925     |
|    value_loss           | 0.00915      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00592        |
|    negative_advantag... | 0.036268994    |
|    positive_advantag... | 0.12328185     |
|    prob_ratio           | 1256275.5      |
|    rollout_return       | -1.6915756     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.17           |
|    collect_rollout/Mean | 4.17           |
|    collect_rollout/Sum  | 4.17           |
|    train_action_adv/... | 0.00946        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 163            |
|    ep_rew_mean          | -162           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 518            |
|    time_elapsed         | 80739          |
|    total_timesteps      | 132608         |
| train/                  |                |
|    active_example       | 141            |
|    approx_kl            | -0.00018125772 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00963       |
|    explained_variance   | 0.963          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 103400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00361        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00948       |
|    negative_advantag... | 0.059720144   |
|    positive_advantag... | 0.13754553    |
|    prob_ratio           | 501556.2      |
|    rollout_return       | -1.9797676    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 519           |
|    time_elapsed         | 80901         |
|    total_timesteps      | 132864        |
| train/                  |               |
|    active_example       | 127           |
|    approx_kl            | 0.00010973215 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0166       |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 103600        |
|    policy_gradient_loss | 0.001         |
|    value_loss           | 0.00157       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00711       |
|    negative_advantag... | 0.058423337   |
|    positive_advantag... | 0.09935014    |
|    prob_ratio           | 170767.77     |
|    rollout_return       | -1.6901963    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 520           |
|    time_elapsed         | 81064         |
|    total_timesteps      | 133120        |
| train/                  |               |
|    active_example       | 253           |
|    approx_kl            | -1.847744e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 103800        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00265       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00689       |
|    negative_advantag... | 0.0533217     |
|    positive_advantag... | 0.088690184   |
|    prob_ratio           | 443090.47     |
|    rollout_return       | -1.7302665    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00977       |
|    train_loss/Sum       | 31.3          |
| rollout/                |               |
|    ep_len_mean          | 164           |
|    ep_rew_mean          | -163          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 521           |
|    time_elapsed         | 81225         |
|    total_timesteps      | 133376        |
| train/                  |               |
|    active_example       | 129           |
|    approx_kl            | -6.198883e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00982      |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 104000        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00269       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00894      |
|    negative_advantag... | 0.07298232   |
|    positive_advantag... | 0.115377165  |
|    prob_ratio           | 631328.06    |
|    rollout_return       | -1.96658     |
| Time/                   |              |
|    collect_computeV/... | 0.0126       |
|    collect_computeV/Sum | 3.22         |
|    collect_rollout/Mean | 4.22         |
|    collect_rollout/Sum  | 4.22         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00981      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 522          |
|    time_elapsed         | 81388        |
|    total_timesteps      | 133632       |
| train/                  |              |
|    active_example       | 130          |
|    approx_kl            | -0.018789642 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0263      |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 104200       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0026       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.011         |
|    negative_advantag... | 0.097032815   |
|    positive_advantag... | 0.07822111    |
|    prob_ratio           | 132041.02     |
|    rollout_return       | -2.132962     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 523           |
|    time_elapsed         | 81550         |
|    total_timesteps      | 133888        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | -0.0004172325 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 104400        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00244       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0108      |
|    negative_advantag... | 0.079331614 |
|    positive_advantag... | 0.08359847  |
|    prob_ratio           | 381481.88   |
|    rollout_return       | -2.134567   |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.18        |
|    collect_rollout/Mean | 4.18        |
|    collect_rollout/Sum  | 4.18        |
|    train_action_adv/... | 0.00952     |
|    train_action_adv/Sum | 30.5        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.5        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00978     |
|    train_loss/Sum       | 31.3        |
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | -160        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 524         |
|    time_elapsed         | 81712       |
|    total_timesteps      | 134144      |
| train/                  |             |
|    active_example       | 134         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0119     |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 104600      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.0041      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00782       |
|    negative_advantag... | 0.058454245   |
|    positive_advantag... | 0.11084268    |
|    prob_ratio           | 339408.72     |
|    rollout_return       | -2.029362     |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.21          |
|    collect_rollout/Sum  | 4.21          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00985       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 525           |
|    time_elapsed         | 81874         |
|    total_timesteps      | 134400        |
| train/                  |               |
|    active_example       | 130           |
|    approx_kl            | 0.00019198656 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 104800        |
|    policy_gradient_loss | 0.0002        |
|    value_loss           | 0.00171       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00835    |
|    negative_advantag... | 0.0735721  |
|    positive_advantag... | 0.08240448 |
|    prob_ratio           | 491688.38  |
|    rollout_return       | -1.4367054 |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.21       |
|    collect_rollout/Mean | 4.21       |
|    collect_rollout/Sum  | 4.21       |
|    train_action_adv/... | 0.00945    |
|    train_action_adv/Sum | 30.2       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.6       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.00975    |
|    train_loss/Sum       | 31.2       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 526        |
|    time_elapsed         | 82036      |
|    total_timesteps      | 134656     |
| train/                  |            |
|    active_example       | 253        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0162    |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 105000     |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00401    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00819       |
|    negative_advantag... | 0.06696218    |
|    positive_advantag... | 0.08343039    |
|    prob_ratio           | 388840.12     |
|    rollout_return       | -1.9478734    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.22          |
|    collect_rollout/Sum  | 4.22          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0184        |
|    train_computeV/Sum   | 58.8          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00976       |
|    train_loss/Sum       | 31.2          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 527           |
|    time_elapsed         | 82199         |
|    total_timesteps      | 134912        |
| train/                  |               |
|    active_example       | 139           |
|    approx_kl            | 2.6196241e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0152       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 105200        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00122       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00652      |
|    negative_advantag... | 0.050379287  |
|    positive_advantag... | 0.096046574  |
|    prob_ratio           | 658961.4     |
|    rollout_return       | -1.8766434   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.7         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00977      |
|    train_loss/Sum       | 31.3         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 528          |
|    time_elapsed         | 82361        |
|    total_timesteps      | 135168       |
| train/                  |              |
|    active_example       | 119          |
|    approx_kl            | 7.599592e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0194      |
|    explained_variance   | 0.94         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 105400       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00624      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0055       |
|    negative_advantag... | 0.042204157  |
|    positive_advantag... | 0.11394823   |
|    prob_ratio           | 157625.38    |
|    rollout_return       | -2.0235052   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.2         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00991      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 529          |
|    time_elapsed         | 82523        |
|    total_timesteps      | 135424       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | -8.34465e-07 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 105600       |
|    policy_gradient_loss | 0.000786     |
|    value_loss           | 0.003        |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00537     |
|    negative_advantag... | 0.04095611  |
|    positive_advantag... | 0.05664156  |
|    prob_ratio           | 4492.3496   |
|    rollout_return       | -1.5806586  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.19        |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.00947     |
|    train_action_adv/Sum | 30.3        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.2        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00983     |
|    train_loss/Sum       | 31.5        |
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | -162        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 530         |
|    time_elapsed         | 82685       |
|    total_timesteps      | 135680      |
| train/                  |             |
|    active_example       | 113         |
|    approx_kl            | 0.014421999 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0098     |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 105800      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00477     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0133       |
|    negative_advantag... | 0.11560092   |
|    positive_advantag... | 0.07858905   |
|    prob_ratio           | 379138.0     |
|    rollout_return       | -2.209538    |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.2          |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00985      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | -163         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 531          |
|    time_elapsed         | 82847        |
|    total_timesteps      | 135936       |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | -0.055072755 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0195      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 106000       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.000921     |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00813       |
|    negative_advantag... | 0.07299337    |
|    positive_advantag... | 0.0895458     |
|    prob_ratio           | 118411.94     |
|    rollout_return       | -1.8495712    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 532           |
|    time_elapsed         | 83009         |
|    total_timesteps      | 136192        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | -7.587671e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00967      |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 106200        |
|    policy_gradient_loss | 0.000326      |
|    value_loss           | 0.00286       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00917      |
|    negative_advantag... | 0.06104996   |
|    positive_advantag... | 0.12361805   |
|    prob_ratio           | 47083.09     |
|    rollout_return       | -2.2299633   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00982      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 533          |
|    time_elapsed         | 83171        |
|    total_timesteps      | 136448       |
| train/                  |              |
|    active_example       | 140          |
|    approx_kl            | 7.390976e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0163      |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 106400       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.000806     |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00715      |
|    negative_advantag... | 0.056861673  |
|    positive_advantag... | 0.1237048    |
|    prob_ratio           | 814277.25    |
|    rollout_return       | -2.207913    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00946      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.1         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00991      |
|    train_loss/Sum       | 31.7         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 534          |
|    time_elapsed         | 83333        |
|    total_timesteps      | 136704       |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | -0.002580896 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 106600       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00584      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0104       |
|    negative_advantag... | 0.060076974  |
|    positive_advantag... | 0.09859494   |
|    prob_ratio           | 1268152.5    |
|    rollout_return       | -2.0714152   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00954      |
|    train_action_adv/Sum | 30.5         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00981      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 535          |
|    time_elapsed         | 83496        |
|    total_timesteps      | 136960       |
| train/                  |              |
|    active_example       | 255          |
|    approx_kl            | -0.007768303 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0201      |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 106800       |
|    policy_gradient_loss | 0.000266     |
|    value_loss           | 0.00236      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0061      |
|    negative_advantag... | 0.040292352 |
|    positive_advantag... | 0.13044992  |
|    prob_ratio           | 300919.66   |
|    rollout_return       | -2.0528572  |
| Time/                   |             |
|    collect_computeV/... | 0.0124      |
|    collect_computeV/Sum | 3.16        |
|    collect_rollout/Mean | 4.17        |
|    collect_rollout/Sum  | 4.17        |
|    train_action_adv/... | 0.00949     |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0182      |
|    train_computeV/Sum   | 58.4        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.00987     |
|    train_loss/Sum       | 31.6        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 536         |
|    time_elapsed         | 83658       |
|    total_timesteps      | 137216      |
| train/                  |             |
|    active_example       | 135         |
|    approx_kl            | 0.010950744 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0114     |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 107000      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00204     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00711      |
|    negative_advantag... | 0.05368502   |
|    positive_advantag... | 0.119655006  |
|    prob_ratio           | 1024600.4    |
|    rollout_return       | -2.0502765   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.21         |
|    collect_rollout/Mean | 4.21         |
|    collect_rollout/Sum  | 4.21         |
|    train_action_adv/... | 0.00949      |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 537          |
|    time_elapsed         | 83820        |
|    total_timesteps      | 137472       |
| train/                  |              |
|    active_example       | 128          |
|    approx_kl            | -1.66893e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0216      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 107200       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00283      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00741    |
|    negative_advantag... | 0.05813873 |
|    positive_advantag... | 0.08395124 |
|    prob_ratio           | 731164.9   |
|    rollout_return       | -1.465794  |
| Time/                   |            |
|    collect_computeV/... | 0.0125     |
|    collect_computeV/Sum | 3.19       |
|    collect_rollout/Mean | 4.19       |
|    collect_rollout/Sum  | 4.19       |
|    train_action_adv/... | 0.00949    |
|    train_action_adv/Sum | 30.4       |
|    train_computeV/Mean  | 0.0184     |
|    train_computeV/Sum   | 58.7       |
|    train_epoch/Mean     | 159        |
|    train_epoch/Sum      | 159        |
|    train_loss/Mean      | 0.00985    |
|    train_loss/Sum       | 31.5       |
| rollout/                |            |
|    ep_len_mean          | 161        |
|    ep_rew_mean          | -160       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 538        |
|    time_elapsed         | 83983      |
|    total_timesteps      | 137728     |
| train/                  |            |
|    active_example       | 255        |
|    approx_kl            | 0.05912286 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00981   |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 107400     |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00317    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00839       |
|    negative_advantag... | 0.062221203   |
|    positive_advantag... | 0.10047428    |
|    prob_ratio           | 117291.79     |
|    rollout_return       | -1.9530983    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.18          |
|    collect_rollout/Sum  | 4.18          |
|    train_action_adv/... | 0.00948       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00982       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 539           |
|    time_elapsed         | 84145         |
|    total_timesteps      | 137984        |
| train/                  |               |
|    active_example       | 138           |
|    approx_kl            | -2.798438e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 107600        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00123       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00843      |
|    negative_advantag... | 0.06804242   |
|    positive_advantag... | 0.10191865   |
|    prob_ratio           | 1311244.1    |
|    rollout_return       | -1.8337041   |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.2          |
|    collect_rollout/Sum  | 4.2          |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.5         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 540          |
|    time_elapsed         | 84307        |
|    total_timesteps      | 138240       |
| train/                  |              |
|    active_example       | 127          |
|    approx_kl            | 5.364418e-07 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 107800       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.0025       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.01           |
|    negative_advantag... | 0.06974335     |
|    positive_advantag... | 0.07890903     |
|    prob_ratio           | 5629.9956      |
|    rollout_return       | -1.7144046     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.19           |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 541            |
|    time_elapsed         | 84468          |
|    total_timesteps      | 138496         |
| train/                  |                |
|    active_example       | 254            |
|    approx_kl            | -1.3113022e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0169        |
|    explained_variance   | 0.93           |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 108000         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00224        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0102        |
|    negative_advantag... | 0.07977791    |
|    positive_advantag... | 0.079675294   |
|    prob_ratio           | 361211.47     |
|    rollout_return       | -1.7741863    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.1          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00988       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 542           |
|    time_elapsed         | 84630         |
|    total_timesteps      | 138752        |
| train/                  |               |
|    active_example       | 163           |
|    approx_kl            | -0.0051219016 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 108200        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00255       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00829        |
|    negative_advantag... | 0.06939499     |
|    positive_advantag... | 0.09839803     |
|    prob_ratio           | 38087.79       |
|    rollout_return       | -1.6611122     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00951        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 543            |
|    time_elapsed         | 84792          |
|    total_timesteps      | 139008         |
| train/                  |                |
|    active_example       | 137            |
|    approx_kl            | -1.6093254e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.017         |
|    explained_variance   | 0.937          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 108400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00173        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0117       |
|    negative_advantag... | 0.09195305   |
|    positive_advantag... | 0.08880871   |
|    prob_ratio           | 768216.94    |
|    rollout_return       | -2.1045518   |
| Time/                   |              |
|    collect_computeV/... | 0.0127       |
|    collect_computeV/Sum | 3.24         |
|    collect_rollout/Mean | 4.26         |
|    collect_rollout/Sum  | 4.26         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.0098       |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 544          |
|    time_elapsed         | 84954        |
|    total_timesteps      | 139264       |
| train/                  |              |
|    active_example       | 253          |
|    approx_kl            | -0.008346617 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 108600       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00496      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00744     |
|    negative_advantag... | 0.056872234 |
|    positive_advantag... | 0.10394816  |
|    prob_ratio           | 873092.4    |
|    rollout_return       | -1.6991377  |
| Time/                   |             |
|    collect_computeV/... | 0.0125      |
|    collect_computeV/Sum | 3.2         |
|    collect_rollout/Mean | 4.19        |
|    collect_rollout/Sum  | 4.19        |
|    train_action_adv/... | 0.0095      |
|    train_action_adv/Sum | 30.4        |
|    train_computeV/Mean  | 0.0183      |
|    train_computeV/Sum   | 58.6        |
|    train_epoch/Mean     | 158         |
|    train_epoch/Sum      | 158         |
|    train_loss/Mean      | 0.0098      |
|    train_loss/Sum       | 31.4        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 545         |
|    time_elapsed         | 85117       |
|    total_timesteps      | 139520      |
| train/                  |             |
|    active_example       | 139         |
|    approx_kl            | 0.015101127 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0141     |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 108800      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00144     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00842      |
|    negative_advantag... | 0.052861106  |
|    positive_advantag... | 0.10694399   |
|    prob_ratio           | 1017576.2    |
|    rollout_return       | -2.00253     |
| Time/                   |              |
|    collect_computeV/... | 0.0125       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00945      |
|    train_action_adv/Sum | 30.2         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 546          |
|    time_elapsed         | 85279        |
|    total_timesteps      | 139776       |
| train/                  |              |
|    active_example       | 132          |
|    approx_kl            | -7.44462e-05 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0167      |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 109000       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.002        |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=140000, episode_reward=-175.00 +/- 17.67
Episode length: 176.00 +/- 17.67
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00864      |
|    negative_advantag... | 0.06516863   |
|    positive_advantag... | 0.12104344   |
|    prob_ratio           | 1565271.6    |
|    rollout_return       | -2.0022478   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.17         |
|    collect_rollout/Mean | 5.83         |
|    collect_rollout/Sum  | 5.83         |
|    train_action_adv/... | 0.0095       |
|    train_action_adv/Sum | 30.4         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.6         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.5         |
| eval/                   |              |
|    mean_ep_length       | 176          |
|    mean_reward          | -175         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 547          |
|    time_elapsed         | 85443        |
|    total_timesteps      | 140032       |
| train/                  |              |
|    active_example       | 256          |
|    approx_kl            | 5.543232e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0101      |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.01         |
|    loss                 | 0.0998       |
|    n_updates            | 109200       |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 0.00122      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00701        |
|    negative_advantag... | 0.05851297     |
|    positive_advantag... | 0.09896759     |
|    prob_ratio           | 498590.53      |
|    rollout_return       | -1.5795393     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.2            |
|    collect_rollout/Sum  | 4.2            |
|    train_action_adv/... | 0.00949        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.5           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.0098         |
|    train_loss/Sum       | 31.3           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 548            |
|    time_elapsed         | 85605          |
|    total_timesteps      | 140288         |
| train/                  |                |
|    active_example       | 138            |
|    approx_kl            | -3.0398369e-06 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00963       |
|    explained_variance   | 0.973          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 109400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00242        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00591       |
|    negative_advantag... | 0.048940767   |
|    positive_advantag... | 0.03318826    |
|    prob_ratio           | 19956.781     |
|    rollout_return       | -1.3703253    |
| Time/                   |               |
|    collect_computeV/... | 0.0126        |
|    collect_computeV/Sum | 3.22          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.6          |
|    train_epoch/Mean     | 159           |
|    train_epoch/Sum      | 159           |
|    train_loss/Mean      | 0.00984       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 549           |
|    time_elapsed         | 85768         |
|    total_timesteps      | 140544        |
| train/                  |               |
|    active_example       | 185           |
|    approx_kl            | -0.0028415322 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00483      |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 109600        |
|    policy_gradient_loss | 0.00125       |
|    value_loss           | 0.00498       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00951       |
|    negative_advantag... | 0.08265789    |
|    positive_advantag... | 0.073768094   |
|    prob_ratio           | 153706.4      |
|    rollout_return       | -2.118268     |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.17          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00952       |
|    train_action_adv/Sum | 30.5          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.4          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 550           |
|    time_elapsed         | 85930         |
|    total_timesteps      | 140800        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | -6.467104e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 109800        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.0017        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00848        |
|    negative_advantag... | 0.06168965     |
|    positive_advantag... | 0.097138904    |
|    prob_ratio           | 144016.83      |
|    rollout_return       | -2.0105674     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.19           |
|    collect_rollout/Sum  | 4.19           |
|    train_action_adv/... | 0.00946        |
|    train_action_adv/Sum | 30.3           |
|    train_computeV/Mean  | 0.0183         |
|    train_computeV/Sum   | 58.4           |
|    train_epoch/Mean     | 158            |
|    train_epoch/Sum      | 158            |
|    train_loss/Mean      | 0.00981        |
|    train_loss/Sum       | 31.4           |
| rollout/                |                |
|    ep_len_mean          | 162            |
|    ep_rew_mean          | -161           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 551            |
|    time_elapsed         | 86092          |
|    total_timesteps      | 141056         |
| train/                  |                |
|    active_example       | 141            |
|    approx_kl            | -0.00085765123 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0149        |
|    explained_variance   | 0.945          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 110000         |
|    policy_gradient_loss | 0.00401        |
|    value_loss           | 0.00343        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.011        |
|    negative_advantag... | 0.09161039   |
|    positive_advantag... | 0.08083129   |
|    prob_ratio           | 750280.2     |
|    rollout_return       | -1.886707    |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.18         |
|    collect_rollout/Mean | 4.18         |
|    collect_rollout/Sum  | 4.18         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00984      |
|    train_loss/Sum       | 31.5         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 552          |
|    time_elapsed         | 86255        |
|    total_timesteps      | 141312       |
| train/                  |              |
|    active_example       | 106          |
|    approx_kl            | 0.0030148625 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0279      |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 110200       |
|    policy_gradient_loss | 0.0012       |
|    value_loss           | 0.00325      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00615       |
|    negative_advantag... | 0.045706548   |
|    positive_advantag... | 0.108356      |
|    prob_ratio           | 844605.44     |
|    rollout_return       | -2.0510573    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.21          |
|    collect_rollout/Mean | 4.23          |
|    collect_rollout/Sum  | 4.23          |
|    train_action_adv/... | 0.0094        |
|    train_action_adv/Sum | 30.1          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.7          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00981       |
|    train_loss/Sum       | 31.4          |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 553           |
|    time_elapsed         | 86417         |
|    total_timesteps      | 141568        |
| train/                  |               |
|    active_example       | 252           |
|    approx_kl            | -3.695488e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00831      |
|    explained_variance   | 0.878         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 110400        |
|    policy_gradient_loss | 0.0011        |
|    value_loss           | 0.00652       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.011      |
|    negative_advantag... | 0.0790678  |
|    positive_advantag... | 0.09677213 |
|    prob_ratio           | 1221035.8  |
|    rollout_return       | -2.0247056 |
| Time/                   |            |
|    collect_computeV/... | 0.0124     |
|    collect_computeV/Sum | 3.18       |
|    collect_rollout/Mean | 4.18       |
|    collect_rollout/Sum  | 4.18       |
|    train_action_adv/... | 0.00947    |
|    train_action_adv/Sum | 30.3       |
|    train_computeV/Mean  | 0.0183     |
|    train_computeV/Sum   | 58.5       |
|    train_epoch/Mean     | 158        |
|    train_epoch/Sum      | 158        |
|    train_loss/Mean      | 0.0098     |
|    train_loss/Sum       | 31.3       |
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | -162       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 554        |
|    time_elapsed         | 86579      |
|    total_timesteps      | 141824     |
| train/                  |            |
|    active_example       | 139        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0227    |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 110600     |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00281    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00853      |
|    negative_advantag... | 0.06568985   |
|    positive_advantag... | 0.08600941   |
|    prob_ratio           | 300570.53    |
|    rollout_return       | -1.8637179   |
| Time/                   |              |
|    collect_computeV/... | 0.0124       |
|    collect_computeV/Sum | 3.19         |
|    collect_rollout/Mean | 4.19         |
|    collect_rollout/Sum  | 4.19         |
|    train_action_adv/... | 0.00948      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0182       |
|    train_computeV/Sum   | 58.3         |
|    train_epoch/Mean     | 158          |
|    train_epoch/Sum      | 158          |
|    train_loss/Mean      | 0.00983      |
|    train_loss/Sum       | 31.4         |
| rollout/                |              |
|    ep_len_mean          | 163          |
|    ep_rew_mean          | -162         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 555          |
|    time_elapsed         | 86741        |
|    total_timesteps      | 142080       |
| train/                  |              |
|    active_example       | 145          |
|    approx_kl            | -0.030147657 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 110800       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00342      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00741       |
|    negative_advantag... | 0.060169235   |
|    positive_advantag... | 0.11404958    |
|    prob_ratio           | 365574.2      |
|    rollout_return       | -2.1055765    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00946       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00987       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | -160          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 556           |
|    time_elapsed         | 86904         |
|    total_timesteps      | 142336        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | -0.0004504621 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0191       |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 111000        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00232       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00588        |
|    negative_advantag... | 0.04691665     |
|    positive_advantag... | 0.11904046     |
|    prob_ratio           | 213568.94      |
|    rollout_return       | -2.1071322     |
| Time/                   |                |
|    collect_computeV/... | 0.0125         |
|    collect_computeV/Sum | 3.2            |
|    collect_rollout/Mean | 4.21           |
|    collect_rollout/Sum  | 4.21           |
|    train_action_adv/... | 0.00951        |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0181         |
|    train_computeV/Sum   | 58.1           |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00983        |
|    train_loss/Sum       | 31.5           |
| rollout/                |                |
|    ep_len_mean          | 161            |
|    ep_rew_mean          | -160           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 557            |
|    time_elapsed         | 87065          |
|    total_timesteps      | 142592         |
| train/                  |                |
|    active_example       | 129            |
|    approx_kl            | -4.7683716e-07 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00999       |
|    explained_variance   | 0.962          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 111200         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00121        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00781      |
|    negative_advantag... | 0.06348605   |
|    positive_advantag... | 0.11159215   |
|    prob_ratio           | 774340.9     |
|    rollout_return       | -1.8692929   |
| Time/                   |              |
|    collect_computeV/... | 0.0123       |
|    collect_computeV/Sum | 3.16         |
|    collect_rollout/Mean | 4.16         |
|    collect_rollout/Sum  | 4.16         |
|    train_action_adv/... | 0.00947      |
|    train_action_adv/Sum | 30.3         |
|    train_computeV/Mean  | 0.0183       |
|    train_computeV/Sum   | 58.4         |
|    train_epoch/Mean     | 157          |
|    train_epoch/Sum      | 157          |
|    train_loss/Mean      | 0.00975      |
|    train_loss/Sum       | 31.2         |
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | -161         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 558          |
|    time_elapsed         | 87226        |
|    total_timesteps      | 142848       |
| train/                  |              |
|    active_example       | 123          |
|    approx_kl            | 7.748604e-06 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0131      |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 111400       |
|    policy_gradient_loss | 0.0049       |
|    value_loss           | 0.00482      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00634       |
|    negative_advantag... | 0.044793673   |
|    positive_advantag... | 0.09958135    |
|    prob_ratio           | 25145.705     |
|    rollout_return       | -1.8469527    |
| Time/                   |               |
|    collect_computeV/... | 0.0124        |
|    collect_computeV/Sum | 3.18          |
|    collect_rollout/Mean | 4.17          |
|    collect_rollout/Sum  | 4.17          |
|    train_action_adv/... | 0.00945       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.3          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00983       |
|    train_loss/Sum       | 31.5          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 559           |
|    time_elapsed         | 87388         |
|    total_timesteps      | 143104        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | 1.8268824e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 111600        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00162       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0086        |
|    negative_advantag... | 0.060822304   |
|    positive_advantag... | 0.11400198    |
|    prob_ratio           | 661730.06     |
|    rollout_return       | -2.0497942    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.2           |
|    collect_rollout/Mean | 4.2           |
|    collect_rollout/Sum  | 4.2           |
|    train_action_adv/... | 0.00947       |
|    train_action_adv/Sum | 30.3          |
|    train_computeV/Mean  | 0.0182        |
|    train_computeV/Sum   | 58.2          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00986       |
|    train_loss/Sum       | 31.6          |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 560           |
|    time_elapsed         | 87550         |
|    total_timesteps      | 143360        |
| train/                  |               |
|    active_example       | 140           |
|    approx_kl            | 0.00021451712 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.027        |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 111800        |
|    policy_gradient_loss | 0.000357      |
|    value_loss           | 0.00489       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00891       |
|    negative_advantag... | 0.07188168    |
|    positive_advantag... | 0.0875911     |
|    prob_ratio           | 204590.92     |
|    rollout_return       | -1.8495141    |
| Time/                   |               |
|    collect_computeV/... | 0.0125        |
|    collect_computeV/Sum | 3.19          |
|    collect_rollout/Mean | 4.19          |
|    collect_rollout/Sum  | 4.19          |
|    train_action_adv/... | 0.00944       |
|    train_action_adv/Sum | 30.2          |
|    train_computeV/Mean  | 0.0183        |
|    train_computeV/Sum   | 58.7          |
|    train_epoch/Mean     | 158           |
|    train_epoch/Sum      | 158           |
|    train_loss/Mean      | 0.00975       |
|    train_loss/Sum       | 31.2          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 561           |
|    time_elapsed         | 87712         |
|    total_timesteps      | 143616        |
| train/                  |               |
|    active_example       | 116           |
|    approx_kl            | -0.0016152561 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 112000        |
|    policy_gradient_loss | 0.00309       |
|    value_loss           | 0.00267       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00957        |
|    negative_advantag... | 0.06357277     |
|    positive_advantag... | 0.103517115    |
|    prob_ratio           | 1166841.6      |
|    rollout_return       | -1.8797798     |
| Time/                   |                |
|    collect_computeV/... | 0.0124         |
|    collect_computeV/Sum | 3.18           |
|    collect_rollout/Mean | 4.18           |
|    collect_rollout/Sum  | 4.18           |
|    train_action_adv/... | 0.0095         |
|    train_action_adv/Sum | 30.4           |
|    train_computeV/Mean  | 0.0181         |
|    train_computeV/Sum   | 58             |
|    train_epoch/Mean     | 157            |
|    train_epoch/Sum      | 157            |
|    train_loss/Mean      | 0.00992        |
|    train_loss/Sum       | 31.7           |
| rollout/                |                |
|    ep_len_mean          | 160            |
|    ep_rew_mean          | -159           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 562            |
|    time_elapsed         | 87874          |
|    total_timesteps      | 143872         |
| train/                  |                |
|    active_example       | 254            |
|    approx_kl            | -0.00042060018 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0213        |
|    explained_variance   | 0.978          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 112200         |
|    policy_gradient_loss | 0.000196       |
|    value_loss           | 0.00198        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00973       |
|    negative_advantag... | 0.056440953   |
|    positive_advantag... | 0.09246533    |
|    prob_ratio           | 1832255.0     |
|    rollout_return       | -1.8530605    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.63          |
|    collect_rollout/Mean | 4.78          |
|    collect_rollout/Sum  | 4.78          |
|    train_action_adv/... | 0.0101        |
|    train_action_adv/Sum | 32.4          |
|    train_computeV/Mean  | 0.0195        |
|    train_computeV/Sum   | 62.2          |
|    train_epoch/Mean     | 166           |
|    train_epoch/Sum      | 166           |
|    train_loss/Mean      | 0.0104        |
|    train_loss/Sum       | 33.3          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 563           |
|    time_elapsed         | 88045         |
|    total_timesteps      | 144128        |
| train/                  |               |
|    active_example       | 132           |
|    approx_kl            | 4.7683716e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00606      |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 112400        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000608      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00873       |
|    negative_advantag... | 0.06361648    |
|    positive_advantag... | 0.12064151    |
|    prob_ratio           | 121295.41     |
|    rollout_return       | -2.0412216    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.63          |
|    collect_rollout/Mean | 4.77          |
|    collect_rollout/Sum  | 4.77          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 34.8          |
|    train_computeV/Mean  | 0.0205        |
|    train_computeV/Sum   | 65.5          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0113        |
|    train_loss/Sum       | 36.3          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 564           |
|    time_elapsed         | 88225         |
|    total_timesteps      | 144384        |
| train/                  |               |
|    active_example       | 129           |
|    approx_kl            | 0.00068973005 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0207       |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 112600        |
|    policy_gradient_loss | 0.000304      |
|    value_loss           | 0.000619      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00898       |
|    negative_advantag... | 0.07307259    |
|    positive_advantag... | 0.0982946     |
|    prob_ratio           | 807725.75     |
|    rollout_return       | -1.8978575    |
| Time/                   |               |
|    collect_computeV/... | 0.0139        |
|    collect_computeV/Sum | 3.56          |
|    collect_rollout/Mean | 4.69          |
|    collect_rollout/Sum  | 4.69          |
|    train_action_adv/... | 0.011         |
|    train_action_adv/Sum | 35.1          |
|    train_computeV/Mean  | 0.0205        |
|    train_computeV/Sum   | 65.7          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0113        |
|    train_loss/Sum       | 36.1          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 565           |
|    time_elapsed         | 88406         |
|    total_timesteps      | 144640        |
| train/                  |               |
|    active_example       | 255           |
|    approx_kl            | 5.9604645e-07 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0181       |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 112800        |
|    policy_gradient_loss | 4.7e-05       |
|    value_loss           | 0.00274       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00569     |
|    negative_advantag... | 0.05332952  |
|    positive_advantag... | 0.076729424 |
|    prob_ratio           | 86223.25    |
|    rollout_return       | -1.555365   |
| Time/                   |             |
|    collect_computeV/... | 0.0143      |
|    collect_computeV/Sum | 3.65        |
|    collect_rollout/Mean | 4.79        |
|    collect_rollout/Sum  | 4.79        |
|    train_action_adv/... | 0.0109      |
|    train_action_adv/Sum | 34.9        |
|    train_computeV/Mean  | 0.0206      |
|    train_computeV/Sum   | 66          |
|    train_epoch/Mean     | 176         |
|    train_epoch/Sum      | 176         |
|    train_loss/Mean      | 0.0112      |
|    train_loss/Sum       | 36          |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 566         |
|    time_elapsed         | 88587       |
|    total_timesteps      | 144896      |
| train/                  |             |
|    active_example       | 93          |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00413    |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 113000      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.00517     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00679      |
|    negative_advantag... | 0.048884086  |
|    positive_advantag... | 0.10519804   |
|    prob_ratio           | 330495.84    |
|    rollout_return       | -1.7144889   |
| Time/                   |              |
|    collect_computeV/... | 0.0142       |
|    collect_computeV/Sum | 3.62         |
|    collect_rollout/Mean | 4.77         |
|    collect_rollout/Sum  | 4.77         |
|    train_action_adv/... | 0.0109       |
|    train_action_adv/Sum | 34.9         |
|    train_computeV/Mean  | 0.0204       |
|    train_computeV/Sum   | 65.4         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36.3         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 567          |
|    time_elapsed         | 88768        |
|    total_timesteps      | 145152       |
| train/                  |              |
|    active_example       | 133          |
|    approx_kl            | 0.0003156513 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 113200       |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 0.006        |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00945      |
|    negative_advantag... | 0.06437653   |
|    positive_advantag... | 0.1116392    |
|    prob_ratio           | 13300.622    |
|    rollout_return       | -2.0389533   |
| Time/                   |              |
|    collect_computeV/... | 0.0142       |
|    collect_computeV/Sum | 3.64         |
|    collect_rollout/Mean | 4.8          |
|    collect_rollout/Sum  | 4.8          |
|    train_action_adv/... | 0.0109       |
|    train_action_adv/Sum | 34.8         |
|    train_computeV/Mean  | 0.0206       |
|    train_computeV/Sum   | 66.1         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36.1         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 568          |
|    time_elapsed         | 88949        |
|    total_timesteps      | 145408       |
| train/                  |              |
|    active_example       | 256          |
|    approx_kl            | -0.025097847 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0244      |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 113400       |
|    policy_gradient_loss | 0            |
|    value_loss           | 0.00168      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00825      |
|    negative_advantag... | 0.069663696  |
|    positive_advantag... | 0.108363695  |
|    prob_ratio           | 642148.75    |
|    rollout_return       | -1.9485924   |
| Time/                   |              |
|    collect_computeV/... | 0.0143       |
|    collect_computeV/Sum | 3.65         |
|    collect_rollout/Mean | 4.81         |
|    collect_rollout/Sum  | 4.81         |
|    train_action_adv/... | 0.011        |
|    train_action_adv/Sum | 35.1         |
|    train_computeV/Mean  | 0.0205       |
|    train_computeV/Sum   | 65.8         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36.2         |
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -160         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 569          |
|    time_elapsed         | 89130        |
|    total_timesteps      | 145664       |
| train/                  |              |
|    active_example       | 133          |
|    approx_kl            | 5.364418e-07 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 113600       |
|    policy_gradient_loss | 0.00591      |
|    value_loss           | 0.00217      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00591       |
|    negative_advantag... | 0.04381372    |
|    positive_advantag... | 0.104213655   |
|    prob_ratio           | 20751.797     |
|    rollout_return       | -1.8427131    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.63          |
|    collect_rollout/Mean | 4.77          |
|    collect_rollout/Sum  | 4.77          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 35            |
|    train_computeV/Mean  | 0.0206        |
|    train_computeV/Sum   | 66            |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0112        |
|    train_loss/Sum       | 35.9          |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 570           |
|    time_elapsed         | 89311         |
|    total_timesteps      | 145920        |
| train/                  |               |
|    active_example       | 116           |
|    approx_kl            | 1.6570091e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 113800        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00223       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.009         |
|    negative_advantag... | 0.053580537   |
|    positive_advantag... | 0.104368776   |
|    prob_ratio           | 680796.3      |
|    rollout_return       | -2.142408     |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.64          |
|    collect_rollout/Mean | 4.78          |
|    collect_rollout/Sum  | 4.78          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 34.9          |
|    train_computeV/Mean  | 0.0206        |
|    train_computeV/Sum   | 65.8          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0113        |
|    train_loss/Sum       | 36.3          |
| rollout/                |               |
|    ep_len_mean          | 157           |
|    ep_rew_mean          | -156          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 571           |
|    time_elapsed         | 89491         |
|    total_timesteps      | 146176        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | 3.8683414e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 114000        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.000817      |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0107         |
|    negative_advantag... | 0.08116309     |
|    positive_advantag... | 0.0914934      |
|    prob_ratio           | 504115.97      |
|    rollout_return       | -2.043488      |
| Time/                   |                |
|    collect_computeV/... | 0.0142         |
|    collect_computeV/Sum | 3.63           |
|    collect_rollout/Mean | 4.77           |
|    collect_rollout/Sum  | 4.77           |
|    train_action_adv/... | 0.0109         |
|    train_action_adv/Sum | 34.8           |
|    train_computeV/Mean  | 0.0207         |
|    train_computeV/Sum   | 66.1           |
|    train_epoch/Mean     | 176            |
|    train_epoch/Sum      | 176            |
|    train_loss/Mean      | 0.0113         |
|    train_loss/Sum       | 36.1           |
| rollout/                |                |
|    ep_len_mean          | 157            |
|    ep_rew_mean          | -156           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 572            |
|    time_elapsed         | 89672          |
|    total_timesteps      | 146432         |
| train/                  |                |
|    active_example       | 140            |
|    approx_kl            | -0.00022089481 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0165        |
|    explained_variance   | 0.949          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 114200         |
|    policy_gradient_loss | 0.00082        |
|    value_loss           | 0.0041         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00617        |
|    negative_advantag... | 0.046232834    |
|    positive_advantag... | 0.112556316    |
|    prob_ratio           | 508126.72      |
|    rollout_return       | -1.9726872     |
| Time/                   |                |
|    collect_computeV/... | 0.0141         |
|    collect_computeV/Sum | 3.62           |
|    collect_rollout/Mean | 4.76           |
|    collect_rollout/Sum  | 4.76           |
|    train_action_adv/... | 0.0109         |
|    train_action_adv/Sum | 35             |
|    train_computeV/Mean  | 0.0205         |
|    train_computeV/Sum   | 65.7           |
|    train_epoch/Mean     | 176            |
|    train_epoch/Sum      | 176            |
|    train_loss/Mean      | 0.0113         |
|    train_loss/Sum       | 36.1           |
| rollout/                |                |
|    ep_len_mean          | 158            |
|    ep_rew_mean          | -157           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 573            |
|    time_elapsed         | 89853          |
|    total_timesteps      | 146688         |
| train/                  |                |
|    active_example       | 97             |
|    approx_kl            | -1.3768673e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0144        |
|    explained_variance   | 0.848          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 114400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.0101         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00671     |
|    negative_advantag... | 0.050491005 |
|    positive_advantag... | 0.11195046  |
|    prob_ratio           | 109478.0    |
|    rollout_return       | -2.079958   |
| Time/                   |             |
|    collect_computeV/... | 0.0142      |
|    collect_computeV/Sum | 3.63        |
|    collect_rollout/Mean | 4.77        |
|    collect_rollout/Sum  | 4.77        |
|    train_action_adv/... | 0.0109      |
|    train_action_adv/Sum | 34.9        |
|    train_computeV/Mean  | 0.0206      |
|    train_computeV/Sum   | 65.8        |
|    train_epoch/Mean     | 176         |
|    train_epoch/Sum      | 176         |
|    train_loss/Mean      | 0.0113      |
|    train_loss/Sum       | 36.2        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 574         |
|    time_elapsed         | 90033       |
|    total_timesteps      | 146944      |
| train/                  |             |
|    active_example       | 251         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0124     |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 114600      |
|    policy_gradient_loss | 0           |
|    value_loss           | 0.0212      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00623      |
|    negative_advantag... | 0.040035084  |
|    positive_advantag... | 0.09953526   |
|    prob_ratio           | 283236.03    |
|    rollout_return       | -1.83256     |
| Time/                   |              |
|    collect_computeV/... | 0.0143       |
|    collect_computeV/Sum | 3.65         |
|    collect_rollout/Mean | 4.8          |
|    collect_rollout/Sum  | 4.8          |
|    train_action_adv/... | 0.011        |
|    train_action_adv/Sum | 35.2         |
|    train_computeV/Mean  | 0.0206       |
|    train_computeV/Sum   | 65.8         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36.1         |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 575          |
|    time_elapsed         | 90214        |
|    total_timesteps      | 147200       |
| train/                  |              |
|    active_example       | 147          |
|    approx_kl            | 0.0153102875 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0206      |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 114800       |
|    policy_gradient_loss | 0.000382     |
|    value_loss           | 0.00758      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00859      |
|    negative_advantag... | 0.06897707   |
|    positive_advantag... | 0.089636266  |
|    prob_ratio           | 396579.62    |
|    rollout_return       | -2.0262475   |
| Time/                   |              |
|    collect_computeV/... | 0.0141       |
|    collect_computeV/Sum | 3.62         |
|    collect_rollout/Mean | 4.76         |
|    collect_rollout/Sum  | 4.76         |
|    train_action_adv/... | 0.011        |
|    train_action_adv/Sum | 35.1         |
|    train_computeV/Mean  | 0.0205       |
|    train_computeV/Sum   | 65.7         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36           |
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | -159         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 576          |
|    time_elapsed         | 90394        |
|    total_timesteps      | 147456       |
| train/                  |              |
|    active_example       | 120          |
|    approx_kl            | -0.006755531 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 115000       |
|    policy_gradient_loss | 0.0014       |
|    value_loss           | 0.00217      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00672     |
|    negative_advantag... | 0.043187708 |
|    positive_advantag... | 0.14333583  |
|    prob_ratio           | 1855895.2   |
|    rollout_return       | -2.195193   |
| Time/                   |             |
|    collect_computeV/... | 0.0141      |
|    collect_computeV/Sum | 3.62        |
|    collect_rollout/Mean | 4.77        |
|    collect_rollout/Sum  | 4.77        |
|    train_action_adv/... | 0.0108      |
|    train_action_adv/Sum | 34.7        |
|    train_computeV/Mean  | 0.0206      |
|    train_computeV/Sum   | 65.9        |
|    train_epoch/Mean     | 176         |
|    train_epoch/Sum      | 176         |
|    train_loss/Mean      | 0.0113      |
|    train_loss/Sum       | 36.1        |
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | -159        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 577         |
|    time_elapsed         | 90575       |
|    total_timesteps      | 147712      |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0148     |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 115200      |
|    policy_gradient_loss | 6.77e-05    |
|    value_loss           | 0.000698    |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00721        |
|    negative_advantag... | 0.06393163     |
|    positive_advantag... | 0.0984512      |
|    prob_ratio           | 333946.38      |
|    rollout_return       | -1.785828      |
| Time/                   |                |
|    collect_computeV/... | 0.0143         |
|    collect_computeV/Sum | 3.65           |
|    collect_rollout/Mean | 4.8            |
|    collect_rollout/Sum  | 4.8            |
|    train_action_adv/... | 0.011          |
|    train_action_adv/Sum | 35             |
|    train_computeV/Mean  | 0.0206         |
|    train_computeV/Sum   | 66             |
|    train_epoch/Mean     | 176            |
|    train_epoch/Sum      | 176            |
|    train_loss/Mean      | 0.0113         |
|    train_loss/Sum       | 36.1           |
| rollout/                |                |
|    ep_len_mean          | 159            |
|    ep_rew_mean          | -158           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 578            |
|    time_elapsed         | 90755          |
|    total_timesteps      | 147968         |
| train/                  |                |
|    active_example       | 135            |
|    approx_kl            | -3.5762787e-07 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0153        |
|    explained_variance   | 0.972          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 115400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00231        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.00787    |
|    negative_advantag... | 0.07568418 |
|    positive_advantag... | 0.11478472 |
|    prob_ratio           | 352968.88  |
|    rollout_return       | -2.1014369 |
| Time/                   |            |
|    collect_computeV/... | 0.0142     |
|    collect_computeV/Sum | 3.65       |
|    collect_rollout/Mean | 4.78       |
|    collect_rollout/Sum  | 4.78       |
|    train_action_adv/... | 0.0109     |
|    train_action_adv/Sum | 35         |
|    train_computeV/Mean  | 0.0207     |
|    train_computeV/Sum   | 66.1       |
|    train_epoch/Mean     | 176        |
|    train_epoch/Sum      | 176        |
|    train_loss/Mean      | 0.0112     |
|    train_loss/Sum       | 36         |
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | -158       |
| time/                   |            |
|    fps                  | 1          |
|    iterations           | 579        |
|    time_elapsed         | 90937      |
|    total_timesteps      | 148224     |
| train/                  |            |
|    active_example       | 122        |
|    approx_kl            | 0.0        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00759   |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.01       |
|    loss                 | 0          |
|    n_updates            | 115600     |
|    policy_gradient_loss | 0          |
|    value_loss           | 0.00103    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00879       |
|    negative_advantag... | 0.08019179    |
|    positive_advantag... | 0.10265984    |
|    prob_ratio           | 1165454.0     |
|    rollout_return       | -1.9209509    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.64          |
|    collect_rollout/Mean | 4.79          |
|    collect_rollout/Sum  | 4.79          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 35            |
|    train_computeV/Mean  | 0.0205        |
|    train_computeV/Sum   | 65.7          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0113        |
|    train_loss/Sum       | 36.2          |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 580           |
|    time_elapsed         | 91117         |
|    total_timesteps      | 148480        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | -0.0013410747 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 115800        |
|    policy_gradient_loss | 6.4e-05       |
|    value_loss           | 0.00196       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00892       |
|    negative_advantag... | 0.06536153    |
|    positive_advantag... | 0.113759644   |
|    prob_ratio           | 1162558.6     |
|    rollout_return       | -2.0155056    |
| Time/                   |               |
|    collect_computeV/... | 0.0141        |
|    collect_computeV/Sum | 3.6           |
|    collect_rollout/Mean | 4.73          |
|    collect_rollout/Sum  | 4.73          |
|    train_action_adv/... | 0.011         |
|    train_action_adv/Sum | 35.1          |
|    train_computeV/Mean  | 0.0205        |
|    train_computeV/Sum   | 65.6          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0113        |
|    train_loss/Sum       | 36.2          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 581           |
|    time_elapsed         | 91297         |
|    total_timesteps      | 148736        |
| train/                  |               |
|    active_example       | 139           |
|    approx_kl            | -0.0017089844 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00783      |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 116000        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00846       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00733        |
|    negative_advantag... | 0.05417836     |
|    positive_advantag... | 0.0698061      |
|    prob_ratio           | 619612.94      |
|    rollout_return       | -1.9206338     |
| Time/                   |                |
|    collect_computeV/... | 0.0142         |
|    collect_computeV/Sum | 3.62           |
|    collect_rollout/Mean | 4.77           |
|    collect_rollout/Sum  | 4.77           |
|    train_action_adv/... | 0.011          |
|    train_action_adv/Sum | 35.2           |
|    train_computeV/Mean  | 0.0205         |
|    train_computeV/Sum   | 65.7           |
|    train_epoch/Mean     | 176            |
|    train_epoch/Sum      | 176            |
|    train_loss/Mean      | 0.0112         |
|    train_loss/Sum       | 35.9           |
| rollout/                |                |
|    ep_len_mean          | 159            |
|    ep_rew_mean          | -158           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 582            |
|    time_elapsed         | 91478          |
|    total_timesteps      | 148992         |
| train/                  |                |
|    active_example       | 83             |
|    approx_kl            | -5.0365925e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0108        |
|    explained_variance   | 0.694          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 116200         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.0117         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.01         |
|    negative_advantag... | 0.08271969   |
|    positive_advantag... | 0.12255379   |
|    prob_ratio           | 1642758.8    |
|    rollout_return       | -1.877037    |
| Time/                   |              |
|    collect_computeV/... | 0.0142       |
|    collect_computeV/Sum | 3.63         |
|    collect_rollout/Mean | 4.78         |
|    collect_rollout/Sum  | 4.78         |
|    train_action_adv/... | 0.0109       |
|    train_action_adv/Sum | 35           |
|    train_computeV/Mean  | 0.0206       |
|    train_computeV/Sum   | 65.9         |
|    train_epoch/Mean     | 176          |
|    train_epoch/Sum      | 176          |
|    train_loss/Mean      | 0.0113       |
|    train_loss/Sum       | 36.1         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 583          |
|    time_elapsed         | 91658        |
|    total_timesteps      | 149248       |
| train/                  |              |
|    active_example       | 254          |
|    approx_kl            | 0.0056857467 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0212      |
|    explained_variance   | 0.879        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 116400       |
|    policy_gradient_loss | 0.00108      |
|    value_loss           | 0.00518      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00733     |
|    negative_advantag... | 0.04931213  |
|    positive_advantag... | 0.13133252  |
|    prob_ratio           | 510625.28   |
|    rollout_return       | -1.753795   |
| Time/                   |             |
|    collect_computeV/... | 0.0142      |
|    collect_computeV/Sum | 3.63        |
|    collect_rollout/Mean | 4.77        |
|    collect_rollout/Sum  | 4.77        |
|    train_action_adv/... | 0.0109      |
|    train_action_adv/Sum | 34.8        |
|    train_computeV/Mean  | 0.0206      |
|    train_computeV/Sum   | 65.8        |
|    train_epoch/Mean     | 176         |
|    train_epoch/Sum      | 176         |
|    train_loss/Mean      | 0.0113      |
|    train_loss/Sum       | 36.1        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 584         |
|    time_elapsed         | 91839       |
|    total_timesteps      | 149504      |
| train/                  |             |
|    active_example       | 135         |
|    approx_kl            | 0.008007675 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0161     |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 116600      |
|    policy_gradient_loss | 0.000178    |
|    value_loss           | 0.00285     |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00764       |
|    negative_advantag... | 0.055657797   |
|    positive_advantag... | 0.10037753    |
|    prob_ratio           | 112265.31     |
|    rollout_return       | -1.8883786    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.64          |
|    collect_rollout/Mean | 4.78          |
|    collect_rollout/Sum  | 4.78          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 34.9          |
|    train_computeV/Mean  | 0.0206        |
|    train_computeV/Sum   | 66            |
|    train_epoch/Mean     | 175           |
|    train_epoch/Sum      | 175           |
|    train_loss/Mean      | 0.0112        |
|    train_loss/Sum       | 35.9          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 585           |
|    time_elapsed         | 92019         |
|    total_timesteps      | 149760        |
| train/                  |               |
|    active_example       | 124           |
|    approx_kl            | -0.0016810298 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 116800        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00129       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=150000, episode_reward=-164.80 +/- 18.84
Episode length: 165.80 +/- 18.84
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00671       |
|    negative_advantag... | 0.05656898    |
|    positive_advantag... | 0.1134701     |
|    prob_ratio           | 158444.69     |
|    rollout_return       | -1.9852072    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.63          |
|    collect_rollout/Mean | 6.55          |
|    collect_rollout/Sum  | 6.55          |
|    train_action_adv/... | 0.0109        |
|    train_action_adv/Sum | 34.8          |
|    train_computeV/Mean  | 0.0207        |
|    train_computeV/Sum   | 66.3          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0112        |
|    train_loss/Sum       | 35.9          |
| eval/                   |               |
|    mean_ep_length       | 166           |
|    mean_reward          | -165          |
| rollout/                |               |
|    ep_len_mean          | 159           |
|    ep_rew_mean          | -158          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 586           |
|    time_elapsed         | 92202         |
|    total_timesteps      | 150016        |
| train/                  |               |
|    active_example       | 256           |
|    approx_kl            | 2.9608607e-05 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 117000        |
|    policy_gradient_loss | 0.000354      |
|    value_loss           | 0.00115       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.00819       |
|    negative_advantag... | 0.06874684    |
|    positive_advantag... | 0.108089134   |
|    prob_ratio           | 1030614.9     |
|    rollout_return       | -1.9271536    |
| Time/                   |               |
|    collect_computeV/... | 0.0142        |
|    collect_computeV/Sum | 3.63          |
|    collect_rollout/Mean | 4.77          |
|    collect_rollout/Sum  | 4.77          |
|    train_action_adv/... | 0.011         |
|    train_action_adv/Sum | 35.1          |
|    train_computeV/Mean  | 0.0206        |
|    train_computeV/Sum   | 65.8          |
|    train_epoch/Mean     | 176           |
|    train_epoch/Sum      | 176           |
|    train_loss/Mean      | 0.0112        |
|    train_loss/Sum       | 36            |
| rollout/                |               |
|    ep_len_mean          | 158           |
|    ep_rew_mean          | -157          |
| time/                   |               |
|    fps                  | 1             |
|    iterations           | 587           |
|    time_elapsed         | 92382         |
|    total_timesteps      | 150272        |
| train/                  |               |
|    active_example       | 139           |
|    approx_kl            | 4.4107437e-06 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0154       |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.01          |
|    loss                 | 0             |
|    n_updates            | 117200        |
|    policy_gradient_loss | 0             |
|    value_loss           | 0.00139       |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.00688        |
|    negative_advantag... | 0.056588937    |
|    positive_advantag... | 0.064407155    |
|    prob_ratio           | 782508.75      |
|    rollout_return       | -1.5959864     |
| Time/                   |                |
|    collect_computeV/... | 0.0142         |
|    collect_computeV/Sum | 3.62           |
|    collect_rollout/Mean | 4.77           |
|    collect_rollout/Sum  | 4.77           |
|    train_action_adv/... | 0.0109         |
|    train_action_adv/Sum | 34.9           |
|    train_computeV/Mean  | 0.0206         |
|    train_computeV/Sum   | 66             |
|    train_epoch/Mean     | 177            |
|    train_epoch/Sum      | 177            |
|    train_loss/Mean      | 0.0113         |
|    train_loss/Sum       | 36.1           |
| rollout/                |                |
|    ep_len_mean          | 159            |
|    ep_rew_mean          | -158           |
| time/                   |                |
|    fps                  | 1              |
|    iterations           | 588            |
|    time_elapsed         | 92564          |
|    total_timesteps      | 150528         |
| train/                  |                |
|    active_example       | 120            |
|    approx_kl            | -3.2126904e-05 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0179        |
|    explained_variance   | 0.924          |
|    learning_rate        | 0.01           |
|    loss                 | 0              |
|    n_updates            | 117400         |
|    policy_gradient_loss | 0              |
|    value_loss           | 0.00589        |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.00647     |
|    negative_advantag... | 0.05087057  |
|    positive_advantag... | 0.081531785 |
|    prob_ratio           | 1180107.4   |
|    rollout_return       | -1.8133988  |
| Time/                   |             |
|    collect_computeV/... | 0.0143      |
|    collect_computeV/Sum | 3.66        |
|    collect_rollout/Mean | 4.81        |
|    collect_rollout/Sum  | 4.81        |
|    train_action_adv/... | 0.0109      |
|    train_action_adv/Sum | 34.9        |
|    train_computeV/Mean  | 0.0205      |
|    train_computeV/Sum   | 65.7        |
|    train_epoch/Mean     | 177         |
|    train_epoch/Sum      | 177         |
|    train_loss/Mean      | 0.0113      |
|    train_loss/Sum       | 36.3        |
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | -158        |
| time/                   |             |
|    fps                  | 1           |
|    iterations           | 589         |
|    time_elapsed         | 92745       |
|    total_timesteps      | 150784      |
| train/                  |             |
|    active_example       | 256         |
|    approx_kl            | 0.0         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0179     |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.01        |
|    loss                 | 0           |
|    n_updates            | 117600      |
|    policy_gradient_loss | 5.68e-05    |
|    value_loss           | 0.0035      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 16
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.00805      |
|    negative_advantag... | 0.05161241   |
|    positive_advantag... | 0.09135645   |
|    prob_ratio           | 34640.297    |
|    rollout_return       | -1.5581583   |
| Time/                   |              |
|    collect_computeV/... | 0.0142       |
|    collect_computeV/Sum | 3.64         |
|    collect_rollout/Mean | 4.78         |
|    collect_rollout/Sum  | 4.78         |
|    train_action_adv/... | 0.0109       |
|    train_action_adv/Sum | 34.9         |
|    train_computeV/Mean  | 0.0205       |
|    train_computeV/Sum   | 65.5         |
|    train_epoch/Mean     | 175          |
|    train_epoch/Sum      | 175          |
|    train_loss/Mean      | 0.0114       |
|    train_loss/Sum       | 36.4         |
| rollout/                |              |
|    ep_len_mean          | 159          |
|    ep_rew_mean          | -158         |
| time/                   |              |
|    fps                  | 1            |
|    iterations           | 590          |
|    time_elapsed         | 92925        |
|    total_timesteps      | 151040       |
| train/                  |              |
|    active_example       | 134          |
|    approx_kl            | 3.540516e-05 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0224      |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.01         |
|    loss                 | 0            |
|    n_updates            | 117800       |
|    policy_gradient_loss | 0.00176      |
|    value_loss           | 0.00175      |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 0.9
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
