gym.envs.registry.env_specs[env_id].entry_point gym.envs.classic_control:AcrobotEnv
gym.envs.registry.env_specs[env_id].entry_point gym.envs.classic_control:AcrobotEnv
OrderedDict([('actor_delay', 3),
             ('advantage_flipped_rate', 0.2),
             ('batch_size', 16),
             ('device', 1),
             ('ent_coef', 0.0),
             ('gae_lambda', 0.94),
             ('gamma', 0.99),
             ('independent_value_net', False),
             ('learning_rate', 0.01),
             ('n_envs', 16),
             ('n_epochs', 100),
             ('n_steps', 256),
             ('n_timesteps', 1500000),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs',
              {'optimizer_class': <class 'torch.optim.sgd.SGD'>}),
             ('policy_update_scheme', 1),
             ('rgamma', 1.0)])
Using 16 environments
Creating test environment
Normalization activated: {'gamma': 0.99, 'norm_reward': False}
Normalization activated: {'gamma': 0.99}
Using cuda:1 device
setup model true
Log path: logs/hpo/Acrobot-v1_158
n_eval_episodes 100
Logging to runs/Acrobot-v1__hpo__123__1665654619/Acrobot-v1/HPO_1
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------
| Time/                   |          |
|    collect_computeV/... | 0.00703  |
|    collect_computeV/Sum | 1.8      |
|    collect_rollout/Mean | 3.83     |
|    collect_rollout/Sum  | 3.83     |
| time/                   |          |
|    fps                  | 1067     |
|    iterations           | 1        |
|    time_elapsed         | 3        |
|    total_timesteps      | 4096     |
--------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0715      |
|    negative_advantag... | 0.45598662  |
|    positive_advantag... | 0.5440134   |
|    prob_ratio           | 1.0729784   |
|    rollout_return       | -0.9675303  |
| Time/                   |             |
|    collect_computeV/... | 0.00701     |
|    collect_computeV/Sum | 1.79        |
|    collect_rollout/Mean | 3.25        |
|    collect_rollout/Sum  | 3.25        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | -500        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 2           |
|    time_elapsed         | 1148        |
|    total_timesteps      | 8192        |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | -0.04913119 |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -0.176      |
|    learning_rate        | 0.01        |
|    loss                 | 0.214       |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0182      |
|    value_loss           | 0.679       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0731     |
|    negative_advantag... | 0.46603066 |
|    positive_advantag... | 0.53396726 |
|    prob_ratio           | 1.220979   |
|    rollout_return       | -1.30558   |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.79       |
|    collect_rollout/Mean | 7.98       |
|    collect_rollout/Sum  | 7.98       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | -500       |
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | -500       |
| time/                   |            |
|    fps                  | 5          |
|    iterations           | 3          |
|    time_elapsed         | 2298       |
|    total_timesteps      | 12288      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.16906026 |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | -0.0537    |
|    learning_rate        | 0.01       |
|    loss                 | 0.464      |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.0169     |
|    value_loss           | 0.0497     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0699     |
|    negative_advantag... | 0.43422833 |
|    positive_advantag... | 0.5657689  |
|    prob_ratio           | 1.192139   |
|    rollout_return       | -1.6312661 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 248        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | -493       |
| time/                   |            |
|    fps                  | 4          |
|    iterations           | 4          |
|    time_elapsed         | 3443       |
|    total_timesteps      | 16384      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.09462768 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.93      |
|    explained_variance   | -0.0939    |
|    learning_rate        | 0.01       |
|    loss                 | 0.000542   |
|    n_updates            | 300        |
|    policy_gradient_loss | 0.0175     |
|    value_loss           | 0.00485    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=20000, episode_reward=-301.60 +/- 108.47
Episode length: 302.40 +/- 108.10
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0647      |
|    negative_advantag... | 0.4467646   |
|    positive_advantag... | 0.553233    |
|    prob_ratio           | 2.3530722   |
|    rollout_return       | -1.7274716  |
| Time/                   |             |
|    collect_computeV/... | 0.00696     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 6.06        |
|    collect_rollout/Sum  | 6.06        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 366         |
|    train_epoch/Mean     | 1.15e+03    |
|    train_epoch/Sum      | 1.15e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 261         |
| eval/                   |             |
|    mean_ep_length       | 302         |
|    mean_reward          | -302        |
| rollout/                |             |
|    ep_len_mean          | 493         |
|    ep_rew_mean          | -493        |
| time/                   |             |
|    fps                  | 4           |
|    iterations           | 5           |
|    time_elapsed         | 4599        |
|    total_timesteps      | 20480       |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.008684084 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.01        |
|    loss                 | 0.104       |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 0.0744      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0638     |
|    negative_advantag... | 0.44395682 |
|    positive_advantag... | 0.5560318  |
|    prob_ratio           | 3.2646677  |
|    rollout_return       | -2.0433197 |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| rollout/                |            |
|    ep_len_mean          | 473        |
|    ep_rew_mean          | -473       |
| time/                   |            |
|    fps                  | 4          |
|    iterations           | 6          |
|    time_elapsed         | 5744       |
|    total_timesteps      | 24576      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.06421623 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.0624     |
|    learning_rate        | 0.01       |
|    loss                 | 0.832      |
|    n_updates            | 500        |
|    policy_gradient_loss | 0.0187     |
|    value_loss           | 0.00607    |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0598     |
|    negative_advantag... | 0.4482795  |
|    positive_advantag... | 0.5517001  |
|    prob_ratio           | 8.659381   |
|    rollout_return       | -2.0812254 |
| Time/                   |            |
|    collect_computeV/... | 0.00696    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.15e+03   |
|    train_epoch/Sum      | 1.15e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | -426       |
| time/                   |            |
|    fps                  | 4          |
|    iterations           | 7          |
|    time_elapsed         | 6893       |
|    total_timesteps      | 28672      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.05175788 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.61      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.01       |
|    loss                 | 0.358      |
|    n_updates            | 600        |
|    policy_gradient_loss | 0.0099     |
|    value_loss           | 0.0994     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=30000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0593     |
|    negative_advantag... | 0.4453849  |
|    positive_advantag... | 0.55460405 |
|    prob_ratio           | 8.560968   |
|    rollout_return       | -2.2589045 |
| Time/                   |            |
|    collect_computeV/... | 0.00715    |
|    collect_computeV/Sum | 1.83       |
|    collect_rollout/Mean | 8.11       |
|    collect_rollout/Sum  | 8.11       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | -500       |
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | -396       |
| time/                   |            |
|    fps                  | 4          |
|    iterations           | 8          |
|    time_elapsed         | 8046       |
|    total_timesteps      | 32768      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.03249961 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.01       |
|    loss                 | 0.658      |
|    n_updates            | 700        |
|    policy_gradient_loss | 0.0147     |
|    value_loss           | 0.0865     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0493     |
|    negative_advantag... | 0.3547933  |
|    positive_advantag... | 0.6451917  |
|    prob_ratio           | 7.9576416  |
|    rollout_return       | -2.3689737 |
| Time/                   |            |
|    collect_computeV/... | 0.007      |
|    collect_computeV/Sum | 1.79       |
|    collect_rollout/Mean | 3.24       |
|    collect_rollout/Sum  | 3.24       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 365        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| rollout/                |            |
|    ep_len_mean          | 369        |
|    ep_rew_mean          | -368       |
| time/                   |            |
|    fps                  | 4          |
|    iterations           | 9          |
|    time_elapsed         | 9192       |
|    total_timesteps      | 36864      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.2007784  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.555     |
|    explained_variance   | 0.475      |
|    learning_rate        | 0.01       |
|    loss                 | 0.195      |
|    n_updates            | 800        |
|    policy_gradient_loss | 0.0167     |
|    value_loss           | 0.0862     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=40000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0454     |
|    negative_advantag... | 0.34368062 |
|    positive_advantag... | 0.6563073  |
|    prob_ratio           | 9.975778   |
|    rollout_return       | -2.410075  |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 7.89       |
|    collect_rollout/Sum  | 7.89       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 365        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | -500       |
| rollout/                |            |
|    ep_len_mean          | 333        |
|    ep_rew_mean          | -332       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 10         |
|    time_elapsed         | 10345      |
|    total_timesteps      | 40960      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.16351019 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.457     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.01       |
|    loss                 | 0.157      |
|    n_updates            | 900        |
|    policy_gradient_loss | 0.00828    |
|    value_loss           | 0.0905     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.045      |
|    negative_advantag... | 0.35197082 |
|    positive_advantag... | 0.64799285 |
|    prob_ratio           | 55.61072   |
|    rollout_return       | -2.5562243 |
| Time/                   |            |
|    collect_computeV/... | 0.00691    |
|    collect_computeV/Sum | 1.77       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 313        |
|    ep_rew_mean          | -312       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 11         |
|    time_elapsed         | 11490      |
|    total_timesteps      | 45056      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.01058631 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.01       |
|    loss                 | 0.669      |
|    n_updates            | 1000       |
|    policy_gradient_loss | 0.0282     |
|    value_loss           | 0.0818     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0438      |
|    negative_advantag... | 0.34037483  |
|    positive_advantag... | 0.6595377   |
|    prob_ratio           | 53.79612    |
|    rollout_return       | -2.4211836  |
| Time/                   |             |
|    collect_computeV/... | 0.00692     |
|    collect_computeV/Sum | 1.77        |
|    collect_rollout/Mean | 3.2         |
|    collect_rollout/Sum  | 3.2         |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 364         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 292         |
|    ep_rew_mean          | -291        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 12          |
|    time_elapsed         | 12637       |
|    total_timesteps      | 49152       |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.057642214 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.01        |
|    loss                 | 0.233       |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.0203      |
|    value_loss           | 0.076       |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=50000, episode_reward=-250.40 +/- 23.03
Episode length: 251.40 +/- 23.03
New best mean reward!
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0431     |
|    negative_advantag... | 0.34864783 |
|    positive_advantag... | 0.65131384 |
|    prob_ratio           | 116.284935 |
|    rollout_return       | -2.6348052 |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.79       |
|    collect_rollout/Mean | 5.6        |
|    collect_rollout/Sum  | 5.6        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 251        |
|    mean_reward          | -250       |
| rollout/                |            |
|    ep_len_mean          | 274        |
|    ep_rew_mean          | -273       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 13         |
|    time_elapsed         | 13787      |
|    total_timesteps      | 53248      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.03670823 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.01       |
|    loss                 | 0.00437    |
|    n_updates            | 1200       |
|    policy_gradient_loss | 0.0108     |
|    value_loss           | 0.0689     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.04       |
|    negative_advantag... | 0.31855103 |
|    positive_advantag... | 0.6812398  |
|    prob_ratio           | 4266.074   |
|    rollout_return       | -2.6222265 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.15e+03   |
|    train_epoch/Sum      | 1.15e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 285        |
|    ep_rew_mean          | -284       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 14         |
|    time_elapsed         | 14936      |
|    total_timesteps      | 57344      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.10158084 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.313     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.01       |
|    loss                 | 1.3        |
|    n_updates            | 1300       |
|    policy_gradient_loss | 0.0188     |
|    value_loss           | 0.0614     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=60000, episode_reward=-436.00 +/- 128.00
Episode length: 436.20 +/- 127.60
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0403       |
|    negative_advantag... | 0.32401407   |
|    positive_advantag... | 0.67578614   |
|    prob_ratio           | 1287.1929    |
|    rollout_return       | -2.653667    |
| Time/                   |              |
|    collect_computeV/... | 0.00696      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 7.32         |
|    collect_rollout/Sum  | 7.32         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0143       |
|    train_computeV/Sum   | 366          |
|    train_epoch/Mean     | 1.15e+03     |
|    train_epoch/Sum      | 1.15e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 436          |
|    mean_reward          | -436         |
| rollout/                |              |
|    ep_len_mean          | 279          |
|    ep_rew_mean          | -278         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 15           |
|    time_elapsed         | 16092        |
|    total_timesteps      | 61440        |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.022774361 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.308       |
|    explained_variance   | 0.865        |
|    learning_rate        | 0.01         |
|    loss                 | 0.369        |
|    n_updates            | 1400         |
|    policy_gradient_loss | 0.0154       |
|    value_loss           | 0.0752       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0411       |
|    negative_advantag... | 0.33648297   |
|    positive_advantag... | 0.6632342    |
|    prob_ratio           | 1052.9888    |
|    rollout_return       | -2.7136438   |
| Time/                   |              |
|    collect_computeV/... | 0.00688      |
|    collect_computeV/Sum | 1.76         |
|    collect_rollout/Mean | 3.18         |
|    collect_rollout/Sum  | 3.18         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 363          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 283          |
|    ep_rew_mean          | -282         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 16           |
|    time_elapsed         | 17238        |
|    total_timesteps      | 65536        |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.033471156 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.875        |
|    learning_rate        | 0.01         |
|    loss                 | 0.262        |
|    n_updates            | 1500         |
|    policy_gradient_loss | 0.0106       |
|    value_loss           | 0.0599       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0372     |
|    negative_advantag... | 0.29838744 |
|    positive_advantag... | 0.7012463  |
|    prob_ratio           | 26441.893  |
|    rollout_return       | -2.5639548 |
| Time/                   |            |
|    collect_computeV/... | 0.00694    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 286        |
|    ep_rew_mean          | -285       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 17         |
|    time_elapsed         | 18383      |
|    total_timesteps      | 69632      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.13004042 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.276     |
|    explained_variance   | 0.909      |
|    learning_rate        | 0.01       |
|    loss                 | 0.0155     |
|    n_updates            | 1600       |
|    policy_gradient_loss | 0.0181     |
|    value_loss           | 0.0512     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=70000, episode_reward=-258.80 +/- 37.71
Episode length: 259.80 +/- 37.71
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0413      |
|    negative_advantag... | 0.3399837   |
|    positive_advantag... | 0.65893555  |
|    prob_ratio           | 166681.84   |
|    rollout_return       | -2.5958529  |
| Time/                   |             |
|    collect_computeV/... | 0.00692     |
|    collect_computeV/Sum | 1.77        |
|    collect_rollout/Mean | 5.62        |
|    collect_rollout/Sum  | 5.62        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0141      |
|    train_computeV/Sum   | 361         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | -259        |
| rollout/                |             |
|    ep_len_mean          | 270         |
|    ep_rew_mean          | -269        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 18          |
|    time_elapsed         | 19526       |
|    total_timesteps      | 73728       |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.042867392 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.01        |
|    loss                 | 0.589       |
|    n_updates            | 1700        |
|    policy_gradient_loss | 0.0173      |
|    value_loss           | 0.0396      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.036        |
|    negative_advantag... | 0.29550266   |
|    positive_advantag... | 0.70251524   |
|    prob_ratio           | 367142.38    |
|    rollout_return       | -2.5104287   |
| Time/                   |              |
|    collect_computeV/... | 0.00693      |
|    collect_computeV/Sum | 1.77         |
|    collect_rollout/Mean | 3.19         |
|    collect_rollout/Sum  | 3.19         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.15e+03     |
|    train_epoch/Sum      | 1.15e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 266          |
|    ep_rew_mean          | -265         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 19           |
|    time_elapsed         | 20676        |
|    total_timesteps      | 77824        |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.010508552 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.01         |
|    loss                 | 0.251        |
|    n_updates            | 1800         |
|    policy_gradient_loss | 0.0128       |
|    value_loss           | 0.067        |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=80000, episode_reward=-236.00 +/- 136.87
Episode length: 236.80 +/- 136.49
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0316      |
|    negative_advantag... | 0.25410467  |
|    positive_advantag... | 0.74213624  |
|    prob_ratio           | 422236.62   |
|    rollout_return       | -2.386246   |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 5.43        |
|    collect_rollout/Sum  | 5.43        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 365         |
|    train_epoch/Mean     | 1.15e+03    |
|    train_epoch/Sum      | 1.15e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | -236        |
| rollout/                |             |
|    ep_len_mean          | 253         |
|    ep_rew_mean          | -252        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 20          |
|    time_elapsed         | 21828       |
|    total_timesteps      | 81920       |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.055871673 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.218      |
|    explained_variance   | 0.903       |
|    learning_rate        | 0.01        |
|    loss                 | 0.243       |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.0158      |
|    value_loss           | 0.0393      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0324     |
|    negative_advantag... | 0.26044556 |
|    positive_advantag... | 0.73522115 |
|    prob_ratio           | 388093.3   |
|    rollout_return       | -2.5430906 |
| Time/                   |            |
|    collect_computeV/... | 0.00688    |
|    collect_computeV/Sum | 1.76       |
|    collect_rollout/Mean | 3.15       |
|    collect_rollout/Sum  | 3.15       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 255        |
|    ep_rew_mean          | -254       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 21         |
|    time_elapsed         | 22973      |
|    total_timesteps      | 86016      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.04045915 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.01       |
|    loss                 | 0.502      |
|    n_updates            | 2000       |
|    policy_gradient_loss | 0.0211     |
|    value_loss           | 0.0548     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=90000, episode_reward=-282.00 +/- 109.57
Episode length: 282.80 +/- 109.17
------------------------------------------
| HPO/                    |              |
|    margin               | 0.036        |
|    negative_advantag... | 0.29738736   |
|    positive_advantag... | 0.6988923    |
|    prob_ratio           | 471603.38    |
|    rollout_return       | -2.5702364   |
| Time/                   |              |
|    collect_computeV/... | 0.00695      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 5.86         |
|    collect_rollout/Sum  | 5.86         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | -282         |
| rollout/                |              |
|    ep_len_mean          | 256          |
|    ep_rew_mean          | -255         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 22           |
|    time_elapsed         | 24123        |
|    total_timesteps      | 90112        |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.016674623 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.01         |
|    loss                 | 0.306        |
|    n_updates            | 2100         |
|    policy_gradient_loss | 0.00839      |
|    value_loss           | 0.0312       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0376     |
|    negative_advantag... | 0.30915982 |
|    positive_advantag... | 0.6754107  |
|    prob_ratio           | 849202.5   |
|    rollout_return       | -2.4635315 |
| Time/                   |            |
|    collect_computeV/... | 0.00702    |
|    collect_computeV/Sum | 1.8        |
|    collect_rollout/Mean | 3.25       |
|    collect_rollout/Sum  | 3.25       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 362        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 257        |
|    ep_rew_mean          | -256       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 23         |
|    time_elapsed         | 25266      |
|    total_timesteps      | 94208      |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.1318333  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.188     |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.01       |
|    loss                 | 0.359      |
|    n_updates            | 2200       |
|    policy_gradient_loss | 0.0184     |
|    value_loss           | 0.0439     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.038       |
|    negative_advantag... | 0.31187114  |
|    positive_advantag... | 0.6740544   |
|    prob_ratio           | 986806.0    |
|    rollout_return       | -2.4418435  |
| Time/                   |             |
|    collect_computeV/... | 0.00696     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 3.23        |
|    collect_rollout/Sum  | 3.23        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 364         |
|    train_epoch/Mean     | 1.15e+03    |
|    train_epoch/Sum      | 1.15e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 261         |
|    ep_rew_mean          | -260        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 24          |
|    time_elapsed         | 26415       |
|    total_timesteps      | 98304       |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.086638995 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.01        |
|    loss                 | 0.181       |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.0194      |
|    value_loss           | 0.0571      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=100000, episode_reward=-223.00 +/- 54.37
Episode length: 224.00 +/- 54.37
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0403      |
|    negative_advantag... | 0.32569543  |
|    positive_advantag... | 0.64753556  |
|    prob_ratio           | 920252.2    |
|    rollout_return       | -2.2848382  |
| Time/                   |             |
|    collect_computeV/... | 0.00694     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 5.31        |
|    collect_rollout/Sum  | 5.31        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 364         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 261         |
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | -223        |
| rollout/                |             |
|    ep_len_mean          | 263         |
|    ep_rew_mean          | -262        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 25          |
|    time_elapsed         | 27564       |
|    total_timesteps      | 102400      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.059321336 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.01        |
|    loss                 | 0.127       |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.00541     |
|    value_loss           | 0.0477      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0373      |
|    negative_advantag... | 0.2847951   |
|    positive_advantag... | 0.67599946  |
|    prob_ratio           | 1140554.1   |
|    rollout_return       | -2.3211033  |
| Time/                   |             |
|    collect_computeV/... | 0.00694     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 3.2         |
|    collect_rollout/Sum  | 3.2         |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 256         |
|    ep_rew_mean          | -255        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 26          |
|    time_elapsed         | 28710       |
|    total_timesteps      | 106496      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.030898876 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.01        |
|    loss                 | 0.101       |
|    n_updates            | 2500        |
|    policy_gradient_loss | 0.0203      |
|    value_loss           | 0.0592      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=110000, episode_reward=-175.40 +/- 54.79
Episode length: 176.40 +/- 54.79
New best mean reward!
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0357     |
|    negative_advantag... | 0.24023022 |
|    positive_advantag... | 0.67859346 |
|    prob_ratio           | 970580.94  |
|    rollout_return       | -2.11038   |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 4.87       |
|    collect_rollout/Sum  | 4.87       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | -175       |
| rollout/                |            |
|    ep_len_mean          | 242        |
|    ep_rew_mean          | -241       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 27         |
|    time_elapsed         | 29856      |
|    total_timesteps      | 110592     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.19999209 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.166     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.01       |
|    loss                 | 0.333      |
|    n_updates            | 2600       |
|    policy_gradient_loss | 0.0174     |
|    value_loss           | 0.0509     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0362     |
|    negative_advantag... | 0.2502313  |
|    positive_advantag... | 0.6376107  |
|    prob_ratio           | 1182293.6  |
|    rollout_return       | -2.1358368 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.22       |
|    collect_rollout/Sum  | 3.22       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | -221       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 28         |
|    time_elapsed         | 31001      |
|    total_timesteps      | 114688     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.18142076 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.01       |
|    loss                 | 0.256      |
|    n_updates            | 2700       |
|    policy_gradient_loss | 0.00897    |
|    value_loss           | 0.0309     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0376        |
|    negative_advantag... | 0.26767388    |
|    positive_advantag... | 0.6051316     |
|    prob_ratio           | 1487565.8     |
|    rollout_return       | -1.8905265    |
| Time/                   |               |
|    collect_computeV/... | 0.00702       |
|    collect_computeV/Sum | 1.8           |
|    collect_rollout/Mean | 3.25          |
|    collect_rollout/Sum  | 3.25          |
|    train_action_adv/... | 0.00972       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0142        |
|    train_computeV/Sum   | 362           |
|    train_epoch/Mean     | 1.14e+03      |
|    train_epoch/Sum      | 1.14e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| rollout/                |               |
|    ep_len_mean          | 204           |
|    ep_rew_mean          | -204          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 29            |
|    time_elapsed         | 32145         |
|    total_timesteps      | 118784        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0129037425 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.137        |
|    explained_variance   | 0.818         |
|    learning_rate        | 0.01          |
|    loss                 | 2.3           |
|    n_updates            | 2800          |
|    policy_gradient_loss | 0.0179        |
|    value_loss           | 0.0423        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=120000, episode_reward=-164.40 +/- 14.42
Episode length: 165.40 +/- 14.42
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0396      |
|    negative_advantag... | 0.30214605  |
|    positive_advantag... | 0.5843308   |
|    prob_ratio           | 1764923.0   |
|    rollout_return       | -1.9075515  |
| Time/                   |             |
|    collect_computeV/... | 0.00694     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 4.77        |
|    collect_rollout/Sum  | 4.77        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | -164        |
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | -194        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 30          |
|    time_elapsed         | 33292       |
|    total_timesteps      | 122880      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.023479424 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.01        |
|    loss                 | 0.431       |
|    n_updates            | 2900        |
|    policy_gradient_loss | 0.0216      |
|    value_loss           | 0.0426      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0415      |
|    negative_advantag... | 0.3296337   |
|    positive_advantag... | 0.523255    |
|    prob_ratio           | 1003988.56  |
|    rollout_return       | -1.9146935  |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 3.33        |
|    collect_rollout/Sum  | 3.33        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | -189        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 31          |
|    time_elapsed         | 34439       |
|    total_timesteps      | 126976      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.017392091 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.01        |
|    loss                 | 0.378       |
|    n_updates            | 3000        |
|    policy_gradient_loss | 0.00961     |
|    value_loss           | 0.0564      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=130000, episode_reward=-278.20 +/- 182.19
Episode length: 278.80 +/- 181.71
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0369      |
|    negative_advantag... | 0.27550176  |
|    positive_advantag... | 0.56222135  |
|    prob_ratio           | 1612880.4   |
|    rollout_return       | -1.9374706  |
| Time/                   |             |
|    collect_computeV/... | 0.00703     |
|    collect_computeV/Sum | 1.8         |
|    collect_rollout/Mean | 5.86        |
|    collect_rollout/Sum  | 5.86        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | -278        |
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | -188        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 32          |
|    time_elapsed         | 35587       |
|    total_timesteps      | 131072      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.007761568 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.01        |
|    loss                 | 0.361       |
|    n_updates            | 3100        |
|    policy_gradient_loss | 0.0189      |
|    value_loss           | 0.0462      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.034      |
|    negative_advantag... | 0.24542503 |
|    positive_advantag... | 0.58143556 |
|    prob_ratio           | 1414337.0  |
|    rollout_return       | -1.9334651 |
| Time/                   |            |
|    collect_computeV/... | 0.00693    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.18       |
|    collect_rollout/Sum  | 3.18       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | -185       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 33         |
|    time_elapsed         | 36735      |
|    total_timesteps      | 135168     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.04235836 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.01       |
|    loss                 | 0.375      |
|    n_updates            | 3200       |
|    policy_gradient_loss | 0.0217     |
|    value_loss           | 0.0604     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0362       |
|    negative_advantag... | 0.269238     |
|    positive_advantag... | 0.5119928    |
|    prob_ratio           | 1352512.1    |
|    rollout_return       | -1.9235746   |
| Time/                   |              |
|    collect_computeV/... | 0.00693      |
|    collect_computeV/Sum | 1.77         |
|    collect_rollout/Mean | 3.19         |
|    collect_rollout/Sum  | 3.19         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 261          |
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | -181         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 34           |
|    time_elapsed         | 37882        |
|    total_timesteps      | 139264       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.008066408 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.785        |
|    learning_rate        | 0.01         |
|    loss                 | 0.129        |
|    n_updates            | 3300         |
|    policy_gradient_loss | 0.0132       |
|    value_loss           | 0.0589       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=140000, episode_reward=-259.80 +/- 112.73
Episode length: 260.80 +/- 112.73
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0375       |
|    negative_advantag... | 0.29094478   |
|    positive_advantag... | 0.54005194   |
|    prob_ratio           | 1254173.8    |
|    rollout_return       | -1.9672977   |
| Time/                   |              |
|    collect_computeV/... | 0.007        |
|    collect_computeV/Sum | 1.79         |
|    collect_rollout/Mean | 5.7          |
|    collect_rollout/Sum  | 5.7          |
|    train_action_adv/... | 0.00971      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 363          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | -260         |
| rollout/                |              |
|    ep_len_mean          | 176          |
|    ep_rew_mean          | -175         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 35           |
|    time_elapsed         | 39030        |
|    total_timesteps      | 143360       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.031397253 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.01         |
|    loss                 | 0.411        |
|    n_updates            | 3400         |
|    policy_gradient_loss | 0.0215       |
|    value_loss           | 0.0588       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0352      |
|    negative_advantag... | 0.25816268  |
|    positive_advantag... | 0.5375504   |
|    prob_ratio           | 1102613.5   |
|    rollout_return       | -1.9863919  |
| Time/                   |             |
|    collect_computeV/... | 0.00691     |
|    collect_computeV/Sum | 1.77        |
|    collect_rollout/Mean | 3.19        |
|    collect_rollout/Sum  | 3.19        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 363         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | -177        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 36          |
|    time_elapsed         | 40176       |
|    total_timesteps      | 147456      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.047387496 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.01        |
|    loss                 | 0.736       |
|    n_updates            | 3500        |
|    policy_gradient_loss | 0.0272      |
|    value_loss           | 0.0542      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=150000, episode_reward=-141.80 +/- 11.69
Episode length: 142.80 +/- 11.69
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0352      |
|    negative_advantag... | 0.27119747  |
|    positive_advantag... | 0.5338826   |
|    prob_ratio           | 1314018.2   |
|    rollout_return       | -1.9836679  |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 4.56        |
|    collect_rollout/Sum  | 4.56        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0142      |
|    train_computeV/Sum   | 364         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | -142        |
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | -188        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 37          |
|    time_elapsed         | 41326       |
|    total_timesteps      | 151552      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.040018335 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.01        |
|    loss                 | 0.675       |
|    n_updates            | 3600        |
|    policy_gradient_loss | 0.0147      |
|    value_loss           | 0.0607      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0362     |
|    negative_advantag... | 0.28151762 |
|    positive_advantag... | 0.5058142  |
|    prob_ratio           | 1256010.8  |
|    rollout_return       | -2.0479608 |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.22       |
|    collect_rollout/Sum  | 3.22       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 363        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | -199       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 38         |
|    time_elapsed         | 42473      |
|    total_timesteps      | 155648     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.00796178 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0974    |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.01       |
|    loss                 | 0.131      |
|    n_updates            | 3700       |
|    policy_gradient_loss | 0.0122     |
|    value_loss           | 0.0685     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0382     |
|    negative_advantag... | 0.299368   |
|    positive_advantag... | 0.50202376 |
|    prob_ratio           | 1309311.0  |
|    rollout_return       | -2.058383  |
| Time/                   |            |
|    collect_computeV/... | 0.00692    |
|    collect_computeV/Sum | 1.77       |
|    collect_rollout/Mean | 3.19       |
|    collect_rollout/Sum  | 3.19       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0141     |
|    train_computeV/Sum   | 362        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | -208       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 39         |
|    time_elapsed         | 43615      |
|    total_timesteps      | 159744     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.07826726 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.01       |
|    loss                 | 0.129      |
|    n_updates            | 3800       |
|    policy_gradient_loss | 0.0294     |
|    value_loss           | 0.0744     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=160000, episode_reward=-179.40 +/- 42.65
Episode length: 180.40 +/- 42.65
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0345       |
|    negative_advantag... | 0.24934341   |
|    positive_advantag... | 0.4761402    |
|    prob_ratio           | 1826551.5    |
|    rollout_return       | -2.0707102   |
| Time/                   |              |
|    collect_computeV/... | 0.00696      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 4.99         |
|    collect_rollout/Sum  | 4.99         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 180          |
|    mean_reward          | -179         |
| rollout/                |              |
|    ep_len_mean          | 205          |
|    ep_rew_mean          | -204         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 40           |
|    time_elapsed         | 44764        |
|    total_timesteps      | 163840       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.021562994 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0898      |
|    explained_variance   | 0.795        |
|    learning_rate        | 0.01         |
|    loss                 | 0.443        |
|    n_updates            | 3900         |
|    policy_gradient_loss | 0.018        |
|    value_loss           | 0.0423       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0324       |
|    negative_advantag... | 0.23079927   |
|    positive_advantag... | 0.46944684   |
|    prob_ratio           | 1629260.6    |
|    rollout_return       | -2.0794156   |
| Time/                   |              |
|    collect_computeV/... | 0.00697      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 3.19         |
|    collect_rollout/Sum  | 3.19         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 363          |
|    train_epoch/Mean     | 1.15e+03     |
|    train_epoch/Sum      | 1.15e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 209          |
|    ep_rew_mean          | -208         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 41           |
|    time_elapsed         | 45912        |
|    total_timesteps      | 167936       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.009735793 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0878      |
|    explained_variance   | 0.793        |
|    learning_rate        | 0.01         |
|    loss                 | 0.475        |
|    n_updates            | 4000         |
|    policy_gradient_loss | 0.0229       |
|    value_loss           | 0.0367       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=170000, episode_reward=-177.60 +/- 57.94
Episode length: 178.60 +/- 57.94
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0309     |
|    negative_advantag... | 0.21820605 |
|    positive_advantag... | 0.42969203 |
|    prob_ratio           | 1440671.9  |
|    rollout_return       | -2.1000664 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 4.86       |
|    collect_rollout/Sum  | 4.86       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 179        |
|    mean_reward          | -178       |
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | -210       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 42         |
|    time_elapsed         | 47062      |
|    total_timesteps      | 172032     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | -0.016727  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.1       |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.01       |
|    loss                 | 0.514      |
|    n_updates            | 4100       |
|    policy_gradient_loss | 0.0218     |
|    value_loss           | 0.0506     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.032        |
|    negative_advantag... | 0.23475716   |
|    positive_advantag... | 0.4137419    |
|    prob_ratio           | 1456351.6    |
|    rollout_return       | -2.1098294   |
| Time/                   |              |
|    collect_computeV/... | 0.00698      |
|    collect_computeV/Sum | 1.79         |
|    collect_rollout/Mean | 3.23         |
|    collect_rollout/Sum  | 3.23         |
|    train_action_adv/... | 0.00971      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 224          |
|    ep_rew_mean          | -223         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 43           |
|    time_elapsed         | 48208        |
|    total_timesteps      | 176128       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | 0.0026705638 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0863      |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.01         |
|    loss                 | 0.144        |
|    n_updates            | 4200         |
|    policy_gradient_loss | 0.0113       |
|    value_loss           | 0.0482       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=180000, episode_reward=-214.40 +/- 133.47
Episode length: 215.40 +/- 133.47
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0327     |
|    negative_advantag... | 0.22762555 |
|    positive_advantag... | 0.41142488 |
|    prob_ratio           | 1734593.2  |
|    rollout_return       | -2.0897632 |
| Time/                   |            |
|    collect_computeV/... | 0.00694    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 5.22       |
|    collect_rollout/Sum  | 5.22       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.15e+03   |
|    train_epoch/Sum      | 1.15e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | -214       |
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | -212       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 44         |
|    time_elapsed         | 49359      |
|    total_timesteps      | 180224     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.09383604 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0873    |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.01       |
|    loss                 | 0.251      |
|    n_updates            | 4300       |
|    policy_gradient_loss | 0.0223     |
|    value_loss           | 0.0653     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0302        |
|    negative_advantag... | 0.21158162    |
|    positive_advantag... | 0.44798356    |
|    prob_ratio           | 1773457.2     |
|    rollout_return       | -2.0989606    |
| Time/                   |               |
|    collect_computeV/... | 0.00695       |
|    collect_computeV/Sum | 1.78          |
|    collect_rollout/Mean | 3.2           |
|    collect_rollout/Sum  | 3.2           |
|    train_action_adv/... | 0.00971       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0141        |
|    train_computeV/Sum   | 361           |
|    train_epoch/Mean     | 1.13e+03      |
|    train_epoch/Sum      | 1.13e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| rollout/                |               |
|    ep_len_mean          | 218           |
|    ep_rew_mean          | -218          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 45            |
|    time_elapsed         | 50496         |
|    total_timesteps      | 184320        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0125050545 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0737       |
|    explained_variance   | 0.854         |
|    learning_rate        | 0.01          |
|    loss                 | 0.75          |
|    n_updates            | 4400          |
|    policy_gradient_loss | 0.0213        |
|    value_loss           | 0.0294        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0306      |
|    negative_advantag... | 0.21732761  |
|    positive_advantag... | 0.4213028   |
|    prob_ratio           | 1528505.9   |
|    rollout_return       | -2.1292188  |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 3.21        |
|    collect_rollout/Sum  | 3.21        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0141      |
|    train_computeV/Sum   | 362         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | -201        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 46          |
|    time_elapsed         | 51640       |
|    total_timesteps      | 188416      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.021316648 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0755     |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.01        |
|    loss                 | 0.127       |
|    n_updates            | 4500        |
|    policy_gradient_loss | 0.014       |
|    value_loss           | 0.0487      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=190000, episode_reward=-173.80 +/- 94.85
Episode length: 174.80 +/- 94.85
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0281       |
|    negative_advantag... | 0.19596435   |
|    positive_advantag... | 0.4082375    |
|    prob_ratio           | 1627867.6    |
|    rollout_return       | -2.0403495   |
| Time/                   |              |
|    collect_computeV/... | 0.007        |
|    collect_computeV/Sum | 1.79         |
|    collect_rollout/Mean | 4.9          |
|    collect_rollout/Sum  | 4.9          |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 365          |
|    train_epoch/Mean     | 1.15e+03     |
|    train_epoch/Sum      | 1.15e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | -174         |
| rollout/                |              |
|    ep_len_mean          | 191          |
|    ep_rew_mean          | -190         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 47           |
|    time_elapsed         | 52791        |
|    total_timesteps      | 192512       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.042563282 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0644      |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.01         |
|    loss                 | 0.864        |
|    n_updates            | 4600         |
|    policy_gradient_loss | 0.0234       |
|    value_loss           | 0.0423       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0308     |
|    negative_advantag... | 0.22965328 |
|    positive_advantag... | 0.39232224 |
|    prob_ratio           | 1414526.8  |
|    rollout_return       | -2.137973  |
| Time/                   |            |
|    collect_computeV/... | 0.00702    |
|    collect_computeV/Sum | 1.8        |
|    collect_rollout/Mean | 3.24       |
|    collect_rollout/Sum  | 3.24       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0141     |
|    train_computeV/Sum   | 362        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | -182       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 48         |
|    time_elapsed         | 53933      |
|    total_timesteps      | 196608     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.05921792 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0727    |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.01       |
|    loss                 | 0.006      |
|    n_updates            | 4700       |
|    policy_gradient_loss | 0.0397     |
|    value_loss           | 0.0433     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=200000, episode_reward=-205.80 +/- 148.23
Episode length: 206.60 +/- 147.83
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0312     |
|    negative_advantag... | 0.23491248 |
|    positive_advantag... | 0.38584217 |
|    prob_ratio           | 1414314.4  |
|    rollout_return       | -2.1363714 |
| Time/                   |            |
|    collect_computeV/... | 0.00697    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 5.13       |
|    collect_rollout/Sum  | 5.13       |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0143     |
|    train_computeV/Sum   | 366        |
|    train_epoch/Mean     | 1.15e+03   |
|    train_epoch/Sum      | 1.15e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 261        |
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | -206       |
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | -173       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 49         |
|    time_elapsed         | 55085      |
|    total_timesteps      | 200704     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.06079802 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0784    |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.01       |
|    loss                 | 0.251      |
|    n_updates            | 4800       |
|    policy_gradient_loss | 0.0189     |
|    value_loss           | 0.0773     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
--------------------------------------------
| HPO/                    |                |
|    margin               | 0.0305         |
|    negative_advantag... | 0.21248654     |
|    positive_advantag... | 0.416232       |
|    prob_ratio           | 1707064.0      |
|    rollout_return       | -2.0660105     |
| Time/                   |                |
|    collect_computeV/... | 0.007          |
|    collect_computeV/Sum | 1.79           |
|    collect_rollout/Mean | 3.28           |
|    collect_rollout/Sum  | 3.28           |
|    train_action_adv/... | 0.00972        |
|    train_action_adv/Sum | 249            |
|    train_computeV/Mean  | 0.0142         |
|    train_computeV/Sum   | 363            |
|    train_epoch/Mean     | 1.14e+03       |
|    train_epoch/Sum      | 1.14e+03       |
|    train_loss/Mean      | 0.0102         |
|    train_loss/Sum       | 260            |
| rollout/                |                |
|    ep_len_mean          | 176            |
|    ep_rew_mean          | -175           |
| time/                   |                |
|    fps                  | 3              |
|    iterations           | 50             |
|    time_elapsed         | 56232          |
|    total_timesteps      | 204800         |
| train/                  |                |
|    active_example       | 4096           |
|    approx_kl            | -0.00037144125 |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0741        |
|    explained_variance   | 0.851          |
|    learning_rate        | 0.01           |
|    loss                 | 0.127          |
|    n_updates            | 4900           |
|    policy_gradient_loss | 0.0264         |
|    value_loss           | 0.0473         |
--------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.029        |
|    negative_advantag... | 0.2100196    |
|    positive_advantag... | 0.41368908   |
|    prob_ratio           | 1277543.9    |
|    rollout_return       | -2.1397996   |
| Time/                   |              |
|    collect_computeV/... | 0.00694      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 3.19         |
|    collect_rollout/Sum  | 3.19         |
|    train_action_adv/... | 0.00972      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0143       |
|    train_computeV/Sum   | 365          |
|    train_epoch/Mean     | 1.15e+03     |
|    train_epoch/Sum      | 1.15e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | -182         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 51           |
|    time_elapsed         | 57384        |
|    total_timesteps      | 208896       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.019828886 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0761      |
|    explained_variance   | 0.809        |
|    learning_rate        | 0.01         |
|    loss                 | 0.768        |
|    n_updates            | 5000         |
|    policy_gradient_loss | 0.0286       |
|    value_loss           | 0.0639       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=210000, episode_reward=-220.40 +/- 144.42
Episode length: 221.20 +/- 144.03
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0321       |
|    negative_advantag... | 0.2260417    |
|    positive_advantag... | 0.43216896   |
|    prob_ratio           | 1811363.5    |
|    rollout_return       | -2.2608914   |
| Time/                   |              |
|    collect_computeV/... | 0.00701      |
|    collect_computeV/Sum | 1.8          |
|    collect_rollout/Mean | 5.39         |
|    collect_rollout/Sum  | 5.39         |
|    train_action_adv/... | 0.00971      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 363          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | -220         |
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | -181         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 52           |
|    time_elapsed         | 58529        |
|    total_timesteps      | 212992       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.025623113 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0792      |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.01         |
|    loss                 | 0.25         |
|    n_updates            | 5100         |
|    policy_gradient_loss | 0.0188       |
|    value_loss           | 0.0495       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.029      |
|    negative_advantag... | 0.20752198 |
|    positive_advantag... | 0.3913894  |
|    prob_ratio           | 1229223.5  |
|    rollout_return       | -2.118244  |
| Time/                   |            |
|    collect_computeV/... | 0.00696    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.21       |
|    collect_rollout/Sum  | 3.21       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | -178       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 53         |
|    time_elapsed         | 59676      |
|    total_timesteps      | 217088     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.02665919 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0738    |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.01       |
|    loss                 | 0.00398    |
|    n_updates            | 5200       |
|    policy_gradient_loss | 0.0268     |
|    value_loss           | 0.0715     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=220000, episode_reward=-113.60 +/- 16.51
Episode length: 114.60 +/- 16.51
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0271      |
|    negative_advantag... | 0.19929764  |
|    positive_advantag... | 0.3960784   |
|    prob_ratio           | 1502616.0   |
|    rollout_return       | -2.0042913  |
| Time/                   |             |
|    collect_computeV/... | 0.00693     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 4.31        |
|    collect_rollout/Sum  | 4.31        |
|    train_action_adv/... | 0.0097      |
|    train_action_adv/Sum | 248         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 366         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | -114        |
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | -171        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 54          |
|    time_elapsed         | 60825       |
|    total_timesteps      | 221184      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.025724113 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0681     |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.01        |
|    loss                 | 0.347       |
|    n_updates            | 5300        |
|    policy_gradient_loss | 0.0224      |
|    value_loss           | 0.0523      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0244        |
|    negative_advantag... | 0.17674227    |
|    positive_advantag... | 0.37135345    |
|    prob_ratio           | 1327389.0     |
|    rollout_return       | -1.9058107    |
| Time/                   |               |
|    collect_computeV/... | 0.00695       |
|    collect_computeV/Sum | 1.78          |
|    collect_rollout/Mean | 3.21          |
|    collect_rollout/Sum  | 3.21          |
|    train_action_adv/... | 0.00971       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0142        |
|    train_computeV/Sum   | 365           |
|    train_epoch/Mean     | 1.14e+03      |
|    train_epoch/Sum      | 1.14e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| rollout/                |               |
|    ep_len_mean          | 162           |
|    ep_rew_mean          | -161          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 55            |
|    time_elapsed         | 61971         |
|    total_timesteps      | 225280        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0122513175 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0722       |
|    explained_variance   | 0.809         |
|    learning_rate        | 0.01          |
|    loss                 | 0.237         |
|    n_updates            | 5400          |
|    policy_gradient_loss | 0.0159        |
|    value_loss           | 0.046         |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0268        |
|    negative_advantag... | 0.18813676    |
|    positive_advantag... | 0.38072336    |
|    prob_ratio           | 1276002.4     |
|    rollout_return       | -1.9820569    |
| Time/                   |               |
|    collect_computeV/... | 0.00695       |
|    collect_computeV/Sum | 1.78          |
|    collect_rollout/Mean | 3.2           |
|    collect_rollout/Sum  | 3.2           |
|    train_action_adv/... | 0.00971       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0143        |
|    train_computeV/Sum   | 365           |
|    train_epoch/Mean     | 1.14e+03      |
|    train_epoch/Sum      | 1.14e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| rollout/                |               |
|    ep_len_mean          | 154           |
|    ep_rew_mean          | -153          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 56            |
|    time_elapsed         | 63118         |
|    total_timesteps      | 229376        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0057752132 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0672       |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.01          |
|    loss                 | 0.393         |
|    n_updates            | 5500          |
|    policy_gradient_loss | 0.0308        |
|    value_loss           | 0.0338        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=230000, episode_reward=-139.40 +/- 24.35
Episode length: 140.40 +/- 24.35
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0262     |
|    negative_advantag... | 0.18867664 |
|    positive_advantag... | 0.3496828  |
|    prob_ratio           | 1159224.8  |
|    rollout_return       | -1.8050504 |
| Time/                   |            |
|    collect_computeV/... | 0.00696    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 4.55       |
|    collect_rollout/Sum  | 4.55       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 140        |
|    mean_reward          | -139       |
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | -161       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 57         |
|    time_elapsed         | 64265      |
|    total_timesteps      | 233472     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.1469185  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0626    |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.01       |
|    loss                 | 0.507      |
|    n_updates            | 5600       |
|    policy_gradient_loss | 0.0228     |
|    value_loss           | 0.0483     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0235        |
|    negative_advantag... | 0.16596241    |
|    positive_advantag... | 0.34276128    |
|    prob_ratio           | 1051636.2     |
|    rollout_return       | -1.762273     |
| Time/                   |               |
|    collect_computeV/... | 0.00699       |
|    collect_computeV/Sum | 1.79          |
|    collect_rollout/Mean | 3.21          |
|    collect_rollout/Sum  | 3.21          |
|    train_action_adv/... | 0.00971       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0143        |
|    train_computeV/Sum   | 365           |
|    train_epoch/Mean     | 1.14e+03      |
|    train_epoch/Sum      | 1.14e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| rollout/                |               |
|    ep_len_mean          | 160           |
|    ep_rew_mean          | -159          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 58            |
|    time_elapsed         | 65412         |
|    total_timesteps      | 237568        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0069747865 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0646       |
|    explained_variance   | 0.811         |
|    learning_rate        | 0.01          |
|    loss                 | 0.251         |
|    n_updates            | 5700          |
|    policy_gradient_loss | 0.0212        |
|    value_loss           | 0.0471        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=240000, episode_reward=-114.60 +/- 20.01
Episode length: 115.60 +/- 20.01
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.026       |
|    negative_advantag... | 0.1815867   |
|    positive_advantag... | 0.36084506  |
|    prob_ratio           | 1524136.5   |
|    rollout_return       | -1.8665832  |
| Time/                   |             |
|    collect_computeV/... | 0.00696     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 4.31        |
|    collect_rollout/Sum  | 4.31        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 365         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | -115        |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 59          |
|    time_elapsed         | 66560       |
|    total_timesteps      | 241664      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.027271643 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0598     |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.01        |
|    loss                 | 0.276       |
|    n_updates            | 5800        |
|    policy_gradient_loss | 0.0361      |
|    value_loss           | 0.0344      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.026      |
|    negative_advantag... | 0.19445877 |
|    positive_advantag... | 0.34284642 |
|    prob_ratio           | 1297101.0  |
|    rollout_return       | -1.8056489 |
| Time/                   |            |
|    collect_computeV/... | 0.00703    |
|    collect_computeV/Sum | 1.8        |
|    collect_rollout/Mean | 3.25       |
|    collect_rollout/Sum  | 3.25       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 365        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | -168       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 60         |
|    time_elapsed         | 67706      |
|    total_timesteps      | 245760     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.12307146 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0633    |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.01       |
|    loss                 | 0.656      |
|    n_updates            | 5900       |
|    policy_gradient_loss | 0.021      |
|    value_loss           | 0.0553     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0253       |
|    negative_advantag... | 0.18185349   |
|    positive_advantag... | 0.33909893   |
|    prob_ratio           | 1182783.9    |
|    rollout_return       | -1.7466545   |
| Time/                   |              |
|    collect_computeV/... | 0.00694      |
|    collect_computeV/Sum | 1.78         |
|    collect_rollout/Mean | 3.2          |
|    collect_rollout/Sum  | 3.2          |
|    train_action_adv/... | 0.00971      |
|    train_action_adv/Sum | 249          |
|    train_computeV/Mean  | 0.0142       |
|    train_computeV/Sum   | 364          |
|    train_epoch/Mean     | 1.14e+03     |
|    train_epoch/Sum      | 1.14e+03     |
|    train_loss/Mean      | 0.0102       |
|    train_loss/Sum       | 260          |
| rollout/                |              |
|    ep_len_mean          | 155          |
|    ep_rew_mean          | -154         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 61           |
|    time_elapsed         | 68853        |
|    total_timesteps      | 249856       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.010670856 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0626      |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.01         |
|    loss                 | 0.248        |
|    n_updates            | 6000         |
|    policy_gradient_loss | 0.0246       |
|    value_loss           | 0.0539       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=250000, episode_reward=-127.40 +/- 20.76
Episode length: 128.40 +/- 20.76
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0239      |
|    negative_advantag... | 0.17095253  |
|    positive_advantag... | 0.3228978   |
|    prob_ratio           | 1242251.9   |
|    rollout_return       | -1.7132552  |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 4.41        |
|    collect_rollout/Sum  | 4.41        |
|    train_action_adv/... | 0.00971     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 365         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | -127        |
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | -152        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 62          |
|    time_elapsed         | 70002       |
|    total_timesteps      | 253952      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | -0.13137132 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0531     |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.01        |
|    loss                 | 0.507       |
|    n_updates            | 6100        |
|    policy_gradient_loss | 0.0299      |
|    value_loss           | 0.0361      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0244     |
|    negative_advantag... | 0.17839733 |
|    positive_advantag... | 0.34745002 |
|    prob_ratio           | 986317.94  |
|    rollout_return       | -1.7552345 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 3.2        |
|    collect_rollout/Sum  | 3.2        |
|    train_action_adv/... | 0.00972    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0143     |
|    train_computeV/Sum   | 365        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 157        |
|    ep_rew_mean          | -156       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 63         |
|    time_elapsed         | 71149      |
|    total_timesteps      | 258048     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.02185589 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0671    |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.01       |
|    loss                 | 0.127      |
|    n_updates            | 6200       |
|    policy_gradient_loss | 0.016      |
|    value_loss           | 0.0443     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=260000, episode_reward=-173.40 +/- 43.07
Episode length: 174.40 +/- 43.07
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0247     |
|    negative_advantag... | 0.1815259  |
|    positive_advantag... | 0.31631628 |
|    prob_ratio           | 1133941.9  |
|    rollout_return       | -1.8717602 |
| Time/                   |            |
|    collect_computeV/... | 0.00695    |
|    collect_computeV/Sum | 1.78       |
|    collect_rollout/Mean | 4.86       |
|    collect_rollout/Sum  | 4.86       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0143     |
|    train_computeV/Sum   | 365        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | -173       |
| rollout/                |            |
|    ep_len_mean          | 155        |
|    ep_rew_mean          | -154       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 64         |
|    time_elapsed         | 72298      |
|    total_timesteps      | 262144     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.17186281 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0681    |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.01       |
|    loss                 | 0.193      |
|    n_updates            | 6300       |
|    policy_gradient_loss | 0.0265     |
|    value_loss           | 0.0441     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0247      |
|    negative_advantag... | 0.17876582  |
|    positive_advantag... | 0.32700333  |
|    prob_ratio           | 1470987.5   |
|    rollout_return       | -1.8524045  |
| Time/                   |             |
|    collect_computeV/... | 0.00695     |
|    collect_computeV/Sum | 1.78        |
|    collect_rollout/Mean | 3.19        |
|    collect_rollout/Sum  | 3.19        |
|    train_action_adv/... | 0.00972     |
|    train_action_adv/Sum | 249         |
|    train_computeV/Mean  | 0.0143      |
|    train_computeV/Sum   | 365         |
|    train_epoch/Mean     | 1.14e+03    |
|    train_epoch/Sum      | 1.14e+03    |
|    train_loss/Mean      | 0.0102      |
|    train_loss/Sum       | 260         |
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | -164        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 65          |
|    time_elapsed         | 73447       |
|    total_timesteps      | 266240      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.023615628 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0626     |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.01        |
|    loss                 | 0.11        |
|    n_updates            | 6400        |
|    policy_gradient_loss | 0.0372      |
|    value_loss           | 0.0605      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=270000, episode_reward=-134.00 +/- 68.71
Episode length: 135.00 +/- 68.71
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0236        |
|    negative_advantag... | 0.16986093    |
|    positive_advantag... | 0.31037846    |
|    prob_ratio           | 1064952.1     |
|    rollout_return       | -1.834986     |
| Time/                   |               |
|    collect_computeV/... | 0.00703       |
|    collect_computeV/Sum | 1.8           |
|    collect_rollout/Mean | 4.57          |
|    collect_rollout/Sum  | 4.57          |
|    train_action_adv/... | 0.00971       |
|    train_action_adv/Sum | 249           |
|    train_computeV/Mean  | 0.0143        |
|    train_computeV/Sum   | 365           |
|    train_epoch/Mean     | 1.14e+03      |
|    train_epoch/Sum      | 1.14e+03      |
|    train_loss/Mean      | 0.0102        |
|    train_loss/Sum       | 260           |
| eval/                   |               |
|    mean_ep_length       | 135           |
|    mean_reward          | -134          |
| rollout/                |               |
|    ep_len_mean          | 168           |
|    ep_rew_mean          | -167          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 66            |
|    time_elapsed         | 74594         |
|    total_timesteps      | 270336        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0048066676 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0549       |
|    explained_variance   | 0.696         |
|    learning_rate        | 0.01          |
|    loss                 | 3.06          |
|    n_updates            | 6500          |
|    policy_gradient_loss | 0.0185        |
|    value_loss           | 0.0457        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0217     |
|    negative_advantag... | 0.15441431 |
|    positive_advantag... | 0.30385482 |
|    prob_ratio           | 1027885.06 |
|    rollout_return       | -1.7201535 |
| Time/                   |            |
|    collect_computeV/... | 0.00699    |
|    collect_computeV/Sum | 1.79       |
|    collect_rollout/Mean | 3.26       |
|    collect_rollout/Sum  | 3.26       |
|    train_action_adv/... | 0.00971    |
|    train_action_adv/Sum | 249        |
|    train_computeV/Mean  | 0.0142     |
|    train_computeV/Sum   | 364        |
|    train_epoch/Mean     | 1.14e+03   |
|    train_epoch/Sum      | 1.14e+03   |
|    train_loss/Mean      | 0.0102     |
|    train_loss/Sum       | 260        |
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | -153       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 67         |
|    time_elapsed         | 75742      |
|    total_timesteps      | 274432     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.4044698  |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0497    |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.01       |
|    loss                 | 0.00261    |
|    n_updates            | 6600       |
|    policy_gradient_loss | 0.0228     |
|    value_loss           | 0.0428     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
-------------------------------------------
| HPO/                    |               |
|    margin               | 0.0234        |
|    negative_advantag... | 0.16949728    |
|    positive_advantag... | 0.31119138    |
|    prob_ratio           | 1117096.9     |
|    rollout_return       | -1.6697086    |
| Time/                   |               |
|    collect_computeV/... | 0.00793       |
|    collect_computeV/Sum | 2.03          |
|    collect_rollout/Mean | 3.61          |
|    collect_rollout/Sum  | 3.61          |
|    train_action_adv/... | 0.0106        |
|    train_action_adv/Sum | 271           |
|    train_computeV/Mean  | 0.0153        |
|    train_computeV/Sum   | 391           |
|    train_epoch/Mean     | 1.22e+03      |
|    train_epoch/Sum      | 1.22e+03      |
|    train_loss/Mean      | 0.0111        |
|    train_loss/Sum       | 284           |
| rollout/                |               |
|    ep_len_mean          | 163           |
|    ep_rew_mean          | -162          |
| time/                   |               |
|    fps                  | 3             |
|    iterations           | 68            |
|    time_elapsed         | 76967         |
|    total_timesteps      | 278528        |
| train/                  |               |
|    active_example       | 4096          |
|    approx_kl            | -0.0029300153 |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0517       |
|    explained_variance   | 0.614         |
|    learning_rate        | 0.01          |
|    loss                 | 0.254         |
|    n_updates            | 6700          |
|    policy_gradient_loss | 0.0273        |
|    value_loss           | 0.0501        |
-------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=280000, episode_reward=-140.20 +/- 62.29
Episode length: 141.20 +/- 62.29
----------------------------------------
| HPO/                    |            |
|    margin               | 0.0251     |
|    negative_advantag... | 0.18090655 |
|    positive_advantag... | 0.3665472  |
|    prob_ratio           | 1444902.8  |
|    rollout_return       | -1.8292274 |
| Time/                   |            |
|    collect_computeV/... | 0.00802    |
|    collect_computeV/Sum | 2.05       |
|    collect_rollout/Mean | 5.16       |
|    collect_rollout/Sum  | 5.16       |
|    train_action_adv/... | 0.0112     |
|    train_action_adv/Sum | 285        |
|    train_computeV/Mean  | 0.016      |
|    train_computeV/Sum   | 409        |
|    train_epoch/Mean     | 1.28e+03   |
|    train_epoch/Sum      | 1.28e+03   |
|    train_loss/Mean      | 0.0117     |
|    train_loss/Sum       | 299        |
| eval/                   |            |
|    mean_ep_length       | 141        |
|    mean_reward          | -140       |
| rollout/                |            |
|    ep_len_mean          | 161        |
|    ep_rew_mean          | -160       |
| time/                   |            |
|    fps                  | 3          |
|    iterations           | 69         |
|    time_elapsed         | 78248      |
|    total_timesteps      | 282624     |
| train/                  |            |
|    active_example       | 4096       |
|    approx_kl            | 0.18976063 |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0618    |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.01       |
|    loss                 | 0.402      |
|    n_updates            | 6800       |
|    policy_gradient_loss | 0.0196     |
|    value_loss           | 0.0462     |
----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
------------------------------------------
| HPO/                    |              |
|    margin               | 0.0251       |
|    negative_advantag... | 0.18062413   |
|    positive_advantag... | 0.3511998    |
|    prob_ratio           | 1652313.1    |
|    rollout_return       | -1.8841033   |
| Time/                   |              |
|    collect_computeV/... | 0.0079       |
|    collect_computeV/Sum | 2.02         |
|    collect_rollout/Mean | 3.57         |
|    collect_rollout/Sum  | 3.57         |
|    train_action_adv/... | 0.0112       |
|    train_action_adv/Sum | 286          |
|    train_computeV/Mean  | 0.016        |
|    train_computeV/Sum   | 410          |
|    train_epoch/Mean     | 1.28e+03     |
|    train_epoch/Sum      | 1.28e+03     |
|    train_loss/Mean      | 0.0117       |
|    train_loss/Sum       | 298          |
| rollout/                |              |
|    ep_len_mean          | 169          |
|    ep_rew_mean          | -169         |
| time/                   |              |
|    fps                  | 3            |
|    iterations           | 70           |
|    time_elapsed         | 79529        |
|    total_timesteps      | 286720       |
| train/                  |              |
|    active_example       | 4096         |
|    approx_kl            | -0.018508345 |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0527      |
|    explained_variance   | 0.686        |
|    learning_rate        | 0.01         |
|    loss                 | 0.126        |
|    n_updates            | 6900         |
|    policy_gradient_loss | 0.0276       |
|    value_loss           | 0.0506       |
------------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
self.rollout_buffer.size() 256
rollbutffer_get_counter 256
reward_noise_std 0.0
n_rollout_steps 256
Eval num_timesteps=290000, episode_reward=-111.60 +/- 14.35
Episode length: 112.60 +/- 14.35
New best mean reward!
-----------------------------------------
| HPO/                    |             |
|    margin               | 0.0247      |
|    negative_advantag... | 0.18616916  |
|    positive_advantag... | 0.31485146  |
|    prob_ratio           | 1208811.4   |
|    rollout_return       | -1.9339268  |
| Time/                   |             |
|    collect_computeV/... | 0.00797     |
|    collect_computeV/Sum | 2.04        |
|    collect_rollout/Mean | 4.84        |
|    collect_rollout/Sum  | 4.84        |
|    train_action_adv/... | 0.0112      |
|    train_action_adv/Sum | 286         |
|    train_computeV/Mean  | 0.016       |
|    train_computeV/Sum   | 410         |
|    train_epoch/Mean     | 1.28e+03    |
|    train_epoch/Sum      | 1.28e+03    |
|    train_loss/Mean      | 0.0117      |
|    train_loss/Sum       | 299         |
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | -112        |
| rollout/                |             |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | -174        |
| time/                   |             |
|    fps                  | 3           |
|    iterations           | 71          |
|    time_elapsed         | 80816       |
|    total_timesteps      | 290816      |
| train/                  |             |
|    active_example       | 4096        |
|    approx_kl            | 0.022314727 |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0545     |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.01        |
|    loss                 | 0.501       |
|    n_updates            | 7000        |
|    policy_gradient_loss | 0.026       |
|    value_loss           | 0.0435      |
-----------------------------------------
in hpg.py def train: self .aece CE
self.rgamma 1.0
actor_delay 3
self.policy.optimizer SGD (
Parameter Group 0
    dampening: 0
    lr: 0.01
    momentum: 0
    nesterov: False
    weight_decay: 0
)
